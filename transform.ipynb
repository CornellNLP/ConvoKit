{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_YNRDvb3etb"
   },
   "source": [
    "# Creating Our Dataset and Applying Our Transformer\n",
    "\n",
    "In this notebook, we:\n",
    "- convert the Friends dataset (https://github.com/emorynlp/character-mining) into a Corpus with ConvoKit\n",
    "- apply our Transformer, Genderromantic, to the data\n",
    "- analyze the resulting measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Wp3WIlBN4D7l",
    "outputId": "d175f9db-6bab-416b-d2d1-b98b1af1fd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: astroid==2.2.5 in /Users/emilytseng/Library/Python/3.7/lib/python/site-packages (from -r requirements.txt (line 1)) (2.2.5)\n",
      "Collecting certifi==2019.6.16 (from -r requirements.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl\n",
      "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: convokit==2.0.11 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (2.0.11)\n",
      "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: cymem==1.31.2 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.31.2)\n",
      "Requirement already satisfied: cytoolz==0.9.0.1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.9.0.1)\n",
      "Requirement already satisfied: dill==0.2.9 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (0.2.9)\n",
      "Requirement already satisfied: en-core-web-sm==2.0.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: idna==2.8 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (2.8)\n",
      "Requirement already satisfied: isort==4.3.21 in /Users/emilytseng/Library/Python/3.7/lib/python/site-packages (from -r requirements.txt (line 11)) (4.3.21)\n",
      "Requirement already satisfied: joblib==0.13.2 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (0.13.2)\n",
      "Requirement already satisfied: kiwisolver==1.1.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (1.1.0)\n",
      "Requirement already satisfied: lazy-object-proxy==1.4.2 in /Users/emilytseng/Library/Python/3.7/lib/python/site-packages (from -r requirements.txt (line 14)) (1.4.2)\n",
      "Requirement already satisfied: matplotlib==3.1.1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 15)) (3.1.1)\n",
      "Requirement already satisfied: mccabe==0.6.1 in /Users/emilytseng/Library/Python/3.7/lib/python/site-packages (from -r requirements.txt (line 16)) (0.6.1)\n",
      "Requirement already satisfied: msgpack==0.6.1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 17)) (0.6.1)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.3.2 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 18)) (0.4.3.2)\n",
      "Requirement already satisfied: murmurhash==0.28.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 19)) (0.28.0)\n",
      "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 20)) (3.4.5)\n",
      "Requirement already satisfied: numpy==1.17.2 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 21)) (1.17.2)\n",
      "Requirement already satisfied: pandas==0.25.1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 22)) (0.25.1)\n",
      "Requirement already satisfied: plac==0.9.6 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 23)) (0.9.6)\n",
      "Requirement already satisfied: preshed==1.0.1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 24)) (1.0.1)\n",
      "Requirement already satisfied: pylint==2.3.1 in /Users/emilytseng/Library/Python/3.7/lib/python/site-packages (from -r requirements.txt (line 25)) (2.3.1)\n",
      "Requirement already satisfied: pyparsing==2.4.2 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 26)) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil==2.8.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 27)) (2.8.0)\n",
      "Requirement already satisfied: pytz==2019.2 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 28)) (2019.2)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 29)) (2017.4.5)\n",
      "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 30)) (2.22.0)\n",
      "Requirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 31)) (0.21.3)\n",
      "Requirement already satisfied: scipy==1.3.1 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 32)) (1.3.1)\n",
      "Requirement already satisfied: six==1.12.0 in /Users/emilytseng/Library/Python/3.7/lib/python/site-packages (from -r requirements.txt (line 33)) (1.12.0)\n",
      "Requirement already satisfied: spacy==2.0.12 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 34)) (2.0.12)\n",
      "Requirement already satisfied: thinc==6.10.3 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 35)) (6.10.3)\n",
      "Requirement already satisfied: toolz==0.10.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 36)) (0.10.0)\n",
      "Requirement already satisfied: tqdm==4.35.0 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 37)) (4.35.0)\n",
      "Requirement already satisfied: typed-ast==1.4.0 in /Users/emilytseng/Library/Python/3.7/lib/python/site-packages (from -r requirements.txt (line 38)) (1.4.0)\n",
      "Requirement already satisfied: ujson==1.35 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 39)) (1.35)\n",
      "Requirement already satisfied: urllib3==1.25.3 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 40)) (1.25.3)\n",
      "Requirement already satisfied: wrapt==1.10.11 in /usr/local/lib/python3.7/site-packages (from -r requirements.txt (line 41)) (1.10.11)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver==1.1.0->-r requirements.txt (line 13)) (41.0.1)\n",
      "Installing collected packages: certifi\n",
      "  Found existing installation: certifi 2019.9.11\n",
      "    Uninstalling certifi-2019.9.11:\n",
      "      Successfully uninstalled certifi-2019.9.11\n",
      "Successfully installed certifi-2019.6.16\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'convokit' from '/Users/emilytseng/Cornell-Conversational-Analysis-Toolkit/convokit/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXk4Biep3xHH"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, User, Utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmw72j5J44E4"
   },
   "source": [
    "## The Friends Dataset\n",
    "\n",
    "The original dataset (https://github.com/emorynlp/character-mining) contains a set of 10 JSON files, each of which represents a complete transcript of 1 season of <i>Friends</i>. Since the data are available in JSON format from this GitHub repo, we download the raw data directly using the `requests` module. You will not need to download raw data files to use this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxhlL6aNCvxr"
   },
   "source": [
    "## Generating user information\n",
    "Since our dataset doesn't have any existing user information, we extract speaker information from the conversation. For each user, we collect the episode in which he/she first appears and guess his/her gender based on the name using the gender_guesser module.\n",
    "\n",
    "Users are indexed by their name, which is a `<str>`. For each user, we create an object with:\n",
    "\n",
    "- <b>first_appearance:</b> the episode in which he or she first appeared\n",
    "- <b>gender:</b> the character's gender, as defined by the `gender_guesser` module's guess of his/her name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "sw_7_7YJMIWp",
    "outputId": "db9ad417-6448-4075-fa0b-163126f2b706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gender_guesser in /usr/local/lib/python3.7/site-packages (0.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip3 install gender_guesser\n",
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mjNZT57Qv7Vd",
    "outputId": "cd19eef4-c487-4fd3-a336-705e284c0e32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "users = {}\n",
    "for i in tqdm(range(1,11)):\n",
    "  season_number = '0'+str(i) if i < 10 else '10'\n",
    "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
    "  r = requests.get(json_file)\n",
    "  \n",
    "  season = json.loads(r.text)\n",
    "  episodes = season['episodes']\n",
    "  for j in range(len(episodes)):\n",
    "    episode = episodes[j]\n",
    "    scenes = episode['scenes']\n",
    "    for k in range(len(scenes)):\n",
    "      scene = scenes[k]\n",
    "      utterances = scene['utterances']\n",
    "      for l in range(len(utterances)):\n",
    "        utterance = utterances[l]\n",
    "        speaker_list = utterance['speakers']\n",
    "        for speaker in speaker_list:\n",
    "          if speaker not in users:\n",
    "            users[speaker] = {'first_appearance': episode['episode_id'], 'gender': d.get_gender(speaker.split()[0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJqo7WSSCua1"
   },
   "source": [
    "Sanity-checking the user data, we should see the correct genders assigned to the 6 friends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "YeKBCPZqCMeH",
    "outputId": "3d8d81bf-36f4-4c7f-8424-437b32bbca56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users in the data = 700/700\n",
      "Monica Geller object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n",
      "Joey Tribbiani object:  {'first_appearance': 's01_e01', 'gender': 'male'}\n",
      "Chandler Bing object:  {'first_appearance': 's01_e01', 'gender': 'mostly_male'}\n",
      "Phoebe Buffay object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n",
      "Ross Geller object:  {'first_appearance': 's01_e01', 'gender': 'male'}\n",
      "Rachel Green object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n"
     ]
    }
   ],
   "source": [
    "print(\"number of users in the data = {}/700\".format(len(users)))\n",
    "print(\"Monica Geller object: \", users[\"Monica Geller\"])\n",
    "print(\"Joey Tribbiani object: \", users[\"Joey Tribbiani\"])\n",
    "print(\"Chandler Bing object: \", users[\"Chandler Bing\"])\n",
    "print(\"Phoebe Buffay object: \", users[\"Phoebe Buffay\"])\n",
    "print(\"Ross Geller object: \", users[\"Ross Geller\"])\n",
    "print(\"Rachel Green object: \", users[\"Rachel Green\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Csa06wuFh8U"
   },
   "source": [
    "We then create a User object for each unique character in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xs7QjJJJFg2S"
   },
   "outputs": [],
   "source": [
    "corpus_users = {k: User(name=k, meta=v) for k,v in users.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cfMxU1u_GJD7",
    "outputId": "9608f357-a05d-4305-bc57-f2468f9d119e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica Geller\n",
      "{'first_appearance': 's01_e01', 'gender': 'female'}\n"
     ]
    }
   ],
   "source": [
    "print(corpus_users['Monica Geller'].name)\n",
    "print(corpus_users['Monica Geller'].meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tJ_9BMXFygP"
   },
   "source": [
    "## Generating Utterances\n",
    "\n",
    "We then loop through the data to generate a list of all utterances in the series. To align with the Utterance schema ConvoKit expects, we construct for each utterance:\n",
    "\n",
    "- **id:** index of the utterance\n",
    "\n",
    "- **user:** the user who authored the utterance; the speaker in our case\n",
    "\n",
    "- **root:** id of the conversation root of the utterance; the first utterance in the scene, in our case\n",
    "\n",
    "- **reply_to:** id of the utterance to which this utterance replies to; None if the utterance is not a reply.\n",
    "\n",
    "- **timestamp:** time of the utterance (None for us -- the dataset does not contain this information)\n",
    "\n",
    "- **text:** textual content of the utterance\n",
    "\n",
    "We also pull in the following metadata including:\n",
    "- **tokens** a tokenized representation of the text (handy for sentence separation)\n",
    "-**character_entities** available for some but not all utterances; `None` if unavailable. These are intended to identify who the user is speaking to and/or about.\n",
    "-**emotion** emotion labels for each token. Available for some but not all utterances; `None` if unavailable. \n",
    "-**caption**  available for some but not all utterances; `None` if unavailable. This contains the begin time, end time, and text sans punctuation. Only available for seasons 6-9.\n",
    "-**transcript_with_note**  a version of the text with an action note (e.g. \"(to Ross) Hand me the coffee\" vs. \"Hand me the coffee\"). Available for some but not all utterances; `None` if unavailable.\n",
    "-**token_with_note** a tokenized representation of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MDgFZjF6GN3O",
    "outputId": "3a963fc3-cba2-4a3a-b932-cc9526568439"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "all_utterances = {}\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(1,11)):\n",
    "  season_number = '0'+str(i) if i < 10 else '10'\n",
    "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
    "  r = requests.get(json_file)\n",
    "  \n",
    "  season = json.loads(r.text)\n",
    "  episodes = season['episodes']\n",
    "  for j in range(len(episodes)):\n",
    "    episode = episodes[j]\n",
    "    scenes = episode['scenes']\n",
    "    for k in range(len(scenes)):\n",
    "      scene = scenes[k]\n",
    "      utterances = scene['utterances']\n",
    "      \n",
    "      root = utterances[0] #set the root as the first utterance in the scene for now\n",
    "      \n",
    "      prev_utt = None\n",
    "\n",
    "      for l in range(len(utterances)):\n",
    "        utterance = utterances[l]\n",
    "        \n",
    "        speaker = utterance['speakers']\n",
    "        \n",
    "        if len(speaker) == 0:\n",
    "          prev_utt = None\n",
    "          continue\n",
    "        \n",
    "        # Add meta       \n",
    "        meta = {\n",
    "            'tokens': utterance.get('tokens'),\n",
    "            'character_entities': utterance.get('character_entities'),\n",
    "            'emotion': utterance.get('emotion'),\n",
    "            'caption': utterance.get('caption'),\n",
    "            'transcript_with_note': utterance.get('transcript_with_note'),\n",
    "            'tokens_with_note': utterance.get('tokens_with_note')\n",
    "        }\n",
    "        \n",
    "        # Create the Utterance, including meta\n",
    "        all_utterances[utterance['utterance_id']] = Utterance(\n",
    "            id=utterance['utterance_id'],\n",
    "            user=corpus_users[speaker[0]],\n",
    "            root=root['utterance_id'],\n",
    "            reply_to=prev_utt,\n",
    "            timestamp=None,\n",
    "            text=utterance['transcript'],\n",
    "            meta=meta\n",
    "        )\n",
    "        \n",
    "        # Get the prev_utt for the next iteration\n",
    "        prev_utt = utterance['utterance_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pmVQJYBiI1aY",
    "outputId": "5fe98756-0ba8-4052-a288-59117417cac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus has 61338/61309 utterances\n"
     ]
    }
   ],
   "source": [
    "print(\"This corpus has {}/61309 utterances\".format(len(all_utterances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "NkOm5qDdZYRQ",
    "outputId": "ea4e4a62-1260-4179-8e41-0450404fe3e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': 's01_e18_c05_u021', 'user': User([('name', 'Ross Geller')]), 'root': 's01_e18_c05_u001', 'reply_to': 's01_e18_c05_u020', 'timestamp': None, 'text': 'Alright.', 'meta': {'tokens': [['Alright', '.']], 'character_entities': [[]], 'emotion': ['Neutral', ['Neutral', 'Neutral', 'Neutral', 'Neutral']], 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_utterances['s01_e18_c05_u021']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjb5fkwmN5Ma"
   },
   "source": [
    "## Creating the corpus from a list of utterances\n",
    "\n",
    "We now create the corpus from our dict of utterances. Note, we are are allowing convokit to create conversations IDs automatically after loading the utterances list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0C_xjYMfOKVL"
   },
   "outputs": [],
   "source": [
    "utterance_list = [utt for k, utt in all_utterances.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVyGuspKOU7E"
   },
   "outputs": [],
   "source": [
    "friends_corpus = Corpus(utterances=utterance_list, version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UW5uEDZIcjWO"
   },
   "source": [
    "Sanity checks for the number of conversations in the dataset and the first 5 conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oCQcxnu1OZXv",
    "outputId": "d6c78d98-9a75-4f64-a1e0-4ab8634e023b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations in the dataset=3099\n"
     ]
    }
   ],
   "source": [
    "print(\"number of conversations in the dataset={}\".format(len(friends_corpus.get_conversation_ids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "Uypy9F-Vx0kM",
    "outputId": "7d7dcf7c-40cf-4366-c15d-df2d96397877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample conversation 0:\n",
      "['s01_e01_c01_u001', 's01_e01_c01_u002', 's01_e01_c01_u003', 's01_e01_c01_u004', 's01_e01_c01_u006', 's01_e01_c01_u007', 's01_e01_c01_u008', 's01_e01_c01_u010', 's01_e01_c01_u011', 's01_e01_c01_u012', 's01_e01_c01_u013', 's01_e01_c01_u014', 's01_e01_c01_u015', 's01_e01_c01_u016', 's01_e01_c01_u017', 's01_e01_c01_u018', 's01_e01_c01_u019', 's01_e01_c01_u021', 's01_e01_c01_u022', 's01_e01_c01_u023', 's01_e01_c01_u024', 's01_e01_c01_u025', 's01_e01_c01_u026', 's01_e01_c01_u027', 's01_e01_c01_u028', 's01_e01_c01_u029', 's01_e01_c01_u030', 's01_e01_c01_u031', 's01_e01_c01_u032', 's01_e01_c01_u033', 's01_e01_c01_u034', 's01_e01_c01_u035', 's01_e01_c01_u036', 's01_e01_c01_u037', 's01_e01_c01_u038', 's01_e01_c01_u039', 's01_e01_c01_u040', 's01_e01_c01_u041', 's01_e01_c01_u042', 's01_e01_c01_u044', 's01_e01_c01_u045', 's01_e01_c01_u047', 's01_e01_c01_u048', 's01_e01_c01_u049', 's01_e01_c01_u050', 's01_e01_c01_u051', 's01_e01_c01_u052', 's01_e01_c01_u053', 's01_e01_c01_u055', 's01_e01_c01_u056', 's01_e01_c01_u057', 's01_e01_c01_u058']\n",
      "sample conversation 1:\n",
      "['s01_e01_c02_u001', 's01_e01_c02_u002', 's01_e01_c02_u003', 's01_e01_c02_u004', 's01_e01_c02_u006', 's01_e01_c02_u007', 's01_e01_c02_u008', 's01_e01_c02_u009', 's01_e01_c02_u011', 's01_e01_c02_u012', 's01_e01_c02_u013', 's01_e01_c02_u014', 's01_e01_c02_u015', 's01_e01_c02_u017', 's01_e01_c02_u018', 's01_e01_c02_u019', 's01_e01_c02_u020', 's01_e01_c02_u021', 's01_e01_c02_u022', 's01_e01_c02_u023', 's01_e01_c02_u024', 's01_e01_c02_u026', 's01_e01_c02_u027', 's01_e01_c02_u028', 's01_e01_c02_u029', 's01_e01_c02_u030', 's01_e01_c02_u031', 's01_e01_c02_u032', 's01_e01_c02_u033', 's01_e01_c02_u034', 's01_e01_c02_u035', 's01_e01_c02_u036', 's01_e01_c02_u037', 's01_e01_c02_u038', 's01_e01_c02_u039', 's01_e01_c02_u040', 's01_e01_c02_u041', 's01_e01_c02_u043', 's01_e01_c02_u044', 's01_e01_c02_u045', 's01_e01_c02_u046', 's01_e01_c02_u047', 's01_e01_c02_u048', 's01_e01_c02_u049', 's01_e01_c02_u051', 's01_e01_c02_u052', 's01_e01_c02_u053', 's01_e01_c02_u054', 's01_e01_c02_u055', 's01_e01_c02_u056', 's01_e01_c02_u057', 's01_e01_c02_u058', 's01_e01_c02_u059', 's01_e01_c02_u060', 's01_e01_c02_u061', 's01_e01_c02_u062']\n",
      "sample conversation 2:\n",
      "['s01_e01_c03_u001']\n",
      "sample conversation 3:\n",
      "['s01_e01_c04_u001', 's01_e01_c04_u003', 's01_e01_c04_u004', 's01_e01_c04_u005', 's01_e01_c04_u006', 's01_e01_c04_u007', 's01_e01_c04_u008', 's01_e01_c04_u010', 's01_e01_c04_u011', 's01_e01_c04_u012', 's01_e01_c04_u013', 's01_e01_c04_u014', 's01_e01_c04_u015', 's01_e01_c04_u016', 's01_e01_c04_u017', 's01_e01_c04_u018', 's01_e01_c04_u019']\n",
      "sample conversation 4:\n",
      "['s01_e01_c05_u001', 's01_e01_c05_u002', 's01_e01_c05_u003', 's01_e01_c05_u004', 's01_e01_c05_u005', 's01_e01_c05_u006', 's01_e01_c05_u007', 's01_e01_c05_u008', 's01_e01_c05_u009']\n"
     ]
    }
   ],
   "source": [
    "convo_ids = friends_corpus.get_conversation_ids()\n",
    "for i, convo_idx in enumerate(convo_ids[0:5]):\n",
    "    print(\"sample conversation {}:\".format(i))\n",
    "    print(friends_corpus.get_conversation(convo_idx).get_utterance_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAHTbGQxcor3"
   },
   "source": [
    "Summary stats for the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "G0pyZSmWW9_1",
    "outputId": "e8e9d98a-c07b-4664-a048-4322ff105c43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 699\n",
      "Number of Utterances: 61338\n",
      "Number of Conversations: 3099\n"
     ]
    }
   ],
   "source": [
    "friends_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0WxYF6jbY5l"
   },
   "source": [
    "## Adding corpus-level metadata\n",
    "\n",
    "We add the name of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oGMAvEza5hL"
   },
   "outputs": [],
   "source": [
    "friends_corpus.meta['name'] = 'Friends Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use locally defined Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convokit.genderromantic.genderromantic.Genderromantic"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import convokit\n",
    "from convokit import Genderromantic\n",
    "Genderromantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "grr = Genderromantic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'RomanticWords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-18657a509dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfriends_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Cornell-Conversational-Analysis-Toolkit/convokit/transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \"\"\"\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Cornell-Conversational-Analysis-Toolkit/convokit/genderromantic/genderromantic.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mmale_about_female\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mfemale_about_male\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RomanticWords.txt'"
     ]
    }
   ],
   "source": [
    "grr.fit_transform(friends_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('__abstractmethods__', frozenset()),\n",
       " ('__class__', abc.ABCMeta),\n",
       " ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': 'convokit.genderromantic.genderromantic',\n",
       "                '__init__': <function convokit.genderromantic.genderromantic.Genderromantic.__init__(self)>,\n",
       "                'genderDictionary': <staticmethod at 0x11792b750>,\n",
       "                'transform': <function convokit.genderromantic.genderromantic.Genderromantic.transform(self, corpus: convokit.model.corpus.Corpus)>,\n",
       "                '__doc__': None,\n",
       "                '__abstractmethods__': frozenset(),\n",
       "                '_abc_impl': <_abc_data at 0x117911db0>})),\n",
       " ('__dir__', <method '__dir__' of 'object' objects>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__',\n",
       "  <function convokit.genderromantic.genderromantic.Genderromantic.__init__(self)>),\n",
       " ('__init_subclass__', <function Genderromantic.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', 'convokit.genderromantic.genderromantic'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n",
       " ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__slots__', ()),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function Genderromantic.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'Transformer' objects>),\n",
       " ('_abc_impl', <_abc_data at 0x117911db0>),\n",
       " ('fit',\n",
       "  <function convokit.transformer.Transformer.fit(self, corpus: convokit.model.corpus.Corpus)>),\n",
       " ('fit_transform',\n",
       "  <function convokit.transformer.Transformer.fit_transform(self, corpus: convokit.model.corpus.Corpus) -> convokit.model.corpus.Corpus>),\n",
       " ('genderDictionary',\n",
       "  <function convokit.genderromantic.genderromantic.Genderromantic.genderDictionary(arg)>),\n",
       " ('transform',\n",
       "  <function convokit.genderromantic.genderromantic.Genderromantic.transform(self, corpus: convokit.model.corpus.Corpus)>)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getmembers(Genderromantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convert-et397.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
