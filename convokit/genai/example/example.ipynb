{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52b677a",
   "metadata": {},
   "source": [
    "# Example of How to Use GenAI with ConvoKit genai Module\n",
    "\n",
    "The ConvoKit GenAI module provides a unified interface for working with large language models (LLMs) while doing conversational analysis in ConvoKit. It supports multiple providers including OpenAI GPT, Google Gemini, and local models through a simple factory pattern. This module makes it easy to integrate AI-powered text generation into your ConvoKit workflows for diverse tasks. The module handles API key management, response formatting, and provides consistent interfaces across different LLM providers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321da09c",
   "metadata": {},
   "source": [
    "## Setup config for GenAI with GPT\n",
    "\n",
    "Setting up config info to access models is mandatory but simple. For models we implemented (GPT and Gemini), we provide methods to set API keys so they are stored in the environment. For other models or local LLMs, users can also implement them in similar manner. Here we provide a simple demonstration with configuring for OpenAI's GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be24454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.genai.genai_config import GenAIConfigManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully set OpenAI API key in config.\n"
     ]
    }
   ],
   "source": [
    "config = GenAIConfigManager()\n",
    "config.set_api_key(\"gpt\", \"YOUR API KEY\")\n",
    "print(f\"Successfully set OpenAI API key in config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dce4b7",
   "metadata": {},
   "source": [
    "## Initialize clients to Communicate with models\n",
    "\n",
    "After setting the API key, we are ready to communicate with the models. Retrieve response the same as you would normally do interacting with models through API. However, we do wrap the LLM responses in a unified class, so we can handle all LLM response format easily. Users are expected to follow similar template when implementing clients for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.genai import get_llm_client\n",
    "\n",
    "MODEL_PROVIDER = \"gpt\"\n",
    "client = get_llm_client(MODEL_PROVIDER, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ea36378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: A fun fact about Cornell University is that it is home to the world's first university-based hotel management program. Established in 1922, the School of Hotel Administration at Cornell has been a leader in hospitality education, shaping the future of the industry and producing many influential leaders in hospitality and tourism. The program is renowned for its rigorous curriculum and strong connections to the hospitality industry.\n",
      "Tokens: 90\n",
      "Latency: 2.0230631828308105\n"
     ]
    }
   ],
   "source": [
    "response = client.generate([{\"role\": \"user\", \"content\": \"Tell me a fun fact about Cornell University.\"}])\n",
    "print(\"Text:\", response.text)\n",
    "print(\"Tokens:\", response.tokens)\n",
    "print(\"Latency:\", response.latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b72969",
   "metadata": {},
   "source": [
    "# Setup ConvoKit GenAI with Google Gemini Through Vertex AI\n",
    "\n",
    "Very similar to GPT. Checkout Vertex AI: https://cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb38d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROVIDER = \"gemini\"\n",
    "MODEL = \"gemini-2.0-flash-001\"\n",
    "config.set_google_cloud_config(\"YOUR PROJECT\", \"YOUR LOCATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32fad57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Here's a fun fact about Cornell University:\n",
      "\n",
      "Cornell has a tradition called \"Slope Day,\" a massive student-organized festival held on the last day of classes each spring. It's a day of music, food, and general celebration on Libe Slope, a large grassy hill on campus. While it's a beloved tradition, it's also known for its, shall we say, *spirited* atmosphere!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = get_llm_client(MODEL_PROVIDER, config)\n",
    "response = client.generate(\"Tell me a fun fact about Cornell University.\")\n",
    "print(\"Text:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d983b",
   "metadata": {},
   "source": [
    "# Using GenAITransformer with ConvoKit\n",
    "\n",
    "The GenAITransformer provides a powerful way to apply LLM processing to different levels of ConvoKit objects (utterances, conversations, speakers, or corpus). Let's demonstrate this with the friends corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72195e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the friends corpus and get the first conversation\n",
    "from convokit import Corpus, download\n",
    "from convokit.genai import GenAITransformer\n",
    "\n",
    "# Load the corpus\n",
    "corpus = Corpus(filename=download(\"friends-corpus\"))\n",
    "\n",
    "# Get the first conversation and its first two utterances\n",
    "first_convo = list(corpus.iter_conversations())[0]\n",
    "utterances = first_convo.get_chronological_utterance_list()[:2]\n",
    "\n",
    "print(f\"First conversation ID: {first_convo.id}\")\n",
    "print(f\"Number of utterances in first conversation: {len(first_convo.get_chronological_utterance_list())}\")\n",
    "print(f\"We'll process the first 2 utterances:\")\n",
    "for i, utt in enumerate(utterances):\n",
    "    print(f\"  Utterance {i+1}: {utt.speaker.id}: {utt.text[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a7438",
   "metadata": {},
   "source": [
    "## Example: Sentiment Analysis on Utterances\n",
    "\n",
    "Let's create a GenAI transformer that analyzes the sentiment of utterances and stores the result as metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sentiment analysis transformer\n",
    "sentiment_transformer = GenAITransformer(\n",
    "    provider=\"gpt\",  # Use the same provider we configured earlier\n",
    "    model=\"gpt-4o-mini\",\n",
    "    object_level=\"utterance\",  # Process at utterance level\n",
    "    prompt=\"Analyze the sentiment of the following text and respond with just one word: 'positive', 'negative', or 'neutral'. Text: {formatted_object}\",\n",
    "    formatter=lambda utterance: utterance.text,  # Extract text from utterance\n",
    "    metadata_name=\"sentiment\",  # Store result in 'sentiment' metadata field\n",
    "    selector=lambda utterance: utterance.id in [utt.id for utt in utterances],  # Only process our 2 utterances\n",
    "    config_manager=config\n",
    ")\n",
    "\n",
    "# Apply the transformer to the corpus\n",
    "corpus = sentiment_transformer.transform(corpus)\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "for utt in utterances:\n",
    "    sentiment = utt.meta.get(\"sentiment\", \"Not processed\")\n",
    "    print(f\"  {utt.speaker.id}: '{utt.text[:50]}...' -> Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba34dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4498e9a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d64f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
