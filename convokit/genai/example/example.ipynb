{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52b677a",
   "metadata": {},
   "source": [
    "# Example of How to Use GenAI with ConvoKit genai Module\n",
    "\n",
    "The ConvoKit GenAI module provides a unified interface for working with large language models (LLMs) while doing conversational analysis in ConvoKit. It supports multiple providers including OpenAI GPT, Google Gemini, and local models through a simple factory pattern. This module makes it easy to integrate AI-powered text generation into your ConvoKit workflows for diverse tasks. The module handles API key management, response formatting, and provides consistent interfaces across different LLM providers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321da09c",
   "metadata": {},
   "source": [
    "## Setup config for GenAI\n",
    "\n",
    "Setting up config info to access models is mandatory but simple. For models we implemented (GPT and Gemini), we provide methods to set API keys so they are stored in the environment. For other models or local LLMs, users can also implement them in similar manner. Here we provide a simple demonstration with configuring for OpenAI's GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.genai.genai_config import GenAIConfigManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GenAIConfigManager()\n",
    "config.set_api_key(\"gpt\", \"YOUR_API_KEY\")\n",
    "print(f\"Successfully set OpenAI API key in config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dce4b7",
   "metadata": {},
   "source": [
    "## Initialize clients to Communicate with models\n",
    "\n",
    "After setting the API key, we are ready to communicate with the models. Retrieve response the same as you would normally do interacting with models through API. However, we do wrap the LLM responses in a unified class, so we can handle all LLM response format easily. Users are expected to follow similar template when implementing clients for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.genai import get_llm_client\n",
    "\n",
    "client = get_llm_client(\"gpt\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea36378",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.generate([{\"role\": \"user\", \"content\": \"Tell me a fun fact about Cornell University.\"}])\n",
    "print(\"Text:\", response.text)\n",
    "print(\"Tokens:\", response.tokens)\n",
    "print(\"Latency:\", response.latency)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
