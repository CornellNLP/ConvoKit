{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52b677a",
   "metadata": {},
   "source": [
    "# Example of How to Use GenAI with ConvoKit genai Module\n",
    "\n",
    "The ConvoKit GenAI module provides a unified interface for working with large language models (LLMs) while doing conversational analysis in ConvoKit. It supports multiple providers including OpenAI GPT, Google Gemini, and local models through a simple factory pattern. This module makes it easy to integrate AI-powered text generation into your ConvoKit workflows for diverse tasks. The module handles API key management, response formatting, and provides consistent interfaces across different LLM providers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321da09c",
   "metadata": {},
   "source": [
    "## Setup config for GenAI with GPT\n",
    "\n",
    "Setting up config info to access models is mandatory but simple. For models we implemented (GPT and Gemini), we provide methods to set API keys so they are stored in the environment. For other models or local LLMs, users can also implement them in similar manner. Here we provide a simple demonstration with configuring for OpenAI's GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be24454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.genai.genai_config import GenAIConfigManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully set OpenAI API key in config.\n"
     ]
    }
   ],
   "source": [
    "config = GenAIConfigManager()\n",
    "config.set_api_key(\"gpt\", \"YOUR API KEY\")\n",
    "print(f\"Successfully set OpenAI API key in config.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dce4b7",
   "metadata": {},
   "source": [
    "## Initialize clients to Communicate with models\n",
    "\n",
    "After setting the API key, we are ready to communicate with the models. Retrieve response the same as you would normally do interacting with models through API. However, we do wrap the LLM responses in a unified class, so we can handle all LLM response format easily. Users are expected to follow similar template when implementing clients for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "220c33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.genai import get_llm_client\n",
    "\n",
    "MODEL_PROVIDER = \"gpt\"\n",
    "client = get_llm_client(MODEL_PROVIDER, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ea36378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: A fun fact about Cornell University is that it is home to the world's first university-based hotel management program. Established in 1922, the Cornell School of Hotel Administration has become a leader in hospitality education, attracting students from around the globe who aspire to careers in the hospitality industry. The program is renowned for its rigorous curriculum and strong connections to the industry, making it a top choice for aspiring hotel and restaurant managers.\n",
      "Tokens: 99\n",
      "Latency: 1.7928547859191895\n"
     ]
    }
   ],
   "source": [
    "response = client.generate([{\"role\": \"user\", \"content\": \"Tell me a fun fact about Cornell University.\"}])\n",
    "print(\"Text:\", response.text)\n",
    "print(\"Tokens:\", response.tokens)\n",
    "print(\"Latency:\", response.latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b72969",
   "metadata": {},
   "source": [
    "# Setup ConvoKit GenAI with Google Gemini Through Vertex AI\n",
    "\n",
    "Very similar to GPT. Checkout Vertex AI: https://cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb38d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PROVIDER = \"gemini\"\n",
    "MODEL = \"gemini-2.0-flash-001\"\n",
    "config.set_google_cloud_config(\"YOUR PROJECT\", \"YOUR LOCATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32fad57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Here's a fun fact about Cornell University:\n",
      "\n",
      "Cornell has a tradition called \"Dragon Day\" where architecture students build a giant dragon and parade it across campus. The dragon is often pitted against a phoenix built by engineering students, leading to a playful rivalry and a spectacle of creativity and engineering!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = get_llm_client(MODEL_PROVIDER, config)\n",
    "response = client.generate(\"Tell me a fun fact about Cornell University.\")\n",
    "print(\"Text:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d983b",
   "metadata": {},
   "source": [
    "# Using LLMPromptTransformer with ConvoKit\n",
    "\n",
    "The LLMPromptTransformer provides a powerful way to apply LLM processing to different levels of ConvoKit objects (utterances, conversations, speakers, or corpus). Let's demonstrate this with the friends corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72195e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /reef/kz88/convokit/download_corpus/friends-corpus\n",
      "First conversation ID: s01_e01_c01_u001\n",
      "We'll process the first 2 utterances:\n",
      "  Utterance 1: Monica Geller: There's nothing to tell! He's just some guy I work with!...\n",
      "  Utterance 2: Joey Tribbiani: C'mon, you're going out with the guy! There's gotta be something wrong with him!...\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "from convokit.genai import LLMPromptTransformer\n",
    "\n",
    "corpus = Corpus(filename=download(\"friends-corpus\"))\n",
    "\n",
    "first_convo = corpus.get_conversation(corpus.get_conversation_ids()[0])\n",
    "assert len(first_convo.get_utterance_ids()) > 2\n",
    "utterances = first_convo.get_utterance_ids()[:2]\n",
    "\n",
    "print(f\"First conversation ID: {first_convo.id}\")\n",
    "print(f\"We'll process the first 2 utterances:\")\n",
    "for i, uttid in enumerate(utterances):\n",
    "    utt = corpus.get_utterance(uttid)\n",
    "    print(f\"  Utterance {i+1}: {utt.speaker.id}: {utt.text[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a7438",
   "metadata": {},
   "source": [
    "## Example: Sentiment Analysis on Utterances\n",
    "\n",
    "Let's create a GenAI transformer that analyzes the sentiment of utterances and stores the result as metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis results:\n",
      "  Monica Geller: 'There's nothing to tell! He's just some guy I work...' -> Sentiment: Negative\n",
      "  Joey Tribbiani: 'C'mon, you're going out with the guy! There's gott...' -> Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "sentiment_transformer = LLMPromptTransformer(\n",
    "    provider=\"gpt\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    object_level=\"utterance\",  # Process at utterance level\n",
    "    prompt=\"Analyze the sentiment of the following text and respond with just one word: 'positive', 'negative', or 'neutral'. Text: {formatted_object}\",\n",
    "    formatter=lambda utterance: utterance.text,\n",
    "    metadata_name=\"gpt_sentiment\",  # Store result in 'gpt_sentiment' metadata field\n",
    "    selector=lambda utterance: utterance.id in utterances,\n",
    "    config_manager=config\n",
    ")\n",
    "\n",
    "corpus = sentiment_transformer.transform(corpus)\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "for uttid in utterances:\n",
    "    utt = corpus.get_utterance(uttid)\n",
    "    sentiment = utt.meta.get(\"gpt_sentiment\", \"Not processed\")\n",
    "    print(f\"  {utt.speaker.id}: '{utt.text[:50]}...' -> Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9408e2",
   "metadata": {},
   "source": [
    "## Testing Prompts on Single Objects\n",
    "\n",
    "The `transform_single` method allows you to test your prompts on individual objects without processing an entire corpus. This function allows user to test prompt development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11439f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a single utterance using string input\n",
    "test_text = \"I absolutely love this new feature! It's amazing!\"\n",
    "result = sentiment_transformer.transform_single(test_text)\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Sentiment: {result.meta.get('gpt_sentiment', 'Not processed')}\")\n",
    "print(f\"Result type: {type(result)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82858c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with an actual utterance object\n",
    "from convokit import Utterance, Speaker\n",
    "\n",
    "test_utterance = Utterance(\n",
    "    id=\"test_utt\",\n",
    "    text=\"This is terrible! I hate it!\",\n",
    "    speaker=Speaker(id=\"test_speaker\")\n",
    ")\n",
    "\n",
    "result = sentiment_transformer.transform_single(test_utterance)\n",
    "print(f\"Input utterance: {test_utterance.text}\")\n",
    "print(f\"Speaker: {test_utterance.speaker.id}\")\n",
    "print(f\"Sentiment: {result.meta.get('gpt_sentiment', 'Not processed')}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93446e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Testing conversation-level transformation\n",
    "conversation_transformer = LLMPromptTransformer(\n",
    "    provider=\"gpt\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    object_level=\"conversation\",\n",
    "    prompt=\"Summarize this conversation in one sentence: {formatted_object}\",\n",
    "    formatter=lambda conv: \" \".join([utt.text for utt in conv.iter_utterances()]),\n",
    "    metadata_name=\"conversation_summary\",\n",
    "    config_manager=config\n",
    ")\n",
    "\n",
    "test_conv = corpus.get_conversation(corpus.get_conversation_ids()[0])\n",
    "result = conversation_transformer.transform_single(test_conv)\n",
    "print(f\"Conversation ID: {test_conv.id}\")\n",
    "print(f\"Summary: {result.meta.get('conversation_summary', 'Not processed')}\")\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
