{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f8cc3c",
   "metadata": {},
   "source": [
    "# Validation Setup for ConDynS\n",
    "\n",
    "This notebook demonstrates the validation procedure for ConDynS, our similarity measure for comparing conversational dynamics, introduced in the [paper: A Similarity Measure for Comparing Conversational Dynamics](https://arxiv.org/abs/2507.18956). It constructs anchor–positive–negative triplets of conversations from Reddit, where positives share similar dynamics with the anchor and negatives differ, and evaluates how well ConDynS distinguishes them relative to baseline similarity measures (e.g., SBERT cosine similarity, BERTScore), as demonstrate in the other demo notebook. This notebook follows the methodology described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb45738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "random.seed(4300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ee87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))\n",
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the human and machine summary ids ###\n",
    "human_summary_ids = corpus.get_conversation_ids(selector=lambda conversation: conversation.meta[\"summary_meta\"] != []\n",
    "and any(summary_meta[\"summary_type\"] == \"human_written_SCD\" for summary_meta in conversation.meta[\"summary_meta\"]))\n",
    "machine_summary_ids = corpus.get_conversation_ids(selector=lambda conversation: conversation.meta[\"summary_meta\"] != []\n",
    "               and any(summary_meta[\"summary_type\"] == \"machine_generated_SCD\" for summary_meta in conversation.meta[\"summary_meta\"]))\n",
    "pair_of = {}\n",
    "for convo_id in human_summary_ids:\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    pair_of[convo.id] = convo.meta['pair_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get pair info ###\n",
    "human_summary_pair = [] # (calm, awry) \n",
    "for convo_id in human_summary_ids:\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    if convo.meta['has_removed_comment']:\n",
    "        if (convo.meta['pair_id'],convo.id) not in human_summary_pair:\n",
    "            human_summary_pair.append((convo.meta['pair_id'],convo.id))\n",
    "    else:\n",
    "        if (convo.id, convo.meta['pair_id']) not in human_summary_pair:\n",
    "            human_summary_pair.append((convo.meta['pair_id'],convo.id))\n",
    "print(\"Number of conversation pair: \", len(human_summary_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7be53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTEFACTS_DIR = \"./artefacts/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5680368",
   "metadata": {},
   "source": [
    "# ConDynS Validation\n",
    "\n",
    "Here we compute ConDynS on a subset of Reddit conversations with constructed triplets to validate the measure's usefulness in capturing and comparing conversational dynamics (discussed in detail in paper Section 5). The followings are steps to conduct the validation setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4130a",
   "metadata": {},
   "source": [
    "## Simulating Conversations\n",
    "\n",
    "To construct the triplets used for validating ConDynS (see Section 5 of the paper), we simulate synthetic conversations from human-written SCDs provided in the ConvoKit corpus. These SCDs abstract away surface content while preserving conversational dynamics. By generating conversations from these summaries, we can also assign new topics—allowing us to test whether ConDynS remains sensitive to dynamics while being invariant to topical changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6dd235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.convo_similarity.utils import format_transcript_from_convokit, get_human_summary\n",
    "from convokit.genai import get_llm_client\n",
    "from convokit.genai.genai_config import GenAIConfigManager\n",
    "\n",
    "config = GenAIConfigManager() ### make sure to set your own config if this is never set before\n",
    "client = get_llm_client(\"gpt\", config)\n",
    "\n",
    "def gpt_query(prompt, **kwargs):\n",
    "    response = client.generate(prompt, **kwargs)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e31604",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract topic of the conversations ###\n",
    "topic_msg = \"\"\"Here are two conversations of the same topic. Summarize the topic of the conversations in a concise phrase that accurately captures the main subject being discussed.\n",
    "Here is the transcript of the first conversation:\n",
    "{transcript1}\n",
    "\n",
    "Here is the transcript of the second conversation:\n",
    "{transcript2}\n",
    "\n",
    "Now, write the topic of the conversation in a concise phrase:\n",
    "\"\"\"\n",
    "topic = {}\n",
    "for calm_convo_id, awry_convo_id in tqdm(human_summary_pair):\n",
    "    calm_transcript = format_transcript_from_convokit(corpus, calm_convo_id)\n",
    "    awry_transcript = format_transcript_from_convokit(corpus, awry_convo_id)\n",
    "    query = topic_msg.format(transcript1 = '\\n'.join(calm_transcript), transcript2 = '\\n'.join(awry_transcript))\n",
    "    response = gpt_query(query)\n",
    "    topic[calm_convo_id] = response\n",
    "    topic[awry_convo_id] = response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulate transcript ###\n",
    "simulation_msg = \"\"\"You are given a task to recreate an online conversation that occured on reddit. Here is a list of information you are given.\n",
    "1. Topic of the conversation: {topic}\n",
    "2. Trajectory summary that summarizes the conversational and speakers' dynamics: {trajectory_summary}\n",
    "\n",
    "Each utterance of the transcript should be formatted as the following:\n",
    "Speaker_ID (e.g. \"SPEAKER2\") : [Added text of the utterance]\n",
    "\n",
    "\n",
    "#Output\n",
    "Add your recreated conversation. Only generate the transcript of the conversation. \n",
    "\"\"\"\n",
    "generated_transcripts = {}\n",
    "for calm_convo_id, awry_convo_id in tqdm(human_summary_pair):\n",
    "    calm_human_summary = get_human_summary(corpus, calm_convo_id)\n",
    "    awry_human_summary = get_human_summary(corpus, awry_convo_id)\n",
    "    calm_query = simulation_msg.format(topic=topic[calm_convo_id],trajectory_summary=calm_human_summary['summary_text'])\n",
    "    calm_response = gpt_query(calm_query)\n",
    "    generated_transcripts[calm_convo_id] = calm_response\n",
    "    awry_query = simulation_msg.format(topic=topic[awry_convo_id],trajectory_summary=awry_human_summary['summary_text'])\n",
    "    awry_response = gpt_query(awry_query)\n",
    "    generated_transcripts[awry_convo_id] = awry_response\n",
    "\n",
    "output = {}\n",
    "for convo_id in generated_transcripts:\n",
    "    output[convo_id] = {\n",
    "        'transcript': generated_transcripts[convo_id],\n",
    "        'topic': topic[convo_id]\n",
    "    }\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e01407",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Topic shuffle transcript simulation ###\n",
    "topic_set = []\n",
    "for i, (calm_convo_id, awry_convo_id) in enumerate(human_summary_pair):\n",
    "    topic_set.append(generated_transcripts[human_summary_pair[(i) % len(human_summary_pair)][0]]['topic'])\n",
    "\n",
    "new_topic = {}\n",
    "for i, (calm_convo_id, awry_convo_id) in enumerate(human_summary_pair):\n",
    "    new_topic[calm_convo_id] = random.choice(topic_set)\n",
    "    new_topic[awry_convo_id] = random.choice(topic_set)\n",
    "for convo_id in new_topic:\n",
    "    assert new_topic[convo_id] != generated_transcripts[convo_id]['topic']\n",
    "assert len(new_topic) == len(generated_transcripts)\n",
    "\n",
    "generated_transcripts_topic_shuffled = {}\n",
    "for calm_convo_id, awry_convo_id in tqdm(human_summary_pair):\n",
    "    calm_human_summary = get_human_summary(corpus, calm_convo_id)\n",
    "    awry_human_summary = get_human_summary(corpus, awry_convo_id)\n",
    "    calm_query = simulation_msg.format(topic=new_topic[calm_convo_id],trajectory_summary=calm_human_summary['summary_text']) #Adding new topic \n",
    "    calm_response = gpt_query(calm_query)\n",
    "    generated_transcripts_topic_shuffled[calm_convo_id] = calm_response\n",
    "    awry_query = simulation_msg.format(topic=new_topic[awry_convo_id],trajectory_summary=awry_human_summary['summary_text']) #Adding new topic\n",
    "    awry_response = gpt_query(awry_query)\n",
    "    generated_transcripts_topic_shuffled[awry_convo_id] = awry_response\n",
    "\n",
    "output = {}\n",
    "for convo_id in generated_transcripts_topic_shuffled:\n",
    "    output[convo_id] = {\n",
    "        'generated_transcript': generated_transcripts_topic_shuffled[convo_id],\n",
    "        'topic': new_topic[convo_id]\n",
    "    }\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations_topic_shuffled.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95aecba",
   "metadata": {},
   "source": [
    "## Writing SCDs and SoPs\n",
    "\n",
    "Now we generate the Summaries of Conversational Dynamics (SCDs) and extracts their corresponding Sequences of Patterns (SoPs), which are required inputs for computing the ConDynS score. The SCDs provide high-level abstractions of conversational flow, while the SoPs capture the ordered interaction patterns needed for alignment. These representations are prepared for both real and simulated conversations to ensure consistency during the validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb20680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.convo_similarity.summary import SCDWriter\n",
    "scd_writer_gpt = SCDWriter(model_provider=\"gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a161155",
   "metadata": {},
   "outputs": [],
   "source": [
    "scd = {}\n",
    "bulletpoints = {}\n",
    "for convo_id in tqdm(pair_of):\n",
    "    summary = scd_writer_gpt.get_scd_summary(\"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id)))\n",
    "    scd[convo_id] = summary\n",
    "    bulletpoints[convo_id] = scd_writer_gpt.get_sop_from_summary(summary)\n",
    "\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/scd_og.json\", 'w') as f:\n",
    "    json.dump(scd, f, indent=4)\n",
    "\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/sop_og.json\", 'w') as f:\n",
    "    json.dump(bulletpoints, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce24e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations.json\", \"r\") as f:\n",
    "    simulated_transcripts = json.load(f)\n",
    "\n",
    "scd = {}\n",
    "bulletpoints = {}\n",
    "for convo_id in tqdm(pair_of):\n",
    "    summary = scd_writer_gpt.get_scd_summary(simulated_transcripts[convo_id]['generated_transcript'])\n",
    "    scd[convo_id] = summary\n",
    "    bulletpoints[convo_id] = scd_writer_gpt.get_sop_from_summary(summary)\n",
    "\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/scd_sim.json\", 'w') as f:\n",
    "    json.dump(scd, f, indent=4)\n",
    "\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/sop_sim.json\", 'w') as f:\n",
    "    json.dump(bulletpoints, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e678550",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations_topic_shuffled.json\", \"r\") as f:\n",
    "    simulated_transcripts_topic_shuffled = json.load(f)\n",
    "\n",
    "scd = {}\n",
    "bulletpoints = {}\n",
    "for convo_id in tqdm(pair_of):\n",
    "    summary = scd_writer_gpt.get_scd_summary(simulated_transcripts_topic_shuffled[convo_id]['generated_transcript'])\n",
    "    scd[convo_id] = summary\n",
    "    bulletpoints[convo_id] = scd_writer_gpt.get_sop_from_summary(summary)\n",
    "\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/scd_sim_topic_shuffled.json\", 'w') as f:\n",
    "    json.dump(scd, f, indent=4)\n",
    "\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/sop_sim_topic_shuffled.json\", 'w') as f:\n",
    "    json.dump(bulletpoints, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029b312",
   "metadata": {},
   "source": [
    "## Compute ConDynS Score\n",
    "\n",
    "Finally, we are now ready to compute the ConDynS scores between conversation pairs. Using the SoP from one conversation and the transcript of the other, we apply the alignment procedure described in the paper to quantify how similar their dynamics are.\n",
    "\n",
    "In this validation of our ConDynS measure, we compare ConDynS scores for each triplet (anchor, positive, negative, introduced in Section 5). The metric is expected to assign a higher similarity score to the anchor–positive pair (which shares dynamics) than to the anchor–negative pair (which differs in dynamics). Accuracy is computed as the proportion of triplets where this condition holds. As reported in Table 1 of the paper, ConDynS achieves substantially higher accuracy than baseline methods across same-topic, different-topic, and adversarial-topic conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b493eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.convo_similarity.condyns import ConDynS\n",
    "condyns_gpt = ConDynS(model_provider=\"gpt\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b032bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_condyns_results(self_scores, pair_scores):\n",
    "    performance = []\n",
    "    for score1, score2 in zip(self_scores, pair_scores):\n",
    "        performance.append(score1 > score2)\n",
    "    print(\"Accuracy:\",sum(performance) / len(performance), f\"for {len(performance)} pairs\")\n",
    "    print(stats.wilcoxon(self_scores, pair_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load SCDs and SoPs ###\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/scd_og.json\", \"r\") as f:\n",
    "    scd_og = json.load(f)\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/sop_og.json\", \"r\") as f:\n",
    "    sop_og = json.load(f)\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/scd_sim.json\", \"r\") as f:\n",
    "    scd_sim = json.load(f)\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/sop_sim.json\", \"r\") as f:\n",
    "    sop_sim = json.load(f)\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/scd_sim_topic_shuffled.json\", \"r\") as f:\n",
    "    scd_sim_topic_shuffled = json.load(f)\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/sop_sim_topic_shuffled.json\", \"r\") as f:\n",
    "    sop_sim_topic_shuffled = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute ConDynS with simulated transcripts ###\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations.json\", \"r\") as f:\n",
    "    simulated_transcripts = json.load(f)\n",
    "\n",
    "self_scores = []\n",
    "self_results = {}\n",
    "\n",
    "for convo_id in tqdm(pair_of):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id))\n",
    "    transcript2 = simulated_transcripts[convo_id]['generated_transcript']\n",
    "    sop1 = sop_og[convo_id]\n",
    "    sop2 = sop_sim[convo_id]\n",
    "    results = condyns_gpt.compute_bidirectional_similarity(transcript1, transcript2, sop1, sop2)\n",
    "    self_results[convo_id] = results\n",
    "    self_scores.append(condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "pair_scores = []\n",
    "pair_results = {}\n",
    "for convo_id in tqdm(pair_of):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id))\n",
    "    transcript2 = simulated_transcripts[pair_of[convo_id]]['generated_transcript']\n",
    "    sop1 = sop_og[convo_id]\n",
    "    sop2 = sop_sim[pair_of[convo_id]]\n",
    "    results = condyns_gpt.compute_bidirectional_similarity(transcript1, transcript2, sop1, sop2)\n",
    "    pair_results[convo_id] = results\n",
    "    pair_scores.append(condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "output = {\"self\" : self_results, \"pair\" : pair_results}\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/condyns_og-sim.json\", 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "evaluate_condyns_results(self_scores, pair_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute ConDynS with topic shuffled simulated transcripts ###\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations_topic_shuffled.json\", \"r\") as f:\n",
    "    simulated_transcripts_topic_shuffled = json.load(f)\n",
    "\n",
    "self_scores = []\n",
    "self_results = {}\n",
    "\n",
    "for convo_id in tqdm(pair_of):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id))\n",
    "    transcript2 = simulated_transcripts_topic_shuffled[convo_id]['generated_transcript']\n",
    "    scd1 = sop_og[convo_id]\n",
    "    scd2 = sop_sim_topic_shuffled[convo_id]\n",
    "    results = condyns_gpt.compute_bidirectional_similarity(transcript1, transcript2, scd1, scd2)\n",
    "    self_results[convo_id] = results\n",
    "    self_scores.append(condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "pair_scores = []\n",
    "pair_results = {}\n",
    "for convo_id in tqdm(pair_of):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id))\n",
    "    transcript2 = simulated_transcripts_topic_shuffled[pair_of[convo_id]]['generated_transcript']\n",
    "    scd1 = sop_og[convo_id]\n",
    "    scd2 = sop_sim_topic_shuffled[pair_of[convo_id]]\n",
    "    results = condyns_gpt.compute_bidirectional_similarity(transcript1, transcript2, scd1, scd2)\n",
    "    pair_results[convo_id] = results\n",
    "    pair_scores.append(condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "output = {\"self\" : self_results, \"pair\" : pair_results}\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/condyns_og-sim_topic_shuffled.json\", 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "evaluate_condyns_results(self_scores, pair_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd6560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute ConDynS with Adversarial simulated transcripts ###\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/condyns_og-sim.json\", 'r') as f:\n",
    "    sim_results = json.load(f)\n",
    "\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/condyns_og-sim_topic_shuffled.json\", 'r') as f:\n",
    "    topic_shuffle_results = json.load(f)\n",
    "\n",
    "self_results = topic_shuffle_results['self']\n",
    "pair_results = sim_results['pair']\n",
    "\n",
    "self_scores = []\n",
    "for convo_id in self_results:\n",
    "    results = self_results[convo_id]\n",
    "    self_scores.append(condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "pair_scores = []\n",
    "for convo_id in pair_results:\n",
    "    results = pair_results[convo_id]\n",
    "    pair_scores.append(condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "evaluate_condyns_results(self_scores, pair_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0332c40",
   "metadata": {},
   "source": [
    "## ConDynS SoP to SoP Alignment\n",
    "\n",
    "Here we also include ConDynS computation with SoP-to-SoP alignment that is presented in the paper, where both conversations use their pattern sequences. This keeps order information but can miss overlapping patterns. This can serve as a comparison to highlight ConDynS’s benefit of combining SoP precision with transcript recall.\n",
    "\n",
    "Notice in the following code, we call it Naive ConDynS, because it is more \"naive\" comparing to our advanced ConDynS above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit.convo_similarity.naive_condyns import NaiveConDynS\n",
    "naive_condyns_gpt = NaiveConDynS(model_provider=\"gpt\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b921ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute NaiveConDynS with simulated transcripts ###\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations.json\", \"r\") as f:\n",
    "    simulated_transcripts = json.load(f)\n",
    "\n",
    "self_scores = []\n",
    "self_results = {}\n",
    "\n",
    "for convo_id in tqdm(pair_of):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id))\n",
    "    transcript2 = simulated_transcripts[convo_id]['generated_transcript']\n",
    "    sop1 = sop_og[convo_id]\n",
    "    sop2 = sop_sim[convo_id]\n",
    "    results = naive_condyns_gpt.compute_bidirectional_naive_condyns(transcript1, transcript2, sop1, sop2)\n",
    "    self_results[convo_id] = results\n",
    "    self_scores.append(naive_condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "pair_scores = []\n",
    "pair_results = {}\n",
    "for convo_id in tqdm(pair_of):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id))\n",
    "    transcript2 = simulated_transcripts[pair_of[convo_id]]['generated_transcript']\n",
    "    sop1 = sop_og[convo_id]\n",
    "    sop2 = sop_sim[pair_of[convo_id]]\n",
    "    results = naive_condyns_gpt.compute_bidirectional_naive_condyns(transcript1, transcript2, sop1, sop2)\n",
    "    pair_results[convo_id] = results\n",
    "    pair_scores.append(naive_condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "output = {\"self\" : self_results, \"pair\" : pair_results}\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/naive_condyns_og-sim.json\", 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "evaluate_condyns_results(self_scores, pair_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute NaiveConDynS with topic shuffled simulated transcripts ###\n",
    "with open(ARTEFACTS_DIR + \"validation_gpt/transcript_simulations_topic_shuffled.json\", \"r\") as f:\n",
    "    simulated_transcripts_topic_shuffled = json.load(f)\n",
    "\n",
    "self_scores = []\n",
    "self_results = {}\n",
    "\n",
    "for convo_id in tqdm(pair_of):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id))\n",
    "    transcript2 = simulated_transcripts_topic_shuffled[convo_id]['generated_transcript']\n",
    "    sop1 = sop_og[convo_id]\n",
    "    sop2 = sop_sim_topic_shuffled[convo_id]\n",
    "    results = naive_condyns_gpt.compute_bidirectional_naive_condyns(transcript1, transcript2, sop1, sop2)\n",
    "    self_results[convo_id] = results\n",
    "    self_scores.append(naive_condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "pair_scores = []\n",
    "pair_results = {}\n",
    "for convo_id in tqdm(pair_of):\n",
    "    transcript1 = \"\\n\\n\".join(format_transcript_from_convokit(corpus, convo_id))\n",
    "    transcript2 = simulated_transcripts_topic_shuffled[pair_of[convo_id]]['generated_transcript']\n",
    "    sop1 = sop_og[convo_id]\n",
    "    sop2 = sop_sim_topic_shuffled[pair_of[convo_id]]\n",
    "    results = naive_condyns_gpt.compute_bidirectional_naive_condyns(transcript1, transcript2, sop1, sop2)\n",
    "    pair_results[convo_id] = results\n",
    "    pair_scores.append(naive_condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "output = {\"self\" : self_results, \"pair\" : pair_results}\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/naive_condyns_og-sim_topic_shuffled.json\", 'w') as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "evaluate_condyns_results(self_scores, pair_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute NaiveConDynS with Adversarial simulated transcripts ###\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/naive_condyns_og-sim.json\", 'r') as f:\n",
    "    sim_results = json.load(f)\n",
    "\n",
    "with open(ARTEFACTS_DIR + f\"validation_gpt/naive_condyns_og-sim_topic_shuffled.json\", 'r') as f:\n",
    "    topic_shuffle_results = json.load(f)\n",
    "\n",
    "self_results = topic_shuffle_results['self']\n",
    "pair_results = sim_results['pair']\n",
    "\n",
    "self_scores = []\n",
    "for convo_id in self_results:\n",
    "    results = self_results[convo_id]\n",
    "    self_scores.append(naive_condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "pair_scores = []\n",
    "for convo_id in pair_results:\n",
    "    results = pair_results[convo_id]\n",
    "    pair_scores.append(naive_condyns_gpt.compute_score_from_results(results))\n",
    "\n",
    "evaluate_condyns_results(self_scores, pair_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convokit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
