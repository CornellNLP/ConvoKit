{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4085d1b5",
   "metadata": {},
   "source": [
    "# Applications with ConDynS\n",
    "\n",
    "This notebook demonstrates how to apply ConDynS to analyze conversational dynamics, replicating the application results discussed in Section 6 of our [paper: A Similarity Measure for Comparing Conversational Dynamics](https://arxiv.org/abs/2507.18956). It includes examples of clustering conversations, comparing inter- and intra-group similarity, and examining which speaker drives the conversation’s dynamics. The notebook serves as a quick reference for using the metric on other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a225a-9b45-4043-bf69-3a8c24d2fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import re, os, random, time, math, json, string\n",
    "import scipy.stats as stats\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "\n",
    "from convokit.genai.genai_config import GenAIConfigManager\n",
    "from convokit.convo_similarity.summary import SCDWriter\n",
    "from convokit.convo_similarity.condyns import ConDynS\n",
    "\n",
    "random.seed(4300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup path for data and corpus ###\n",
    "DATA_PATH = \"./data\"\n",
    "\n",
    "### Select which model provider to use for ConDynS ###\n",
    "MODLE_PROVIDER = \"gemini\"\n",
    "\n",
    "### Set up config for GenAI ###\n",
    "config = GenAIConfigManager() ### make sure to set your own config if this is never set before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a1ff3",
   "metadata": {},
   "source": [
    "### Experiment Setup\n",
    "\n",
    "We first focus on conversations from the ChangeMyView sub-Reddit. We need to annotate delta information for conversations to prepare for conversation selections later. We would also define methods to filter out invalid conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04756fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=download(\"subreddit-changemyview\", data_dir=DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede2639-1f73-4f9f-802a-44424f247e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annotate Corpus with Delta Information Based on DeltaBot ###\n",
    "for utt in tqdm(corpus.iter_utterances()):\n",
    "    if (\n",
    "        utt.reply_to is not None\n",
    "        and utt.speaker.id == \"DeltaBot\"\n",
    "        and \"delta awarded\" in utt.text\n",
    "    ):\n",
    "        deltabot_text = utt.text\n",
    "        match = re.search(\n",
    "            r\"(?:Confirmed: 1 delta awarded to )(?:\\/)?(?:u\\/)([\\w-]+)\", deltabot_text\n",
    "        )\n",
    "        if match is not None:\n",
    "            try:\n",
    "                delta_utt = corpus.get_utterance(utt.reply_to)\n",
    "                delta_utt.meta['got_delta'] = True\n",
    "            except KeyError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0a9c6-8c4b-4409-b0bf-c042cb33c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove Speakers that has delta / # convo ratio > 1 and had more than 10000 conversations (likely bots) ###\n",
    "invalid_speakers = ['Cou', 'rightplacewr0ngtime', 'ThursdayTrashTyrant', 'UggoJesus', 'WideLight', 'urge_to_merge', 'LukeJovanovic'] + ['hacksoncode', 'Ansuz07', 'tbdabbholm'] \n",
    "### Remove Convos that contain toxic content which prevents model from processing ###\n",
    "toxic_convos = ['unbvfc_d', 'cvs2dx_c', 'brshwd_d', 'vhmoje_b', 'qdd7k1_b', '3mhbgl_d', 'xr0bxq_b']\n",
    "\n",
    "### Controlling for the length of the conversation ###\n",
    "# def is_correct_length_convo(utt_lst, low_lim=5, high_lim=7):\n",
    "#     num_of_utt = len(utt_lst)\n",
    "#     return num_of_utt >= low_lim and num_of_utt <= high_lim\n",
    "def is_correct_length_convo(utt_lst, low_lim=5):\n",
    "    return len(utt_lst) >= low_lim\n",
    "\n",
    "### Controlling for the number of speakers ###\n",
    "def is_two_speaker_convo(utt_lst):\n",
    "    speaker_lst = []\n",
    "    for utt in utt_lst:\n",
    "        if utt.speaker.id not in speaker_lst:\n",
    "            speaker_lst.append(utt.speaker.id)\n",
    "        if len(speaker_lst) > 2:\n",
    "            return False\n",
    "    return len(speaker_lst) == 2\n",
    "\n",
    "### Filtering out invalid convos ###\n",
    "def is_valid_convo(convo, utt_lst, invalid_sp=invalid_speakers, toxic_convo=toxic_convos):\n",
    "    if convo.id in toxic_convo or not is_correct_length_convo(utt_lst) or not is_two_speaker_convo(utt_lst):\n",
    "        return False\n",
    "\n",
    "    for utt in utt_lst:\n",
    "        if utt.speaker.id in invalid_sp:\n",
    "            return False\n",
    "    for utt in utt_lst:\n",
    "        if utt.text == \"[deleted]\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "### Helper functions ###\n",
    "def get_all_speakers(utt_lst):\n",
    "    speaker_lst = []\n",
    "    for utt in utt_lst:\n",
    "        speaker_lst.append(utt.speaker.id)\n",
    "    return speaker_lst\n",
    "\n",
    "def get_convo_year(utt_lst):\n",
    "    timestamp = utt_lst[1].meta['retrieved_on']\n",
    "    time = datetime.utcfromtimestamp(timestamp)\n",
    "    if time.year > 2014:\n",
    "        return time.year\n",
    "    else:\n",
    "        for utt in utt_lst:\n",
    "            timestamp = utt.meta['retrieved_on']\n",
    "            time = datetime.utcfromtimestamp(timestamp)\n",
    "            if time.year > 2014:\n",
    "                return time.year\n",
    "        return None\n",
    "\n",
    "def read_convo(utt_lst):\n",
    "    for utt in utt_lst:\n",
    "        print(f\"{utt.speaker.id} : {utt.text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ead648-69c4-4db7-829a-ab3c06498e4b",
   "metadata": {},
   "source": [
    "### Select Conversations From Year of 2018\n",
    "\n",
    "Our experiments are conducted on Reddit Data from 2018 so there is no LLM influence on contents. Now we filter out valid conversations from 2018, and select conversations from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e7e21-f83b-4aa4-994c-1502e94247c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtering out invalid conversations ###\n",
    "random.seed(4300)\n",
    "valid_convos = {}\n",
    "for convo in tqdm(corpus.iter_conversations()):\n",
    "    try:\n",
    "        all_convos = convo.get_root_to_leaf_paths()\n",
    "    except ValueError:\n",
    "        continue\n",
    "    cur_convo_valid_utts = []\n",
    "    for utt_lst in all_convos:\n",
    "        if is_valid_convo(convo, utt_lst):\n",
    "            cur_convo_valid_utts.append(utt_lst)\n",
    "    if cur_convo_valid_utts:\n",
    "        cur_convo_utt_lst = random.choice(cur_convo_valid_utts)\n",
    "        valid_convos[convo.id] = cur_convo_utt_lst\n",
    "\n",
    "print(\"Number of valid convos:\", len(valid_convos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc35b9e-b4cb-49cd-b994-a16766477590",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get year 2018 conversations ###\n",
    "convo_to_year = {'2018' : []}\n",
    "for convo_id, utt_lst in tqdm(valid_convos.items()):\n",
    "    convo_year = get_convo_year(utt_lst)\n",
    "    for year, _ in convo_to_year.items():\n",
    "        if convo_year == int(year):\n",
    "            convo_to_year[year].append(convo_id)\n",
    "            break\n",
    "\n",
    "for year, convos in convo_to_year.items():\n",
    "    print(f\"{year}: {len(convos)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e4e06-f4d9-4926-89b0-c0af2d596de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select conversations from 2018 ###\n",
    "random.seed(4300)\n",
    "K = 220\n",
    "used_speakers = []\n",
    "selected_convos_each_year = {'2018' : []}\n",
    "for year, convo_ids in tqdm(convo_to_year.items()):\n",
    "    random.shuffle(convo_ids)\n",
    "    for convo_id in convo_ids:\n",
    "        convo_speakers = get_all_speakers(valid_convos[convo_id])\n",
    "        \n",
    "        if all(s not in used_speakers for s in convo_speakers):\n",
    "            selected_convos_each_year[year].append(convo_id)\n",
    "            used_speakers.extend(convo_speakers)\n",
    "        \n",
    "        if len(selected_convos_each_year[year]) == K:\n",
    "            break\n",
    "\n",
    "for year, convos in selected_convos_each_year.items():\n",
    "    print(f\"{year}: {len(convos)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9be6ca-fccc-472d-8896-a4d1a6d2619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constructing the corpus for selected conversations ###\n",
    "all_utterances = []\n",
    "for utt_lst in convo_2018:\n",
    "    all_utterances.extend(utt_lst)\n",
    "\n",
    "selected_corpus = Corpus(utterances=all_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2bab74-a942-4d8d-9bbb-f1735b62eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annotating the new corpus with metadata information ###\n",
    "for convo in tqdm(selected_corpus.iter_conversations()):\n",
    "    delta_convo = False\n",
    "    for utt in convo.iter_utterances():\n",
    "        if 'got_delta' in utt.meta and utt.meta['got_delta']:\n",
    "            delta_convo = True\n",
    "    og_convo = corpus.get_conversation(convo.id)\n",
    "    convo.meta = og_convo.meta\n",
    "    convo.meta['year'] = '2018'\n",
    "    convo.meta['has_delta'] = delta_convo\n",
    "\n",
    "for sp in tqdm(selected_corpus.iter_speakers()):\n",
    "    og_sp = corpus.get_speaker(sp.id)\n",
    "    sp.meta = og_sp.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba2710-4f2c-4b4a-9914-b24c4d5a0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_corpus.dump(\"cmv_selected_convos_2018\", base_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643d61a-0460-4da0-a48f-ce8962c1c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + \"cmv_selected_convo_ids_2018.json\", \"w\") as f:\n",
    "    json.dump(selected_convos_each_year['2018'], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef74a7-e042-4082-9055-750532817507",
   "metadata": {},
   "source": [
    "## ConDynS Computation - Random 2018 Set\n",
    "\n",
    "In this section, we compute ConDynS scores for a randomly selected set of 2018 conversations. These scores form the basis for the subsequent analyses, including clustering and group-level comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e2c10-9f0f-4748-b65f-14682f30724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./artefacts\"\n",
    "CUR_ANALYSIS = \"random_set_2018\"\n",
    "\n",
    "if not os.path.exists(f\"{BASE_PATH}\"):\n",
    "    print(\"Making directory: \", f\"{BASE_PATH}\")\n",
    "    os.makedirs(f\"{BASE_PATH}\")\n",
    "\n",
    "if not os.path.exists(f\"{BASE_PATH}/{CUR_ANALYSIS}/\"):\n",
    "    print(\"Making directory: \", f\"{BASE_PATH}/{CUR_ANALYSIS}/\")\n",
    "    os.makedirs(f\"{BASE_PATH}/{CUR_ANALYSIS}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f1a75-d644-4883-89cd-df6ec889a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + \"cmv_selected_convo_ids_2018.json\", \"r\") as f:\n",
    "    convo_2018 = json.load(f)\n",
    "\n",
    "corpus = Corpus(filename=DATA_PATH + \"cmv_selected_convos_2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e24ddb-1244-41c9-b190-394c561f7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cur_convo_transcript(corpus, convo_id):\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    utt_list = convo.get_chronological_utterance_list()\n",
    "    transcription = []\n",
    "    spk_list = {utt_list[0].speaker.id : \"SPEAKER1\"}\n",
    "    for utt in utt_list:\n",
    "        if utt.speaker.id not in spk_list.keys():\n",
    "            spk_list[utt.speaker.id] = \"SPEAKER2\"\n",
    "            assert len(spk_list) == 2\n",
    "        transcription.append(spk_list[utt.speaker.id] +\": \"+utt.text)\n",
    "    transcription = transcription[1:] # remove first OP message\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66277f",
   "metadata": {},
   "source": [
    "Here, we initialize our modules for writing the SCDs and computing ConDynS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623734f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the modules for ConDynS ###\n",
    "scd_writer = SCDWriter(model_provider=MODLE_PROVIDER, config=config)\n",
    "condyns = ConDynS(model_provider=MODLE_PROVIDER, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5966d3-330d-4d2a-921c-59b8a3a9c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_within_group_similarity(all_convos, summaries_and_bullets, similarity_result, incomplete, data_path):\n",
    "    \"\"\"\n",
    "    Compute the similarity between all conversations in the same group of conversations using ConDynS.\n",
    "    This function is build with parallelization in mind to speed up the computation.\n",
    "    \"\"\"\n",
    "    all_convos_combos = list(combinations(all_convos, 2))\n",
    "    len(all_convos_combos)\n",
    "    \n",
    "    # ### Calling GPT to create the SCD and bullet points. Run with caution.\n",
    "    summaries_and_bullets = summaries_and_bullets\n",
    "    incomplete = incomplete\n",
    "    similarity_result = similarity_result\n",
    "    \n",
    "    for convo_id in tqdm(all_convos, desc=\"generating SCDs\"):\n",
    "        if convo_id in summaries_and_bullets.keys(): continue\n",
    "        summary, bulletpoint = scd_writer.get_scd_and_sop(corpus, convo_id)\n",
    "        summaries_and_bullets.update({convo_id : {\"summary\" : summary, \"bulletpoint\" : bulletpoint}})\n",
    "    with open(f\"{data_path}summary.json\", \"w\") as file:\n",
    "        json.dump(summaries_and_bullets, file, indent=4)\n",
    "\n",
    "    def get_bidirection_similarity_with_retry(corpus, convo1_id, convo2_id, summaries_and_bullets, retries=10):\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                result = condyns.compute_bidirectional_similarity(corpus, convo1_id, convo2_id, summaries_and_bullets)\n",
    "                score = condyns.compute_score_from_results(result)\n",
    "                return score, result\n",
    "            except Exception as e:\n",
    "                wait = 0.5 ** i + random.random()\n",
    "                print(f\"Retrying ({convo1_id}, {convo2_id}) after {wait:.2f}s due to error: {e}\")\n",
    "                incomplete.append(f'{convo1_id}_{convo2_id}')\n",
    "                time.sleep(wait)\n",
    "        return None, None\n",
    "    \n",
    "    # The thread worker function\n",
    "    def worker(corpus, convo1_id, convo2_id, summaries_and_bullets):\n",
    "        score, result = get_bidirection_similarity_with_retry(corpus, convo1_id, convo2_id, summaries_and_bullets)\n",
    "        return (f'{convo1_id}_{convo2_id}', {\"score\": score, \"result\": result})\n",
    "\n",
    "    MAX_WORKER = 50\n",
    "    \n",
    "    # Build task list\n",
    "    tasks = [(corpus, id1, id2, summaries_and_bullets) \n",
    "             for id1, id2 in all_convos_combos \n",
    "             if f'{id1}_{id2}' not in similarity_result and f'{id1}_{id2}' not in incomplete and f'{id2}_{id1}' not in similarity_result and f'{id2}_{id1}' not in incomplete]\n",
    "        \n",
    "    # Set up thread pool\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKER) as executor:\n",
    "        futures = [executor.submit(worker, *task) for task in tasks]\n",
    "    \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Computing Pair-wise Similarity\"):\n",
    "            key, value = future.result()\n",
    "            if key in similarity_result.keys():\n",
    "                print(\"not good, repeated keys\")\n",
    "            if key not in incomplete:\n",
    "                similarity_result[key] = value\n",
    "\n",
    "    with open(f\"{data_path}similarity.json\", \"w\") as file:\n",
    "        json.dump(similarity_result, file, indent=4)\n",
    "\n",
    "    with open(f\"{data_path}incomplete.json\", \"w\") as file:\n",
    "        json.dump(incomplete, file, indent=4)\n",
    "\n",
    "    return summaries_and_bullets, similarity_result, incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95bcc5-764d-4f87-a4e7-9455009e7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute within-group similarity using ConDynS on selected 2018 conversations ###\n",
    "all_convos = convo_2018\n",
    "data_path = f\"{BASE_PATH}/{CUR_ANALYSIS}/\"\n",
    "\n",
    "if os.path.exists(f\"{data_path}summary.json\"):\n",
    "    with open(f\"{data_path}summary.json\", \"r\") as file:\n",
    "        summaries_and_bullets = json.load(file)\n",
    "else:\n",
    "    summaries_and_bullets = {}\n",
    "\n",
    "if os.path.exists(f\"{data_path}similarity.json\"):\n",
    "    with open(f\"{data_path}similarity.json\", \"r\") as file:\n",
    "        similarity_result = json.load(file)\n",
    "else:\n",
    "    similarity_result = {}\n",
    "\n",
    "incomplete = []\n",
    "\n",
    "summaries_and_bullets, similarity_result, incomplete = compute_within_group_similarity(all_convos, summaries_and_bullets, similarity_result, incomplete, data_path)\n",
    "\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739ecd4-daa3-4564-b8f0-27a8f27acf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract problematic conversations from the computation, expected to be [] ###\n",
    "def extract_ids(id_list):\n",
    "    id_count = {}\n",
    "    for item in id_list:\n",
    "        parts = item.split('_')\n",
    "        mid = len(parts) // 2\n",
    "        id1 = '_'.join(parts[:mid])\n",
    "        id2 = '_'.join(parts[mid:])\n",
    "\n",
    "        for id in [id1, id2]:\n",
    "            if id not in id_count.keys():\n",
    "                id_count[id] = 0\n",
    "            id_count[id] += 1\n",
    "    return id_count\n",
    "problem_convos = [id for id, count in extract_ids(incomplete).items()]\n",
    "problem_convos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de468801-1fa8-4e14-84ff-f47ac177861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_incomplete = [idx for idx in convo_2018 if idx not in problem_convos]\n",
    "random.seed(4300)\n",
    "convo_2018 = random.sample(after_incomplete, 200)\n",
    "print(len(convo_2018))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246a73d-498a-48aa-8301-db479b26d93e",
   "metadata": {},
   "source": [
    "## Analysis - Clustering\n",
    "\n",
    "We use the computed ConDynS scores to cluster the conversation set into two groups and analyze their distinguishing interaction patterns (via fighting words) and the distribution of persuasion outcomes (Δ awards) across clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177031de-2d2c-4312-88b6-a37588a8a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + \"cmv_selected_convo_ids_2018.json\", \"r\") as f:\n",
    "    convo_2018 = json.load(f)\n",
    "\n",
    "all_convos = convo_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e3e17-a842-4ecd-9e1e-c5abcf28b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verify all similarity are computed valid here ###\n",
    "with open(f\"{data_path}summary.json\", \"r\") as file:\n",
    "    summaries_and_bullets = json.load(file)\n",
    "for convo_id in all_convos:\n",
    "    assert convo_id in summaries_and_bullets\n",
    "\n",
    "with open(f\"{data_path}similarity.json\", \"r\") as file:\n",
    "    similarity_result = json.load(file)\n",
    "all_convos_combos = list(combinations(convo_2018, 2))\n",
    "for id1, id2 in all_convos_combos:\n",
    "    assert f'{id1}_{id2}' in similarity_result or f'{id2}_{id1}' in similarity_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651573c1-c489-420d-a26b-629af14840bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions to read in the results ###\n",
    "def get_similarity(convo1, convo2, all_convos, sim_score):\n",
    "    if convo1 not in all_convos or convo2 not in all_convos:\n",
    "        raise Exception(\"convo not in selected convo\")\n",
    "    key = f\"{convo1}_{convo2}\" if f\"{convo1}_{convo2}\" in sim_score.keys() else f\"{convo2}_{convo1}\"\n",
    "    score = sim_score[key]['score']\n",
    "    return score\n",
    "\n",
    "def compute_intra_group_similarity(group, sim_scores):\n",
    "    similarities = []\n",
    "    for i in range(len(group)):\n",
    "        for j in range(i + 1, len(group)):\n",
    "            sim = np.mean(get_similarity(group[i], group[j], group, sim_scores))\n",
    "            similarities.append(sim)\n",
    "    return np.array(similarities)\n",
    "\n",
    "def two_groups_intra_group_similarity_check(group1, group2, group1_name, group2_name, all_convos, sim_score, y_lim=4):\n",
    "    group1_similarities = compute_intra_group_similarity(group1, sim_score)\n",
    "    group2_similarities = compute_intra_group_similarity(group2, sim_score)\n",
    "    \n",
    "    group1_mean = np.mean(group1_similarities)\n",
    "    group2_mean = np.mean(group2_similarities)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(group1_similarities, bins=20, alpha=0.5, color='blue', label=f'{group1_name} convos', density=False)\n",
    "    plt.hist(group2_similarities, bins=20, alpha=0.5, color='red', label=f'{group2_name} convos', density=False)\n",
    "    \n",
    "    plt.xlabel(\"similarity score\")\n",
    "    plt.ylabel(\"number of conversation pairs\")\n",
    "    # plt.title(f\"Distribution of Similarity Scores for {group1_name} and {group2_name}\")\n",
    "    plt.legend()\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, y_lim)\n",
    "    # plt.savefig(\"within-group-sim.png\") ### Save figure for submissions\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Compare distributions with statistical tests\n",
    "    t_stat, p_value = stats.ttest_ind(group1_similarities, group2_similarities, equal_var=False)\n",
    "    levene_stat, levene_p = stats.levene(group1_similarities, group2_similarities)\n",
    "    statistic, p_value = mannwhitneyu(group1_similarities, group2_similarities, alternative='two-sided')\n",
    "\n",
    "    print(f\"Mann-Whitney U statistic = {statistic}, p-value = {p_value:.5f}\")\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{group1_name} Mean Similarity: {group1_mean:.4f}\")\n",
    "    print(f\"{group2_name} Mean Similarity: {group2_mean:.4f}\")\n",
    "    print(f\"T-Test p-value (Are means different?): {p_value:.4f}\")\n",
    "    print(f\"Levene's Test p-value (Is variance different?): {levene_p:.4f}\")\n",
    "\n",
    "def compute_between_group_similarity(group1, group2, all_convos, sim_score):\n",
    "    similarities = []\n",
    "    for convo1 in group1:\n",
    "        for convo2 in group2:\n",
    "            sim = np.mean(get_similarity(convo1, convo2, all_convos, sim_score))\n",
    "            similarities.append(sim)\n",
    "    return np.array(similarities)\n",
    "    \n",
    "def between_group_similarity_check(group1, group2, group1_name, group2_name, all_convos, sim_score, y_lim=4):\n",
    "    between_similarities = compute_between_group_similarity(group1, group2, all_convos, sim_score)\n",
    "    between_mean = np.mean(between_similarities)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(between_similarities, bins=20, alpha=0.7, color='purple', label=f'{group1_name} vs {group2_name}', density=False)\n",
    "    plt.xlabel(\"similarity score\")\n",
    "    plt.ylabel(\"number of conversation pairs\")\n",
    "    plt.title(f\"Between-Group Similarity: {group1_name} vs {group2_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, y_lim)\n",
    "    plt.show()\n",
    "    print(f\"Between-Group Mean Similarity ({group1_name} vs {group2_name}): {between_mean:.4f}\")\n",
    "\n",
    "def compute_intra_group_similarity_plot(group, group_name, sim_scores, y_lim=2000):\n",
    "    similarities = []\n",
    "    for i in range(len(group)):\n",
    "        for j in range(i + 1, len(group)):\n",
    "            sim = np.mean(get_similarity(group[i], group[j], group, sim_scores))\n",
    "            similarities.append(sim)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(np.array(similarities), bins=20, alpha=0.5, color='red', label=f'{group_name}', density=False)\n",
    "    plt.xlabel(\"similarity score\")\n",
    "    plt.ylabel(\"number of conversation pairs\")\n",
    "    # plt.title(f\"Distribution of Similarity Scores for {group1_name} and {group2_name}\")\n",
    "    plt.legend()\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, y_lim)\n",
    "    # plt.savefig(\"within-group-sim-random-set.png\") ### Save figure for submissions\n",
    "    print(np.mean(np.array(similarities)))\n",
    "    plt.show()\n",
    "    mean_val = np.mean(similarities)\n",
    "    median_val = np.median(similarities)\n",
    "    percentile_25 = np.percentile(similarities, 25)\n",
    "    percentile_75 = np.percentile(similarities, 75)\n",
    "    print(f\"{group_name} Similarity Stats:\")\n",
    "    print(f\"Mean:     {mean_val:.4f}\")\n",
    "    print(f\"Median:   {median_val:.4f}\")\n",
    "    print(f\"25th pct: {percentile_25:.4f}\")\n",
    "    print(f\"75th pct: {percentile_75:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cea4f7-a94e-42e4-b697-02afa0843a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_intra_group_similarity_plot(convo_2018, \"random set\", similarity_result, y_lim=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e74e7-14c8-401f-90d1-7dfede6b9cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions for clustering conversations ###\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def cluster_conversations(convo_ids, threshold=5):\n",
    "    \"\"\"\n",
    "    Performs hierarchical clustering on a list of conversation IDs based on pairwise similarity.\n",
    "    Parameters:\n",
    "    - convo_ids (list): List of conversation identifiers.\n",
    "    Returns:\n",
    "    - linkage_matrix (ndarray): Linkage matrix from hierarchical clustering.\n",
    "    \"\"\"\n",
    "    n = len(convo_ids)\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            convo1, convo2 = convo_ids[i], convo_ids[j]\n",
    "            try:\n",
    "                similarity = np.mean(get_similarity(convo1, convo2, convo_2018, similarity_result))\n",
    "                distance = 1 - similarity\n",
    "                distance_matrix[i, j] = distance_matrix[j, i] = distance\n",
    "            except Exception as e:\n",
    "                # distance_matrix[i, j] = distance_matrix[j, i] = 1  # Max distance if error\n",
    "                print(e)\n",
    "\n",
    "    condensed_dist_matrix = squareform(distance_matrix)\n",
    "    linkage_matrix = linkage(condensed_dist_matrix, method=\"ward\")\n",
    "\n",
    "    cluster_labels = fcluster(linkage_matrix, threshold, criterion='distance')\n",
    "\n",
    "    clusters = {}\n",
    "    for convo, label in zip(convo_ids, cluster_labels):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        clusters[label].append(convo)\n",
    "    \n",
    "    cluster_lists = list(clusters.values())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    dendrogram(linkage_matrix, labels=convo_ids, leaf_rotation=90)\n",
    "    plt.xlabel(\"Conversation ID\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.show()\n",
    "\n",
    "    # for i, cluster in enumerate(cluster_lists):\n",
    "    #     print(f\"Cluster {i+1}: {cluster}\")\n",
    "    \n",
    "    return linkage_matrix, cluster_lists\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# from https://github.com/jmhessel/FightingWords/blob/master/fighting_words_py3.py\n",
    "def basic_sanitize(in_string):\n",
    "    '''Returns a very roughly sanitized version of the input string.'''\n",
    "    in_string = ''.join([ch for ch in in_string if ch not in exclude])\n",
    "    in_string = in_string.lower()\n",
    "    in_string = ' '.join(in_string.split())\n",
    "    return in_string\n",
    "\n",
    "def bayes_compare_language(l1, l2, ngram = 1, prior=.01, cv = None):\n",
    "    '''\n",
    "    Arguments:\n",
    "    - l1, l2; a list of strings from each language sample\n",
    "    - ngram; an int describing up to what n gram you want to consider (1 is unigrams,\n",
    "    2 is bigrams + unigrams, etc). Ignored if a custom CountVectorizer is passed.\n",
    "    - prior; either a float describing a uniform prior, or a vector describing a prior\n",
    "    over vocabulary items. If you're using a predefined vocabulary, make sure to specify that\n",
    "    when you make your CountVectorizer object.\n",
    "    - cv; a sklearn.feature_extraction.text.CountVectorizer object, if desired.\n",
    "\n",
    "    Returns:\n",
    "    - A list of length |Vocab| where each entry is a (n-gram, zscore) tuple.'''\n",
    "    if cv is None and type(prior) is not float:\n",
    "        print(\"If using a non-uniform prior:\")\n",
    "        print(\"Please also pass a count vectorizer with the vocabulary parameter set.\")\n",
    "        quit()\n",
    "    l1 = [basic_sanitize(l) for l in l1]\n",
    "    l2 = [basic_sanitize(l) for l in l2]\n",
    "    if cv is None:\n",
    "        cv = CV(decode_error = 'ignore', min_df=2, max_df=0.9, ngram_range=(1,ngram),\n",
    "                binary = False,\n",
    "                max_features = 15000)\n",
    "    counts_mat = cv.fit_transform(l1+l2).toarray()\n",
    "    # Now sum over languages...\n",
    "    vocab_size = len(cv.vocabulary_)\n",
    "    print(\"Vocab size is {}\".format(vocab_size))\n",
    "    if type(prior) is float:\n",
    "        priors = np.array([prior for i in range(vocab_size)])\n",
    "    else:\n",
    "        priors = prior\n",
    "    z_scores = np.empty(priors.shape[0])\n",
    "    count_matrix = np.empty([2, vocab_size], dtype=np.float32)\n",
    "    count_matrix[0, :] = np.sum(counts_mat[:len(l1), :], axis = 0)\n",
    "    count_matrix[1, :] = np.sum(counts_mat[len(l1):, :], axis = 0)\n",
    "    a0 = np.sum(priors)\n",
    "    n1 = 1.*np.sum(count_matrix[0,:])\n",
    "    n2 = 1.*np.sum(count_matrix[1,:])\n",
    "    print(\"Comparing language...\")\n",
    "    for i in range(vocab_size):\n",
    "        #compute delta\n",
    "        term1 = np.log((count_matrix[0,i] + priors[i])/(n1 + a0 - count_matrix[0,i] - priors[i]))\n",
    "        term2 = np.log((count_matrix[1,i] + priors[i])/(n2 + a0 - count_matrix[1,i] - priors[i]))\n",
    "        delta = term1 - term2\n",
    "        #compute variance on delta\n",
    "        var = 1./(count_matrix[0,i] + priors[i]) + 1./(count_matrix[1,i] + priors[i])\n",
    "        #store final score\n",
    "        z_scores[i] = delta/np.sqrt(var)\n",
    "    index_to_term = {v:k for k,v in cv.vocabulary_.items()}\n",
    "    sorted_indices = np.argsort(z_scores)\n",
    "    return_list = []\n",
    "    for i in sorted_indices:\n",
    "        return_list.append((index_to_term[i], z_scores[i]))\n",
    "    return return_list\n",
    "\n",
    "def format_bullet_points(bullets):\n",
    "    text = \"\"\n",
    "    for idx, bullet in bullets.items():\n",
    "        text += f\"{idx} : {bullet}\\n\"\n",
    "    return text\n",
    "\n",
    "def get_machine_scd_and_bullets(convo_id, scd_dict=summaries_and_bullets):\n",
    "    summary = scd_dict[convo_id]['summary']\n",
    "    bullet = format_bullet_points(scd_dict[convo_id]['bulletpoint'])\n",
    "    return summary, bullet\n",
    "\n",
    "def get_fighting_words_convincers(cluster1, cluster2):\n",
    "    for lst in [cluster1, cluster2]:\n",
    "        for convo_id in lst:\n",
    "            summary, bullets = get_machine_scd_and_bullets(convo_id)\n",
    "            convo = corpus.get_conversation(convo_id)\n",
    "            convo.meta['new_machine_scd'] = summary\n",
    "            convo.meta['machine_bullet_points'] = bullets\n",
    "            sentences = sent_tokenize(summary)\n",
    "            convincer_sentences = [s for s in sentences if 'CONVINCER' in s]\n",
    "            convo.meta['new_convincer_text_machine_scd'] = \" \".join(convincer_sentences)\n",
    "            convo.meta['dict_bullet_points'] = dict(line.split(' : ', 1) for line in bullets.strip().split('\\n'))\n",
    "            convincer_bullets = \", \".join([s for _, s in convo.meta['dict_bullet_points'].items() if 'convincer' in s.lower()])\n",
    "            convo.meta['convincer_bullet_points'] = convincer_bullets\n",
    "    \n",
    "    cluster1_convincer_scds = []\n",
    "    cluster2_convincer_scds = []\n",
    "    for convo_id in cluster1:\n",
    "        convo = corpus.get_conversation(convo_id)\n",
    "        cluster1_convincer_scds.append(convo.meta['convincer_bullet_points'])\n",
    "    for convo_id in cluster2:\n",
    "        convo = corpus.get_conversation(convo_id)\n",
    "        cluster2_convincer_scds.append(convo.meta['convincer_bullet_points'])\n",
    "    \n",
    "    z_scores = bayes_compare_language(cluster1_convincer_scds, cluster2_convincer_scds, ngram = 3) \n",
    "    top_k = 10\n",
    "    top_k_class1 = list(reversed([(x[0], round(x[1],2)) for x in z_scores[-top_k:]]))\n",
    "    top_k_class2 = [(x[0], round(x[1],2)) for x in z_scores[:top_k]]\n",
    "    top_k_class1 = list(reversed([(x[0], round(x[1],2)) for x in z_scores[-top_k:]]))\n",
    "    top_k_class2 = [(x[0], round(x[1],2)) for x in z_scores[:top_k]]\n",
    "    print(f\"Fighting Words Comments between:\")\n",
    "    print(\"Cluster1: \", top_k_class1)\n",
    "    print(\"Cluster2: \", top_k_class2)\n",
    "\n",
    "def get_fighting_words_matching_bullets(cluster1, cluster2, similarity_result=similarity_result, summaries_and_bullets=summaries_and_bullets):\n",
    "    cluster1_combo = list(combinations(cluster1, 2))\n",
    "    matched_cluster1 = []\n",
    "    for convo_id1, convo_id2 in cluster1_combo:\n",
    "        key = f\"{convo_id1}_{convo_id2}\" if f\"{convo_id1}_{convo_id2}\" in similarity_result.keys() else f\"{convo_id2}_{convo_id1}\"\n",
    "        for k, result in enumerate(similarity_result[key]['result']):\n",
    "            for index in result:\n",
    "                if result[index]['score'] > 0.5:\n",
    "                    if k == 0:\n",
    "                        matched_cluster1.append(summaries_and_bullets[convo_id1]['bulletpoint'][index])\n",
    "                    else:\n",
    "                        try:\n",
    "                            matched_cluster1.append(summaries_and_bullets[convo_id2]['bulletpoint'][index])\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        \n",
    "    cluster2_combo = list(combinations(cluster2, 2))\n",
    "    matched_cluster2 = []\n",
    "    for convo_id1, convo_id2 in cluster2_combo:\n",
    "        key = f\"{convo_id1}_{convo_id2}\" if f\"{convo_id1}_{convo_id2}\" in similarity_result.keys() else f\"{convo_id2}_{convo_id1}\"\n",
    "        for k, result in enumerate(similarity_result[key]['result']):\n",
    "            for index in result:\n",
    "                if result[index]['score'] > 0.5:\n",
    "                    if k == 0:\n",
    "                        matched_cluster2.append(summaries_and_bullets[convo_id1]['bulletpoint'][index])\n",
    "                    else:\n",
    "                        matched_cluster2.append(summaries_and_bullets[convo_id2]['bulletpoint'][index])\n",
    "    \n",
    "    z_scores = bayes_compare_language(matched_cluster1, matched_cluster2, ngram = 3) \n",
    "    top_k = 15\n",
    "    top_k_class1 = list(reversed([(x[0], round(x[1],2)) for x in z_scores[-top_k:]]))\n",
    "    top_k_class2 = [(x[0], round(x[1],2)) for x in z_scores[:top_k]]\n",
    "    top_k_class1 = list(reversed([(x[0], round(x[1],2)) for x in z_scores[-top_k:]]))\n",
    "    top_k_class2 = [(x[0], round(x[1],2)) for x in z_scores[:top_k]]\n",
    "    print(f\"Fighting Words Comments between:\")\n",
    "    print(\"Cluster1: \", top_k_class1)\n",
    "    print(\"Cluster2: \", top_k_class2)\n",
    "    return matched_cluster1, matched_cluster2\n",
    "\n",
    "\n",
    "def find_keywords_in_lst_of_text(lst, keyword):\n",
    "    result = []\n",
    "    for text in lst:\n",
    "        if keyword.lower() in text.lower():\n",
    "            result.append(text)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def get_clusters_delta_percentage(lst_of_clusters):\n",
    "    for i, g in enumerate(lst_of_clusters):\n",
    "        temp = []\n",
    "        for idx in g:\n",
    "            if corpus.get_conversation(idx).meta['has_delta']:\n",
    "                temp.append(\"delta\")\n",
    "            else:\n",
    "                temp.append(\"no\")\n",
    "        print(f\"Cluster {i+1} (N={len(g)}): {temp.count('delta') / len(temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db126689-6872-4231-b36d-3cdbb1f5217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cluster conversations, and we notice one cluster contains much more delta convos than the other, percentage wise ###\n",
    "_, c_lst = cluster_conversations(convo_2018, threshold=2.5)\n",
    "get_clusters_delta_percentage(c_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be584fa-fd05-486c-bd13-88e2b37d6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_lst[0])\n",
    "print(len(c_lst[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf87e9b-905b-4484-b481-b7c37d800933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_lst[1])\n",
    "print(len(c_lst[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77c2ba-4094-4be3-8cce-b7da2832d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_2018_less_delta_set = c_lst[0]\n",
    "random_2018_more_delta_set = c_lst[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394c419-915a-4e5b-a856-846c1c256277",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + \"random_2018_less_delta_set.json\", \"w\") as f:\n",
    "    json.dump(random_2018_less_delta_set, f, indent=4)\n",
    "\n",
    "with open(DATA_PATH + \"random_2018_more_delta_set.json\", \"w\") as f:\n",
    "    json.dump(random_2018_more_delta_set, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed8590",
   "metadata": {},
   "source": [
    "#### FightingWords\n",
    "\n",
    "To interpret these clusters, we apply a fighting words analysis, identifying key words that most distinguish one cluster’s dynamics from the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a6791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1 = [summaries_and_bullets[convo_id]['summary'] for convo_id in random_2018_more_delta_set]\n",
    "cluster2 = [summaries_and_bullets[convo_id]['summary'] for convo_id in random_2018_less_delta_set]\n",
    "\n",
    "df = bayes_compare_language(cluster1, cluster2, ngram = 2)\n",
    "\n",
    "print(\"Top 20 words in fighting words between Cluster 1 and Cluster 2: \\n\", df[:20])\n",
    "print(\"Last 20 words in fighting words between Cluster 2 and Cluster 1: \\n\", df[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a806364-e821-4b3a-b191-ac2e345e8ae5",
   "metadata": {},
   "source": [
    "## Selecting Delta Conversations\n",
    "\n",
    "Next, we sample a set of Δ (persuasive) and ¬Δ (non-persuasive) conversations and compare their similarity to the previously clustered groups, examining how ConDynS captures alignment between known persuasive dynamics and automatically discovered clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea50d9a-a574-4db8-a356-ea993dbaf0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, we need to download the annotated CMV corpus ###\n",
    "corpus = Corpus(filename=DATA_PATH + \"annotated_cmv_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8467ff1-4cda-450b-b06e-06dad6ca7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions ###\n",
    "def is_delta_convo(convo):\n",
    "    return convo.meta['has_delta']\n",
    "def check_only_one_delta(convo1, convo2):\n",
    "    return convo1.meta['has_delta'] + convo2.meta['has_delta'] == 1\n",
    "def check_both_convo_valid(convo1, convo2, convo_2018=convo_2018):\n",
    "    utt_lst1 = [utt for utt in convo1.iter_utterances()]\n",
    "    utt_lst2 = [utt for utt in convo2.iter_utterances()]\n",
    "    return is_valid_convo(convo1, utt_lst1) and is_valid_convo(convo2, utt_lst2) and extract_real_id(convo1.id) not in convo_2018 and extract_real_id(convo2.id) not in convo_2018\n",
    "def extract_real_id(idx):\n",
    "    return idx.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45867eea-bb6f-45ab-ac40-a718b32ae03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtering for delta and non-delta convos in the year of 2018 ###\n",
    "valid_convos = {\"delta\" : [], \"no_delta\" : []}\n",
    "\n",
    "for convo in tqdm(corpus.iter_conversations()):\n",
    "    if convo.id in valid_convos['delta'] or convo.id in valid_convos['no_delta']:\n",
    "        continue\n",
    "    pair = corpus.get_conversation(convo.meta['pair_id'])\n",
    "    assert check_only_one_delta(convo, pair), \"convo and its pair should have only one delta.\"\n",
    "    if check_both_convo_valid(convo, pair):\n",
    "        if get_convo_year([utt for utt in convo.iter_utterances()]) == 2018 and get_convo_year([utt for utt in pair.iter_utterances()]) == 2018:\n",
    "            if convo.meta['has_delta']:\n",
    "                assert not pair.meta['has_delta']\n",
    "                valid_convos['delta'].append(convo.id)\n",
    "                valid_convos['no_delta'].append(pair.id)\n",
    "            else:\n",
    "                assert not convo.meta['has_delta']\n",
    "                valid_convos['delta'].append(pair.id)\n",
    "                valid_convos['no_delta'].append(convo.id)\n",
    "\n",
    "print(\"Number of delta convos: \", len(valid_convos['delta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3a50b-0bd3-47eb-aa38-4be1aaf1d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pairing up delta and non-delta convos ###\n",
    "valid_convo_pairs = [(valid_convos['delta'][i], valid_convos['no_delta'][i]) for i in range(len(valid_convos['delta']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98bb7e3-d317-45f5-a338-cd0cb11b2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select pairs of delta and non-delta convos, with unique speakers ###\n",
    "random.seed(4300)\n",
    "K = 250\n",
    "used_speakers = []\n",
    "selected_convos = []\n",
    "random.shuffle(valid_convo_pairs)\n",
    "for convo1, convo2 in tqdm(valid_convo_pairs):\n",
    "    convo_1, convo_2 = corpus.get_conversation(convo1), corpus.get_conversation(convo2)\n",
    "    utt_lst1, utt_lst2 = [utt for utt in convo_1.iter_utterances()], [utt for utt in convo_2.iter_utterances()]\n",
    "    convo_speakers = list(set(get_all_speakers(utt_lst1) + get_all_speakers(utt_lst2)))\n",
    "        \n",
    "    if all(s not in used_speakers for s in convo_speakers):\n",
    "        selected_convos.append(convo1)\n",
    "        selected_convos.append(convo2)\n",
    "        used_speakers.extend(convo_speakers)\n",
    "        \n",
    "    if len(selected_convos) == K:\n",
    "        break\n",
    "\n",
    "len(selected_convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635d786-984b-4be8-89f8-8944dd261033",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constructing the corpus with selected convos for the experiment ###\n",
    "all_utterances = []\n",
    "for convo_id in selected_convos:\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    utt_lst = [utt for utt in convo.iter_utterances()]\n",
    "    all_utterances.extend(utt_lst)\n",
    "\n",
    "selected_corpus = Corpus(utterances=all_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5469b4-42aa-490e-b3bf-6929d610784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annotating the corpus ###\n",
    "for convo in tqdm(selected_corpus.iter_conversations()):\n",
    "    og_convo = corpus.get_conversation(convo.id)\n",
    "    convo.meta = og_convo.meta\n",
    "    convo.meta['year'] = '2018'\n",
    "\n",
    "for sp in tqdm(selected_corpus.iter_speakers()):\n",
    "    og_sp = corpus.get_speaker(sp.id)\n",
    "    sp.meta = og_sp.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39e136-b3df-49dd-86f2-a4d300c709e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_corpus.dump(\"cmv_selected_delta_2018\", base_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e3753-8ffe-426f-af45-235544a823c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + \"cmv_selected_delta_convo_ids_2018.json\", \"w\") as f:\n",
    "    json.dump(selected_convos, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b35bc8-a6a0-4d7a-9a92-2c54c8600987",
   "metadata": {},
   "source": [
    "## ConDynS Computation - Delta 2018 Set\n",
    "\n",
    "We compute ConDynS similarity scores within the Δ (persuasive) and ¬Δ (non-persuasive) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cf5b3-8af4-4246-8745-d400102b8547",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./artefacts\"\n",
    "CUR_ANALYSIS = \"delta_set_2018\"\n",
    "\n",
    "if not os.path.exists(f\"{BASE_PATH}\"):\n",
    "    print(\"Making directory: \", f\"{BASE_PATH}\")\n",
    "    os.makedirs(f\"{BASE_PATH}\")\n",
    "\n",
    "if not os.path.exists(f\"{BASE_PATH}/{CUR_ANALYSIS}/\"):\n",
    "    print(\"Making directory: \", f\"{BASE_PATH}/{CUR_ANALYSIS}/\")\n",
    "    os.makedirs(f\"{BASE_PATH}/{CUR_ANALYSIS}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71671ed-46e4-404c-8d44-4e4aa88f4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + \"cmv_selected_delta_convo_ids_2018.json\", \"r\") as f:\n",
    "    delta_2018 = json.load(f)\n",
    "\n",
    "len(delta_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303070cf-1176-4186-a832-0bb3a1aac9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=DATA_PATH + \"cmv_selected_delta_2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafa5df-ef8c-4a68-8ab8-7ec546f44253",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check the distribution of the number of utterances in the corpus ###\n",
    "from collections import Counter\n",
    "numbers = [len(convo.get_utterance_ids()) for convo in corpus.iter_conversations()]\n",
    "counts = Counter(numbers)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a9171-854f-4da6-b4e3-457583149a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computing the similarity between all conversations in the same group for delta 2018 selected convos ###\n",
    "all_convos = delta_2018\n",
    "data_path = f\"{BASE_PATH}/{CUR_ANALYSIS}/\"\n",
    "\n",
    "if os.path.exists(f\"{data_path}summary.json\"):\n",
    "    with open(f\"{data_path}summary.json\", \"r\") as file:\n",
    "        summaries_and_bullets = json.load(file)\n",
    "else:\n",
    "    summaries_and_bullets = {}\n",
    "\n",
    "if os.path.exists(f\"{data_path}similarity.json\"):\n",
    "    with open(f\"{data_path}similarity.json\", \"r\") as file:\n",
    "        similarity_result = json.load(file)\n",
    "else:\n",
    "    similarity_result = {}\n",
    "\n",
    "incomplete = []\n",
    "\n",
    "summaries_and_bullets, similarity_result, incomplete = compute_within_group_similarity(all_convos, summaries_and_bullets, similarity_result, incomplete, data_path)\n",
    "\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c5df6-52be-48e0-a76b-2e7b99a8796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check completed computation ###\n",
    "after_incomplete = [idx for idx in delta_2018 if idx not in problem_convos]\n",
    "delta_set = [convo_id for convo_id in after_incomplete if corpus.get_conversation(convo_id).meta['has_delta']]\n",
    "no_delta_set = [convo_id for convo_id in after_incomplete if convo_id not in delta_set]\n",
    "print(len(delta_set), len(no_delta_set))\n",
    "random.seed(4300)\n",
    "delta_set = random.sample(delta_set, 100)\n",
    "no_delta_set = random.sample(no_delta_set, 100)\n",
    "delta_2018 = delta_set + no_delta_set\n",
    "print(len(delta_2018))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c8206e-305c-4796-8c87-3756f92d6371",
   "metadata": {},
   "source": [
    "# Compute Inter Group Similarity\n",
    "\n",
    "We use ConDynS to measure inter-group similarity between the random conversation set and the delta groups. This shows how similar the dynamics of random conversations are to those of persuasive ones, helping us understand what the earlier clusters actually capture.\n",
    "\n",
    "Here, we also demonstrate the way to set custom SCD prompts for adapting our module to your own data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa9d55-5191-411e-81e9-1e7455e2594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./artefacts\"\n",
    "CUR_ANALYSIS = \"inter_group_sim\"\n",
    "\n",
    "if not os.path.exists(f\"{BASE_PATH}\"):\n",
    "    print(\"Making directory: \", f\"{BASE_PATH}\")\n",
    "    os.makedirs(f\"{BASE_PATH}\")\n",
    "\n",
    "if not os.path.exists(f\"{BASE_PATH}/{CUR_ANALYSIS}/\"):\n",
    "    print(\"Making directory: \", f\"{BASE_PATH}/{CUR_ANALYSIS}/\")\n",
    "    os.makedirs(f\"{BASE_PATH}/{CUR_ANALYSIS}/\")\n",
    "\n",
    "with open(\"cmv_selected_delta_convo_ids_2018.json\", \"r\") as f:\n",
    "    delta_2018 = json.load(f)\n",
    "\n",
    "with open(\"cmv_selected_convo_ids_2018.json\", \"r\") as f:\n",
    "    convo_2018 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91e49f-04eb-470e-8240-f1fbb1b3b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_delta = Corpus(filename=DATA_PATH + \"cmv_selected_delta_2018\")\n",
    "corpus_2018 = Corpus(filename=DATA_PATH + \"cmv_selected_convos_2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d653fcc4-2c18-4496-b0bb-38ecf44dfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_set = [convo_id for convo_id in delta_2018 if corpus_delta.get_conversation(convo_id).meta['has_delta']]\n",
    "no_delta_set = [convo_id for convo_id in delta_2018 if convo_id not in delta_set]\n",
    "assert len(delta_set) == len(no_delta_set) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b485b-6bd6-4a1b-8458-5c51aab80cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + \"random_2018_less_delta_set.json\", \"r\") as f:\n",
    "    random_2018_less_delta_set = json.load(f)\n",
    "\n",
    "with open(DATA_PATH + \"random_2018_more_delta_set.json\", \"r\") as f:\n",
    "    random_2018_more_delta_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modify to SCD prompt to focus on Persuasions ###\n",
    "persuasion_scd_prompt = \"\"\"Write a short summary capturing the trajectory of an online conversation.  \n",
    "Do not include specific topics, claims, or arguments from the conversation. The style you should avoid: \n",
    "Example Sentence 1: \"Speaker1, who is Asian, defended Asians and pointed out that a study found that whites, Hispanics, and blacks were accepted into universities in that order, with Asians being accepted the least. Speaker2 acknowledged that Asians have high household income, but argued that this could be a plausible explanation for the study's findings. Speaker1 disagreed and stated that the study did not take wealth into consideration.\" \n",
    "This style mentions specific claims and topics, which are not needed.\n",
    "\n",
    "Instead, do include indicators of sentiments (e.g., sarcasm, passive-aggressive, polite, frustration, attack, blame), individual intentions (e.g., agreement, disagreement, persistent-agreement, persistent-disagreement, rebuttal, defense, concession, confusion, clarification, neutral, accusation) and conversational strategies (if any) such as 'rhetorical questions', 'straw man fallacy', 'identify fallacies',  and 'appealing to emotions.' \n",
    "The following sentences demonstrate the style you should follow:\n",
    "\n",
    "Example Sentence 2: \"Both speakers have differing opinions and appeared defensive. Speaker1 attacks Speaker2 by diminishing the importance of his argument and Speaker2 blames Speaker1 for using profane words. Both speakers accuse each other of being overly judgemental of their personal qualities rather than arguments.\"\n",
    "\n",
    "Example Sentence 3: \"The two speakers refuted each other with back and forth accusations. Throughout the conversation, they kept harshly fault-finding with overly critical viewpoints, creating an intense and inefficient discussion.\"\n",
    "\n",
    "Example Sentence 4: \"Speaker1 attacks Speaker2 by questioning the relevance of his premise and Speaker2 blames Speaker1 for using profane words. Both speakers accuse each other of being overly judgemental of their personal qualities rather than arguments.\"\n",
    "\n",
    "Overall, the trajectory summary should capture the key moments where the tension of the conversation notably changes. Here is an example of a complete trajectory summary. \n",
    "\n",
    "Trajectory Summary: \n",
    "Multiple users discuss minimum wage. Four speakers express their different points of view subsequently, building off of each other's arguments. Speaker1 disagrees with a specific point from Speaker2's argument, triggering Speaker2 to contradict Speaker1 in response. Then, Speaker3 jumps into the conversation to support Speaker1's argument, which leads Speaker2 to adamantly defend their argument. Speaker2 then quotes a deleted comment, giving an extensive counterargument. The overall tone remains civil.\n",
    "\n",
    "Now, provide the trajectory summary for the following conversation.\n",
    "Conversation Transcript:\n",
    "{transcript}\n",
    "\n",
    "Now, summarize this conversation. Remember, do not include specific topics, claims, or arguments from the conversation. Instead, try to capture the speakers' sentiments, intentions, and conversational/persuasive strategies. Limit the trajectory summary to 80 words. \n",
    "\n",
    "Trajectory Summary:\n",
    "\"\"\"\n",
    "\n",
    "persuasion_scd_writer = SCDWriter(\n",
    "    model_provider=MODLE_PROVIDER,\n",
    "    config=config,\n",
    "    custom_scd_prompt=persuasion_scd_prompt,\n",
    "    custom_prompt_dir=\"../../prompts/custom_prompts/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82c534-03b3-4c7f-b8f4-aca1d6faadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Update helper functions with persuasion prompt ###\n",
    "def get_cur_convo_transcript_persuasion(corpus, convo_id):\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    utt_list = convo.get_chronological_utterance_list()\n",
    "    transcription = []\n",
    "    spk_list = {utt_list[0].speaker.id : \"SPEAKER1\"}\n",
    "    for utt in utt_list:\n",
    "        if utt.speaker.id not in spk_list.keys():\n",
    "            spk_list[utt.speaker.id] = \"SPEAKER2\"\n",
    "            assert len(spk_list) == 2\n",
    "        transcription.append(spk_list[utt.speaker.id] +\": \"+utt.text)\n",
    "    transcription = transcription[1:] # remove first OP message\n",
    "    return transcription\n",
    "\n",
    "\n",
    "def compute_within_group_similarity_persuasion(all_convos, summaries_and_bullets, similarity_result, incomplete, data_path):\n",
    "    all_convos_combos = list(combinations(all_convos, 2))\n",
    "    len(all_convos_combos)\n",
    "    \n",
    "    # ### Calling GPT to create the SCD and bullet points. Run with caution.\n",
    "    summaries_and_bullets = summaries_and_bullets\n",
    "    incomplete = incomplete\n",
    "    similarity_result = similarity_result\n",
    "    \n",
    "    for convo_id in tqdm(all_convos, desc=\"generating SCDs\"):\n",
    "        if convo_id in summaries_and_bullets.keys(): continue\n",
    "        summary, bulletpoint = persuasion_scd_writer.get_scd_and_sop(corpus_2018, corpus_delta, convo_id)\n",
    "        summaries_and_bullets.update({convo_id : {\"summary\" : summary, \"bulletpoint\" : bulletpoint}})\n",
    "    with open(f\"{data_path}summary.json\", \"w\") as file:\n",
    "        json.dump(summaries_and_bullets, file, indent=4)\n",
    "\n",
    "    def get_bidirection_similarity_with_retry_2(corpus_2018, corpus_delta, convo1_id, convo2_id, summaries_and_bullets, retries=10):\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                result = condyns.compute_bidirectional_similarity(corpus_2018, corpus_delta, convo1_id, convo2_id, summaries_and_bullets)\n",
    "                score = condyns.compute_score_from_results(result)\n",
    "                return score, result\n",
    "            except Exception as e:\n",
    "                wait = 0.5 ** i + random.random()\n",
    "                print(f\"Retrying ({convo1_id}, {convo2_id}) after {wait:.2f}s due to error: {e}\")\n",
    "                incomplete.append(f'{convo1_id}_{convo2_id}')\n",
    "                time.sleep(wait)\n",
    "        return None, None\n",
    "    \n",
    "    # The thread worker function\n",
    "    def worker(corpus_2018, corpus_delta, convo1_id, convo2_id, summaries_and_bullets):\n",
    "        score, result = get_bidirection_similarity_with_retry_2(corpus_2018, corpus_delta, convo1_id, convo2_id, summaries_and_bullets)\n",
    "        return (f'{convo1_id}_{convo2_id}', {\"score\": score, \"result\": result})\n",
    "\n",
    "    MAX_WORKER = 50\n",
    "    \n",
    "    # Build task list\n",
    "    tasks = [(corpus_2018, corpus_delta, id1, id2, summaries_and_bullets) \n",
    "             for id1, id2 in all_convos_combos \n",
    "             if f'{id1}_{id2}' not in similarity_result and f'{id1}_{id2}' not in incomplete and f'{id2}_{id1}' not in similarity_result and f'{id2}_{id1}' not in incomplete]\n",
    "        \n",
    "    # Set up thread pool\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKER) as executor:\n",
    "        futures = [executor.submit(worker, *task) for task in tasks]\n",
    "    \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Computing Pair-wise Similarity\"):\n",
    "            key, value = future.result()\n",
    "            if key in similarity_result.keys():\n",
    "                print(\"not good, repeated keys\")\n",
    "            if key not in incomplete:\n",
    "                similarity_result[key] = value\n",
    "\n",
    "    with open(f\"{data_path}similarity.json\", \"w\") as file:\n",
    "        json.dump(similarity_result, file, indent=4)\n",
    "\n",
    "    with open(f\"{data_path}incomplete.json\", \"w\") as file:\n",
    "        json.dump(incomplete, file, indent=4)\n",
    "\n",
    "    return summaries_and_bullets, similarity_result, incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efafe5b-a454-425d-9762-0f79ee8c7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_convos = convo_2018 + delta_2018\n",
    "data_path = f\"{BASE_PATH}/{CUR_ANALYSIS}/\"\n",
    "\n",
    "if os.path.exists(f\"{data_path}summary.json\"):\n",
    "    with open(f\"{data_path}summary.json\", \"r\") as file:\n",
    "        summaries_and_bullets = json.load(file)\n",
    "else:\n",
    "    summaries_and_bullets = {}\n",
    "\n",
    "if os.path.exists(f\"{data_path}similarity.json\"):\n",
    "    with open(f\"{data_path}similarity.json\", \"r\") as file:\n",
    "        similarity_result = json.load(file)\n",
    "else:\n",
    "    similarity_result = {}\n",
    "\n",
    "incomplete = []\n",
    "\n",
    "summaries_and_bullets, similarity_result, incomplete = compute_within_group_similarity_persuasion(all_convos, summaries_and_bullets, similarity_result, incomplete, data_path)\n",
    "\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a25057-ccb4-4ee3-9bd8-03b5dcdf9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verify all similarity are computed valid here:\n",
    "with open(f\"{data_path}summary.json\", \"r\") as file:\n",
    "    summaries_and_bullets = json.load(file)\n",
    "for convo_id in all_convos:\n",
    "    assert convo_id in summaries_and_bullets\n",
    "\n",
    "with open(f\"{data_path}similarity.json\", \"r\") as file:\n",
    "    similarity_result = json.load(file)\n",
    "all_convos_combos = list(combinations(convo_2018, 2))\n",
    "for id1, id2 in all_convos_combos:\n",
    "    assert f'{id1}_{id2}' in similarity_result or f'{id2}_{id1}' in similarity_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5e4c9-430e-4f7d-aa3e-ce9da189012a",
   "metadata": {},
   "source": [
    "### Analysis - Inter/Intra group similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9159e-c470-4586-b3cc-acd29e9d8d39",
   "metadata": {},
   "source": [
    "#### Inter Group Similarity\n",
    "\n",
    "Here we present the between group similarities from the random set with delta/no delta sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7b6a8-b17e-4bb2-99a4-aa57073ed9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_between_group_similarity(group1, group2):\n",
    "    similarities = []\n",
    "    for convo1 in group1:\n",
    "        for convo2 in group2:\n",
    "            sim = np.mean(get_similarity(convo1, convo2, all_convos, similarity_result))\n",
    "            similarities.append(sim)\n",
    "    return np.array(similarities)\n",
    "    \n",
    "\n",
    "def between_group_similarity_check(group1, group2, group1_name, group2_name, y_lim=4):\n",
    "    between_similarities = compute_between_group_similarity(group1, group2)\n",
    "    \n",
    "    between_mean = np.mean(between_similarities)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(between_similarities, bins=20, alpha=0.7, color='purple', label=f'{group1_name} vs {group2_name}', density=False)\n",
    "    \n",
    "    plt.xlabel(\"similarity score\")\n",
    "    plt.ylabel(\"number of conversation pairs\")\n",
    "    plt.title(f\"Between-Group Similarity: {group1_name} vs {group2_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, y_lim)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Between-Group Mean Similarity ({group1_name} vs {group2_name}): {between_mean:.4f}\")\n",
    "\n",
    "    return between_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23efd4e8-2e5d-4bb7-b10f-348e832dc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_delta = between_group_similarity_check(random_2018_less_delta_set, delta_set, \"group1\", \"delta\", y_lim=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397b56a-780d-4e06-8523-b34d8414f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_delta = between_group_similarity_check(random_2018_more_delta_set, delta_set, \"group2\", \"delta\", y_lim=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a4a73-70fe-4a19-8879-050188009818",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_delta = between_group_similarity_check(random_2018_less_delta_set, no_delta_set, \"group1\", \"no delta\", y_lim=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e37ce2-7d40-4c5b-8440-8c0d84371d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_delta = between_group_similarity_check(random_2018_more_delta_set, no_delta_set, \"group2\", \"no delta\", y_lim=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f08dc-ebb1-40c7-827c-0d3d87ee27e0",
   "metadata": {},
   "source": [
    "#### Intra Group Similarity\n",
    "\n",
    "Here we present the Intra group similarity within delta set and within no delta set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4f5f6-a64d-4c39-b080-4009c1899fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_groups_intra_group_similarity_check(delta_set, no_delta_set, \"Persuasive\", \"Non-persuasive\", all_convos, similarity_result, y_lim = 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48fb214-6974-471c-bfa0-9086d10ea7c4",
   "metadata": {},
   "source": [
    "# New Insights: Who Drives The Dynamics?\n",
    "\n",
    "We investigate which speaker’s tendencies shape conversational dynamics by comparing conversations where the same individual appears as the original poster (OP) versus as the challenger. Using ConDynS, we quantify the similarity of dynamics across these role-specific conversations to assess whether situational power (held by the OP) or persuasive strategy (driven by the challenger) has a stronger influence on the interaction’s trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48676c84-1dd4-4c67-a3ba-136b6df97f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(filename=DATA_PATH + \"subreddit-changemyview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f75d8-09a7-4d5c-9121-5de21d82c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Annotate Delta\n",
    "for utt in tqdm(corpus.iter_utterances()):\n",
    "    if (\n",
    "        utt.reply_to is not None\n",
    "        and utt.speaker.id == \"DeltaBot\"\n",
    "        and \"delta awarded\" in utt.text\n",
    "    ):\n",
    "        deltabot_text = utt.text\n",
    "        match = re.search(\n",
    "            r\"(?:Confirmed: 1 delta awarded to )(?:\\/)?(?:u\\/)([\\w-]+)\", deltabot_text\n",
    "        )\n",
    "        if match is not None:\n",
    "            try:\n",
    "                delta_utt = corpus.get_utterance(utt.reply_to)\n",
    "                delta_utt.meta['got_delta'] = True\n",
    "            except KeyError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b3dc1-2057-4396-8a93-a62378a47d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./artefacts\"\n",
    "CUR_ANALYSIS = \"speaker_consistency_same_op_contender\"\n",
    "\n",
    "if not os.path.exists(f\"{BASE_PATH}\"):\n",
    "    print(\"Making directory: \", f\"{BASE_PATH}\")\n",
    "    os.makedirs(f\"{BASE_PATH}\")\n",
    "\n",
    "if not os.path.exists(f\"{BASE_PATH}/{CUR_ANALYSIS}/\"):\n",
    "    print(\"Making directory: \", f\"{BASE_PATH}/{CUR_ANALYSIS}/\")\n",
    "    os.makedirs(f\"{BASE_PATH}/{CUR_ANALYSIS}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667e91b-cf8c-42fb-8786-14315d8048c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replier_id(utt_lst):\n",
    "    return utt_lst[1].speaker.id\n",
    "\n",
    "def get_op_id(utt_lst):\n",
    "    return utt_lst[0].speaker.id\n",
    "\n",
    "def get_rp_id(utt_lst):\n",
    "    return utt_lst[1].speaker.id\n",
    "\n",
    "def get_convo_id(utt_lst):\n",
    "    return utt_lst[0].get_conversation().id\n",
    "\n",
    "def get_convo_op_id(utt_lsts):\n",
    "    return get_op_id(utt_lsts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ede32-8c16-4322-9003-dfa960bd077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, we find all the valid two speaker threads that is at correct length 5+, for each conversation.\n",
    "convo_to_two_speaker_threads = {}\n",
    "for convo in tqdm(corpus.iter_conversations()):\n",
    "    try:\n",
    "        all_utt_lsts = convo.get_root_to_leaf_paths()\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    valid_two_speaker_utt_lsts = []\n",
    "    found_repliers = []\n",
    "    for utt_lst in all_utt_lsts:\n",
    "        if is_valid_convo(convo, utt_lst):\n",
    "            replier = get_replier_id(utt_lst)\n",
    "            if replier not in found_repliers:\n",
    "                valid_two_speaker_utt_lsts.append(utt_lst)\n",
    "                found_repliers.append(replier)\n",
    "    if valid_two_speaker_utt_lsts:\n",
    "        convo_to_two_speaker_threads[convo.id] = valid_two_speaker_utt_lsts\n",
    "\n",
    "len(convo_to_two_speaker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d9efc-2357-4a9b-a8b8-7191ee8461d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for convo_id, utt_lsts in convo_to_two_speaker_threads.items():\n",
    "    op = get_convo_op_id(utt_lsts)\n",
    "    for utt_lst in utt_lsts:\n",
    "        assert get_op_id(utt_lst) == op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83596ac8-9c6f-45ed-8ca1-37f764c693a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we find all the times the speaker is whatever role\n",
    "random.seed(4300)\n",
    "sp_to_convos = {}\n",
    "sp_to_convos_id = {}\n",
    "for convo_id, utt_lsts in convo_to_two_speaker_threads.items():\n",
    "    op = get_convo_op_id(utt_lsts)\n",
    "    if op not in sp_to_convos_id.keys() or convo_id not in sp_to_convos_id[op]:\n",
    "        if op not in sp_to_convos.keys():\n",
    "            sp_to_convos[op] = []\n",
    "        sp_to_convos[op].append(random.choice(utt_lsts))\n",
    "        if op not in sp_to_convos_id.keys():\n",
    "            sp_to_convos_id[op] = []\n",
    "        sp_to_convos_id[op].append(convo_id)\n",
    "    for utt_lst in utt_lsts:\n",
    "        rp = get_rp_id(utt_lst)\n",
    "        if op == rp: continue\n",
    "        if rp not in sp_to_convos_id.keys() or convo_id not in sp_to_convos_id[rp]:\n",
    "            if rp not in sp_to_convos.keys():\n",
    "                sp_to_convos[rp] = []\n",
    "            sp_to_convos[rp].append(utt_lst)\n",
    "            if rp not in sp_to_convos_id.keys():\n",
    "                sp_to_convos_id[rp] = []\n",
    "            sp_to_convos_id[rp].append(convo_id)\n",
    "\n",
    "enough_convo_sps = [sp for sp, lst in sp_to_convos.items() if len(lst) >= 2]\n",
    "print(f\"number of speakers with two or more conversations: {len(enough_convo_sps)}\")\n",
    "selected_convo_sps = random.sample(enough_convo_sps, 2000)\n",
    "selected_convos_random_role = {}\n",
    "for sp_id in selected_convo_sps:\n",
    "    selection_lst = sp_to_convos[sp_id]\n",
    "    assert len(selection_lst) >= 2\n",
    "    if len(selection_lst) == 2:\n",
    "        selected_convos_random_role.update({sp_id : selection_lst})\n",
    "    else:\n",
    "        selected_convos_random_role.update({sp_id : random.sample(selection_lst, 2)})\n",
    "print(f\"selected number of speakers to compare: {len(selected_convos_random_role)}\")\n",
    "\n",
    "### Ensure no repeated convo selected for the same speaker\n",
    "for sp_id, utt_lsts in selected_convos_random_role.items():\n",
    "    assert len(utt_lsts) == 2\n",
    "    assert utt_lsts[0][0].get_conversation().id != utt_lsts[1][0].get_conversation().id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1b878-19d8-436c-a24b-13bb7e341352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cur_convo_transcript(utt_lst):\n",
    "    transcription = []\n",
    "    spk_list = {utt_lst[0].speaker.id : \"SPEAKER1\"}\n",
    "    for utt in utt_lst:\n",
    "        if utt.speaker.id not in spk_list.keys():\n",
    "            spk_list[utt.speaker.id] = \"SPEAKER2\"\n",
    "            assert len(spk_list) == 2\n",
    "        transcription.append(spk_list[utt.speaker.id] +\": \"+utt.text)\n",
    "    transcription = transcription[1:] ### truncate OP first message\n",
    "    return transcription\n",
    "    \n",
    "def get_bidirection_similarity_with_retry_utt_lst(corpus, convo1_id, convo2_id, utt_lst_1, utt_lst_2, summaries_and_bullets, incomplete, retries=10):\n",
    "    transcript1 = get_cur_convo_transcript(utt_lst_1)\n",
    "    transcript2 = get_cur_convo_transcript(utt_lst_2)\n",
    "    \n",
    "    scd1 = summaries_and_bullets[convo1_id]['summary']\n",
    "    scd2 = summaries_and_bullets[convo2_id]['summary']\n",
    "    \n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            result = condyns.compute_bidirectional_similarity(transcript1, transcript2, scd1, scd2)\n",
    "            score = condyns.compute_score_from_results(result)\n",
    "            return score, result, incomplete\n",
    "        except Exception as e:\n",
    "            wait = 0.5 ** i + random.random()\n",
    "            print(f\"Retrying ({convo1_id}, {convo2_id}) after {wait:.2f}s due to error: {e}\")\n",
    "            incomplete.update(f'{convo1_id}_{convo2_id}')\n",
    "            time.sleep(wait)\n",
    "    return None, None, incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee713c-b0db-4cf3-a668-0cb6c222f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dict(d, n):\n",
    "    items = list(d.items())\n",
    "    chunk_size = math.ceil(len(items) / n)\n",
    "    return [dict(items[i:i + chunk_size]) for i in range(0, len(items), chunk_size)]\n",
    "\n",
    "# === Config and setup ===\n",
    "all_need_to_compares = [selected_convos_random_role]\n",
    "all_ROLE = [\"RANDOM_ROLE\"]\n",
    "\n",
    "for ROLE, need_to_compares in zip(all_ROLE, all_need_to_compares):\n",
    "    SAVE_PATH = f\"{BASE_PATH}/{CUR_ANALYSIS}/{ROLE}\"\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "    \n",
    "    # === Load previously saved data ===\n",
    "    def load_json(path, default):\n",
    "        return json.load(open(path)) if os.path.exists(path) else default\n",
    "    \n",
    "    summaries_and_bullets = load_json(f\"{SAVE_PATH}/summary.json\", {})\n",
    "    similarity_result = load_json(f\"{SAVE_PATH}/similarity.json\", {})\n",
    "    incomplete = set(load_json(f\"{SAVE_PATH}/incomplete.json\", []))\n",
    "    \n",
    "    # === Worker Function ===\n",
    "    def process_chunk(chunk):\n",
    "        local_summaries = {}\n",
    "        local_similarity = {}\n",
    "        local_incomplete = set()\n",
    "    \n",
    "        for sp_id, (utt_lst1, utt_lst2) in chunk.items():\n",
    "            id1 = f\"{sp_id}##{utt_lst1[0].get_conversation().id}\"\n",
    "            id2 = f\"{sp_id}##{utt_lst2[0].get_conversation().id}\"\n",
    "    \n",
    "            for idx, utt_lst in zip([id1, id2], [utt_lst1, utt_lst2]):\n",
    "                if idx in summaries_and_bullets:\n",
    "                    continue\n",
    "                summary, bulletpoint = persuasion_scd_writer.get_scd_and_sop(corpus, utt_lst)\n",
    "                local_summaries[idx] = {\"summary\": summary, \"bulletpoint\": bulletpoint}\n",
    "    \n",
    "            id_pair_key_1 = f'{id1}_{id2}'\n",
    "            id_pair_key_2 = f'{id2}_{id1}'\n",
    "    \n",
    "            if id_pair_key_1 not in similarity_result and id_pair_key_1 not in incomplete \\\n",
    "               and id_pair_key_2 not in similarity_result and id_pair_key_2 not in incomplete:\n",
    "                score, result, inc = get_bidirection_similarity_with_retry_utt_lst(corpus, id1, id2, utt_lst1, utt_lst2, summaries_and_bullets, incomplete)\n",
    "                local_similarity[id_pair_key_1] = {\"score\": score, \"result\": result}\n",
    "                local_incomplete.update(inc)\n",
    "    \n",
    "        return local_summaries, local_similarity, local_incomplete\n",
    "    \n",
    "    # === Split work and run with threads ===\n",
    "    NUM_WORKERS = 25\n",
    "    chunks = split_dict(need_to_compares, NUM_WORKERS)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "    \n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            local_summaries, local_similarity, local_incomplete = future.result()\n",
    "            summaries_and_bullets.update(local_summaries)\n",
    "            similarity_result.update(local_similarity)\n",
    "            incomplete.update(local_incomplete)\n",
    "    \n",
    "    # === Save to files ===\n",
    "    with open(f\"{SAVE_PATH}/summary.json\", \"w\") as file:\n",
    "        json.dump(summaries_and_bullets, file, indent=4)\n",
    "    \n",
    "    with open(f\"{SAVE_PATH}/similarity.json\", \"w\") as file:\n",
    "        json.dump(similarity_result, file, indent=4)\n",
    "    \n",
    "    with open(f\"{SAVE_PATH}/incomplete.json\", \"w\") as file:\n",
    "        json.dump(list(incomplete), file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ee36c-85d1-44f0-8576-c9f312022851",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_to_compares = selected_convos_random_role\n",
    "ROLE = \"RANDOM_ROLE\"\n",
    "SAVE_PATH = f\"{BASE_PATH}/{CUR_ANALYSIS}/{ROLE}\"\n",
    "\n",
    "with open(f\"{SAVE_PATH}/similarity.json\", \"r\") as file:\n",
    "    similarity_result_random = json.load(file)\n",
    "\n",
    "RANDOM_similarity = []\n",
    "\n",
    "for sp_id, (utt_lst1, utt_lst2) in need_to_compares.items():\n",
    "    id1 = f\"{sp_id}##{utt_lst1[0].get_conversation().id}\"\n",
    "    id2 = f\"{sp_id}##{utt_lst2[0].get_conversation().id}\"\n",
    "            \n",
    "    sim_key = f'{id1}_{id2}' if f'{id1}_{id2}' in similarity_result_random.keys() else f'{id2}_{id1}'\n",
    "    assert sim_key in similarity_result_random.keys()\n",
    "    try:\n",
    "        RANDOM_similarity.append(np.mean(similarity_result_random[sim_key]['score']))\n",
    "    except TypeError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a956dcd-77df-4992-93ee-51865bb1934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(RANDOM_similarity), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4300)\n",
    "all_convos_to_choose = list(convo_to_two_speaker_threads.keys())\n",
    "random.shuffle(all_convos_to_choose)\n",
    "selected_pairs = list(zip(all_convos_to_choose[::2], all_convos_to_choose[1::2]))[:2000]\n",
    "selected_pairs_utt_lsts = {}\n",
    "for i, (idx1, idx2) in enumerate(selected_pairs):\n",
    "    selected_pairs_utt_lsts.update({i : (random.choice(convo_to_two_speaker_threads[idx1]), random.choice(convo_to_two_speaker_threads[idx2]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dict(d, n):\n",
    "    items = list(d.items())\n",
    "    chunk_size = math.ceil(len(items) / n)\n",
    "    return [dict(items[i:i + chunk_size]) for i in range(0, len(items), chunk_size)]\n",
    "\n",
    "# === Config and setup ===\n",
    "\n",
    "need_to_compares = selected_pairs_utt_lsts\n",
    "ROLE = \"COMPLETELY_RANDOM_SPEAKER_AND_CONVERSATIONS\"\n",
    "\n",
    "SAVE_PATH = f\"{BASE_PATH}/{CUR_ANALYSIS}/{ROLE}\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# === Load previously saved data ===\n",
    "def load_json(path, default):\n",
    "    return json.load(open(path)) if os.path.exists(path) else default\n",
    "\n",
    "summaries_and_bullets = load_json(f\"{SAVE_PATH}/summary.json\", {})\n",
    "similarity_result = load_json(f\"{SAVE_PATH}/similarity.json\", {})\n",
    "incomplete = set(load_json(f\"{SAVE_PATH}/incomplete.json\", []))\n",
    "\n",
    "# === Worker Function ===\n",
    "def process_chunk(chunk):\n",
    "    local_summaries = {}\n",
    "    local_similarity = {}\n",
    "    local_incomplete = set()\n",
    "\n",
    "    for sp_id, (utt_lst1, utt_lst2) in chunk.items():\n",
    "        id1 = f\"{sp_id}##{utt_lst1[0].get_conversation().id}\"\n",
    "        id2 = f\"{sp_id}##{utt_lst2[0].get_conversation().id}\"\n",
    "\n",
    "        for idx, utt_lst in zip([id1, id2], [utt_lst1, utt_lst2]):\n",
    "            if idx in summaries_and_bullets:\n",
    "                continue\n",
    "            summary, bulletpoint = scd_writer.get_scd_and_sop(corpus, utt_lst)\n",
    "            local_summaries[idx] = {\"summary\": summary, \"bulletpoint\": bulletpoint}\n",
    "\n",
    "        id_pair_key_1 = f'{id1}_{id2}'\n",
    "        id_pair_key_2 = f'{id2}_{id1}'\n",
    "\n",
    "        if id_pair_key_1 not in similarity_result and id_pair_key_1 not in incomplete \\\n",
    "           and id_pair_key_2 not in similarity_result and id_pair_key_2 not in incomplete:\n",
    "            score, result, inc = condyns.compute_bidirectional_similarity(\n",
    "                corpus, id1, id2, utt_lst1, utt_lst2,\n",
    "                {**summaries_and_bullets, **local_summaries},\n",
    "                incomplete\n",
    "            )\n",
    "            local_similarity[id_pair_key_1] = {\"score\": score, \"result\": result}\n",
    "            local_incomplete.update(inc)\n",
    "\n",
    "    return local_summaries, local_similarity, local_incomplete\n",
    "\n",
    "# === Split work and run with threads ===\n",
    "NUM_WORKERS = 25\n",
    "chunks = split_dict(need_to_compares, NUM_WORKERS)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "    futures = [executor.submit(process_chunk, chunk) for chunk in chunks]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        local_summaries, local_similarity, local_incomplete = future.result()\n",
    "        summaries_and_bullets.update(local_summaries)\n",
    "        similarity_result.update(local_similarity)\n",
    "        incomplete.update(local_incomplete)\n",
    "\n",
    "# === Save to files ===\n",
    "with open(f\"{SAVE_PATH}/summary.json\", \"w\") as file:\n",
    "    json.dump(summaries_and_bullets, file, indent=4)\n",
    "\n",
    "with open(f\"{SAVE_PATH}/similarity.json\", \"w\") as file:\n",
    "    json.dump(similarity_result, file, indent=4)\n",
    "\n",
    "with open(f\"{SAVE_PATH}/incomplete.json\", \"w\") as file:\n",
    "    json.dump(list(incomplete), file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c628e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_to_compares = selected_pairs_utt_lsts\n",
    "ROLE = \"COMPLETELY_RANDOM_SPEAKER_AND_CONVERSATIONS\"\n",
    "SAVE_PATH = f\"{BASE_PATH}/{CUR_ANALYSIS}/{ROLE}\"\n",
    "\n",
    "with open(f\"{SAVE_PATH}/similarity.json\", \"r\") as file:\n",
    "    similarity_result_c_random = json.load(file)\n",
    "\n",
    "COMPLETE_RANDOME_similarity = []\n",
    "\n",
    "for sp_id, (utt_lst1, utt_lst2) in need_to_compares.items():\n",
    "    id1 = f\"{sp_id}##{utt_lst1[0].get_conversation().id}\"\n",
    "    id2 = f\"{sp_id}##{utt_lst2[0].get_conversation().id}\"\n",
    "            \n",
    "    sim_key = f'{id1}_{id2}' if f'{id1}_{id2}' in similarity_result_c_random.keys() else f'{id2}_{id1}'\n",
    "    assert sim_key in similarity_result_c_random.keys()\n",
    "    try:\n",
    "        COMPLETE_RANDOME_similarity.append(np.mean(similarity_result_c_random[sim_key]['score']))\n",
    "    except TypeError:\n",
    "        print(similarity_result_c_random[sim_key]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af825ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(COMPLETE_RANDOME_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p_value = mannwhitneyu(COMPLETE_RANDOME_similarity, RANDOM_similarity, alternative='two-sided')\n",
    "print(f\"Mann Whitney statistic: {stat}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89b373",
   "metadata": {},
   "source": [
    "## OP or Challenger Drive the Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54b28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_op_convos = {}\n",
    "for convo_id, utt_lsts in convo_to_two_speaker_threads.items():\n",
    "    for utt_lst in utt_lsts:\n",
    "        op = get_op_id(utt_lst)\n",
    "        if op not in sp_op_convos.keys():\n",
    "            sp_op_convos[op] = []\n",
    "        sp_op_convos[op].append(utt_lst)\n",
    "\n",
    "sp_rp_convos = {}\n",
    "for convo_id, utt_lsts in convo_to_two_speaker_threads.items():\n",
    "    for utt_lst in utt_lsts:\n",
    "        rp = get_replier_id(utt_lst)\n",
    "        if rp not in sp_rp_convos.keys():\n",
    "            sp_rp_convos[rp] = []\n",
    "        sp_rp_convos[rp].append(utt_lst)\n",
    "\n",
    "valid_speaker = [op for op, lst in sp_op_convos.items() if len(lst) >= 2 and op in sp_rp_convos.keys() and len(sp_rp_convos[op]) >= 2]\n",
    "random.seed(4300)\n",
    "random.shuffle(valid_speaker)\n",
    "\n",
    "for v in sp_op_convos.values():\n",
    "    random.shuffle(v)\n",
    "\n",
    "for v in sp_rp_convos.values():\n",
    "    random.shuffle(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddeabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pairs = [] # (sp_id, convo1, convo2, convo3, convo4)\n",
    "used_convos = []\n",
    "used_sps = []\n",
    "\n",
    "for sp_id in valid_speaker:\n",
    "    if sp_id in used_sps: continue\n",
    "    op_convos = sp_op_convos[sp_id]\n",
    "    rp_convos = sp_rp_convos[sp_id]\n",
    "    selected_op_convos = []\n",
    "    selected_rp_convos = []\n",
    "\n",
    "    local_used_sps = []\n",
    "    local_used_convos = []\n",
    "\n",
    "    for utt_lst in op_convos:\n",
    "        assert is_valid_convo(utt_lst[0].get_conversation(), utt_lst)\n",
    "        convo_id = get_convo_id(utt_lst)\n",
    "        if convo_id not in used_convos and convo_id not in local_used_convos:\n",
    "            rp = get_replier_id(utt_lst)\n",
    "            all_sps = get_all_speakers(utt_lst)\n",
    "            if all([x not in used_sps for x in all_sps]) and (rp not in local_used_sps):\n",
    "                selected_op_convos.append(utt_lst)\n",
    "                local_used_sps.append(rp)\n",
    "                local_used_convos.append(convo_id)\n",
    "                if len(selected_op_convos) == 2:\n",
    "                    break\n",
    "\n",
    "    for utt_lst in rp_convos:\n",
    "        assert is_valid_convo(utt_lst[0].get_conversation(), utt_lst)\n",
    "        convo_id = utt_lst[0].get_conversation().id\n",
    "        if convo_id not in used_convos and convo_id not in local_used_convos:\n",
    "            op = get_op_id(utt_lst)\n",
    "            all_sps = get_all_speakers(utt_lst)\n",
    "            if all([x not in used_sps for x in all_sps]) and (op not in local_used_sps):\n",
    "                selected_rp_convos.append(utt_lst)\n",
    "                local_used_sps.append(op)\n",
    "                local_used_convos.append(convo_id)\n",
    "                if len(selected_rp_convos) == 2:\n",
    "                    break\n",
    "\n",
    "    if len(selected_op_convos) == 2 and len(selected_rp_convos) == 2:\n",
    "        selected_pairs.append((sp_id, selected_op_convos, selected_rp_convos))\n",
    "        used_convos.extend(local_used_convos)\n",
    "        local_used_sps.append(sp_id)\n",
    "        # used_sps.extend(local_used_sps)\n",
    "        convo1, convo2 = selected_op_convos\n",
    "        convo3, convo4 = selected_rp_convos\n",
    "        all_local_speakers = get_all_speakers(convo1) + get_all_speakers(convo2) + get_all_speakers(convo3) + get_all_speakers(convo4)\n",
    "        all_local_speakers = list(set(all_local_speakers))\n",
    "        used_sps.extend(all_local_speakers)\n",
    "\n",
    "len(selected_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d1bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validate that no duplicate convo or speaker selected.\n",
    "selected_convos = []\n",
    "selected_sps = []\n",
    "for sp_id, (convo1, convo2), (convo3, convo4) in selected_pairs:\n",
    "    selected_convos.extend([get_convo_id(convo1), get_convo_id(convo2), get_convo_id(convo3), get_convo_id(convo4)])\n",
    "    all_local_speakers = get_all_speakers(convo1) + get_all_speakers(convo2) + get_all_speakers(convo3) + get_all_speakers(convo4)\n",
    "    all_local_speakers = list(set(all_local_speakers))\n",
    "    assert len(all_local_speakers) == 5\n",
    "    selected_sps.extend(all_local_speakers)\n",
    "\n",
    "assert len(selected_convos) == len(list(set(selected_convos)))\n",
    "assert len(selected_sps) == len(list(set(selected_sps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cur_convo_transcript(corpus, utt_lst):\n",
    "    # utt_list = [corpus.get_utterance(utt_id) for utt_id in utt_lst]\n",
    "    transcription = []\n",
    "    spk_list = {utt_lst[0].speaker.id : \"SPEAKER1\"}\n",
    "    for utt in utt_lst:\n",
    "        if utt.speaker.id not in spk_list.keys():\n",
    "            spk_list[utt.speaker.id] = \"SPEAKER2\"\n",
    "            assert len(spk_list) == 2\n",
    "        transcription.append(spk_list[utt.speaker.id] +\": \"+utt.text)\n",
    "    transcription = transcription[1:] ### truncate OP first message\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4300)\n",
    "need_to_compare_simiarity = {}\n",
    "\n",
    "for sp_id, selected_op_convos, selected_rp_convos in selected_pairs:\n",
    "    op1, op2 = selected_op_convos\n",
    "    rp1, rp2 = selected_rp_convos\n",
    "\n",
    "    key = f\"{sp_id}\"\n",
    "    need_to_compare_simiarity[key] = {}\n",
    "\n",
    "    need_to_compare_simiarity[key][\"op\"] = (op1, op2)\n",
    "    need_to_compare_simiarity[key][\"rp\"] = (rp1, rp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48099ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"YOUR_PATH\"\n",
    "\n",
    "if not os.path.exists(f\"{BASE_PATH}\"):\n",
    "    print(\"Making directory: \", f\"{BASE_PATH}\")\n",
    "    os.makedirs(f\"{BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ae869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Load previously saved files =======\n",
    "from convokit.convo_similarity.utils import format_transcript_from_convokit_utt_lst\n",
    "\n",
    "\n",
    "if os.path.exists(f\"{BASE_PATH}summary.json\"):\n",
    "    with open(f\"{BASE_PATH}summary.json\", \"r\") as file:\n",
    "        summaries_and_bullets = json.load(file)\n",
    "else:\n",
    "    summaries_and_bullets = {}\n",
    "\n",
    "if os.path.exists(f\"{BASE_PATH}similarity.json\"):\n",
    "    with open(f\"{BASE_PATH}similarity.json\", \"r\") as file:\n",
    "        similarity_result = json.load(file)\n",
    "else:\n",
    "    similarity_result = {}\n",
    "\n",
    "if os.path.exists(f\"{BASE_PATH}incomplete.json\"):\n",
    "    with open(f\"{BASE_PATH}incomplete.json\", \"r\") as file:\n",
    "        incomplete = set(json.load(file))\n",
    "else:\n",
    "    incomplete = set()\n",
    "\n",
    "# ======= Define thread worker function =======\n",
    "def process_key(key, need_to_compares):\n",
    "    local_summaries = {}\n",
    "    local_similarity = {}\n",
    "    local_incomplete = set()\n",
    "\n",
    "    for pair_id, (utt_lst1, utt_lst2) in need_to_compares.items():\n",
    "        if pair_id == \"op\":\n",
    "            id1 = f\"{key}###op1\"\n",
    "            id2 = f\"{key}###op2\"\n",
    "        elif pair_id == \"rp\":\n",
    "            id1 = f\"{key}###rp1\"\n",
    "            id2 = f\"{key}###rp2\"\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        for idx, utt_lst in zip([id1, id2], [utt_lst1, utt_lst2]):\n",
    "            if idx in summaries_and_bullets:\n",
    "                continue\n",
    "            transcript = format_transcript_from_convokit_utt_lst(corpus, utt_lst)\n",
    "            summary, bulletpoint = scd_writer.get_scd_and_sop(transcript)\n",
    "            local_summaries[idx] = {\"summary\": summary, \"bulletpoint\": bulletpoint}\n",
    "\n",
    "        id_pair_key_1 = f'{id1}_{id2}'\n",
    "        id_pair_key_2 = f'{id2}_{id1}'\n",
    "\n",
    "        if id_pair_key_1 not in similarity_result and id_pair_key_1 not in incomplete \\\n",
    "           and id_pair_key_2 not in similarity_result and id_pair_key_2 not in incomplete:\n",
    "            score, result, inc = condyns.compute_bidirectional_similarity(\n",
    "                corpus, id1, id2, utt_lst1, utt_lst2,\n",
    "                {**summaries_and_bullets, **local_summaries},\n",
    "                incomplete\n",
    "            )\n",
    "            local_similarity[id_pair_key_1] = {\"score\": score, \"result\": result}\n",
    "            local_incomplete.update(inc)\n",
    "\n",
    "    return local_summaries, local_similarity, local_incomplete\n",
    "\n",
    "# ======= Run multi-threaded processing =======\n",
    "with ThreadPoolExecutor(max_workers=25) as executor:\n",
    "    futures = [\n",
    "        executor.submit(process_key, key, need_to_compares)\n",
    "        for key, need_to_compares in need_to_compare_simiarity.items()\n",
    "    ]\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        local_summaries, local_similarity, local_incomplete = future.result()\n",
    "        summaries_and_bullets.update(local_summaries)\n",
    "        similarity_result.update(local_similarity)\n",
    "        incomplete.update(local_incomplete)\n",
    "\n",
    "# ======= Save back to files =======\n",
    "with open(f\"{BASE_PATH}summary.json\", \"w\") as file:\n",
    "    json.dump(summaries_and_bullets, file, indent=4)\n",
    "\n",
    "with open(f\"{BASE_PATH}similarity.json\", \"w\") as file:\n",
    "    json.dump(similarity_result, file, indent=4)\n",
    "\n",
    "with open(f\"{BASE_PATH}incomplete.json\", \"w\") as file:\n",
    "    json.dump(list(incomplete), file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_similarity = []\n",
    "rp_similarity = []\n",
    "\n",
    "for key, need_to_compares in tqdm(need_to_compare_simiarity.items()):\n",
    "    for pair_id, _ in need_to_compares.items():\n",
    "        if pair_id == \"op\":\n",
    "            id1 = f\"{key}###op1\"\n",
    "            id2 = f\"{key}###op2\"\n",
    "            sim_key = f'{id1}_{id2}' if f'{id1}_{id2}' in similarity_result.keys() else f'{id2}_{id1}'\n",
    "            assert sim_key in similarity_result.keys()\n",
    "            op_similarity.append(np.mean(similarity_result[sim_key]['score']))\n",
    "        elif pair_id == \"rp\":\n",
    "            id1 = f\"{key}###rp1\"\n",
    "            id2 = f\"{key}###rp2\"\n",
    "            sim_key = f'{id1}_{id2}' if f'{id1}_{id2}' in similarity_result.keys() else f'{id2}_{id1}'\n",
    "            assert sim_key in similarity_result.keys()\n",
    "            rp_similarity.append(np.mean(similarity_result[sim_key]['score']))\n",
    "        else:\n",
    "            raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.mean(op_similarity), 4), round(np.mean(rp_similarity), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat, p_value = stats.ttest_ind(op_similarity, rp_similarity, equal_var=False)\n",
    "print(f\"T-Test p-value {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ad3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Make sure they're lists of the same length\n",
    "assert len(op_similarity) == len(rp_similarity)\n",
    "\n",
    "stat, p_value = wilcoxon(op_similarity, rp_similarity, alternative='two-sided')\n",
    "\n",
    "print(f\"Wilcoxon statistic: {stat}\")\n",
    "print(f\"One-sided p-value: {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "balance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
