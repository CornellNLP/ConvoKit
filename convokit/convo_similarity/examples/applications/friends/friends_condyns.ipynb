{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f96a9a8",
   "metadata": {},
   "source": [
    "# Application of ConDynS on Friends Dataset\n",
    "\n",
    "Dataset information can be found: https://convokit.cornell.edu/documentation/friends.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d45fb-18f6-42ed-ab8e-74ef07c19cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from convokit import Corpus, download\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import scipy.stats as stats\n",
    "from itertools import combinations\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "import string\n",
    "\n",
    "from convokit.genai.genai_config import GenAIConfigManager\n",
    "from convokit.convo_similarity.summary import SCDWriter\n",
    "from convokit.convo_similarity.condyns import ConDynS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774f366-92a2-47ba-a450-a5b67f1a3c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading friends-corpus to /reef/sj597_kz88/scd-sim/wiki_exploration/friends-corpus\n",
      "Downloading friends-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/friends-corpus/friends-corpus.zip (6.1MB)... Done\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download(\"friends-corpus\", data_dir = \"YOUR DATA PATH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "201581ef-6ccc-4b52-a412-b305f0ef5a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica Geller : Here you go. You can wear this.\n",
      "Phoebe Buffay : Thanks!\n",
      "Hold Voice : Please, stay on the line. Your call is important to us.\n",
      "Chandler Bing : Hey! Can you take a duck and a chick to the theatre?\n"
     ]
    }
   ],
   "source": [
    "convo = corpus.random_conversation()\n",
    "utt_lst = convo.get_utterance_ids()\n",
    "speaker_ids = {}\n",
    "transcript = \"\"\n",
    "for utt_id in utt_lst:\n",
    "    utt = corpus.get_utterance(utt_id)\n",
    "    if \"TRANSCRIPT_NOTE\" not in utt.speaker.id:\n",
    "        if utt.speaker.id not in speaker_ids:\n",
    "            print(utt.speaker.id, \":\", utt.text)\n",
    "            speaker_ids[utt.speaker.id] = 1 + len(speaker_ids)\n",
    "        transcript += \"Speaker\"+str(speaker_ids[utt.speaker.id]) + \" : \" + utt.text+ \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44744176",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup path for data and corpus ###\n",
    "\n",
    "DATA_PATH = \"./data\"\n",
    "filepath = DATA_PATH + \"PATH TO WIKI GERMAN DATA\"\n",
    "\n",
    "### Set up config for GenAI ###\n",
    "config = GenAIConfigManager() ### make sure to set your own config if this is never set before\n",
    "\n",
    "### Select which model provider to use for ConDynS ###\n",
    "MODEL_PROVIDER = \"gemini\"\n",
    "MODEL = \"gemini-2.0-flash-001\"\n",
    "config.set_google_cloud_config(\"YOUR PROJECT\", \"YOUR LOCATION\")\n",
    "\n",
    "with open(filepath, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "random.seed(4300)\n",
    "dataset = random.sample(dataset, 100)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a318dd33-d2e9-4fec-9e1d-79b97c4c5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_summary_prompt = \"\"\"\n",
    "Write a short summary capturing the trajectory of a casual conversation.\n",
    "Do not include specific topics, events, or arguments from the conversation. The style you should avoid:\n",
    "Example Sentence 1: “Speaker1 said they had a difficult day at work, and mentioned that their boss was unfair. Speaker2 listened and agreed that bosses can be tough, then suggested they go out for dinner to forget about it..”\n",
    "\n",
    "Instead, do include indicators of sentiments (e.g., warmth, empathy, humor, nostalgia, vulnerability, support), individual intentions (e.g., building rapport, offering reassurance, seeking validation, self-disclosure, active listening, gentle disagreement, creating distance), and conversational strategies (if any) such as 'collaborative storytelling', 'inside jokes', 'mirroring emotions,' and 'affectionate teasing.'\n",
    "The following sentences demonstrate the style you should follow:\n",
    "* Example Sentence 2: “Both speakers have similar feelings and appeared mutually supportive. Speaker1 initiates with a moment of self-disclosure, and Speaker2 responds with empathy and validation. Both speakers build on this exchange, strengthening their rapport.”\n",
    "* Example Sentence 3: “The two speakers connected with back-and-forth affectionate teasing. Throughout the conversation, they kept building on each other’s humor with playful remarks, creating a lighthearted and comfortable discussion.”\n",
    "\n",
    "Overall, the trajectory summary should capture the key moments where the emotional connection of the conversation notably changes. Here is an example of a complete trajectory summary.\n",
    "Trajectory Summary: The conversation begins with two speakers exchanging neutral, surface-level comments. Speaker1 then shifts the tone by sharing a personal anecdote, prompting Speaker2 to respond with warmth and empathy. Speaker1 elaborates on their story and their need, but Speaker2 does not extend their support but retracts it.\n",
    "Now, provide the trajectory summary for the following conversation.\n",
    "Conversation Transcript:{transcript}\n",
    "Now, summarize this conversation. Remember, do not include specific topics, claims, or arguments from the conversation. Instead, try to capture the speakers' sentiments, intentions, and conversational/persuasive strategies. Limit the trajectory summary to 80 words.\n",
    "Trajectory Summary:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c3627-f521-4dde-99b4-2be056c48945",
   "metadata": {},
   "outputs": [],
   "source": [
    "scd_writer = SCDWriter(model_provider=MODEL_PROVIDER, \n",
    "                       model=MODEL, \n",
    "                       config=config, \n",
    "                       custom_scd_prompt=friends_summary_prompt, \n",
    "                       custom_prompt_dir=\"friends_prompts\")\n",
    "condyns = ConDynS(model_provider=MODEL_PROVIDER, \n",
    "                  model=MODEL, \n",
    "                  config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b32c5-73a9-455e-bb97-1871259a3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_friends_transcript_from_convokit(corpus, convo_id):\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    utt_lst = convo.get_utterance_ids()\n",
    "    speaker_ids = {}\n",
    "    transcript = \"\"\n",
    "    for utt_id in utt_lst:\n",
    "        utt = corpus.get_utterance(utt_id)\n",
    "        if \"TRANSCRIPT_NOTE\" not in utt.speaker.id:\n",
    "            if utt.speaker.id not in speaker_ids:\n",
    "                speaker_ids[utt.speaker.id] = 1 + len(speaker_ids)\n",
    "            transcript += \"Speaker\"+str(speaker_ids[utt.speaker.id]) + \" : \" + utt.text+ \"\\n\\n\"\n",
    "    return transcript\n",
    "\n",
    "def count_real_utterance_num(convo_id):\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    utt_lst = convo.get_utterance_ids()\n",
    "    count = 0\n",
    "    for utt_id in utt_lst:\n",
    "        utt = corpus.get_utterance(utt_id)\n",
    "        if \"TRANSCRIPT_NOTE\" not in utt.speaker.id:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de77132b-b86b-402f-902e-4d969a92b360",
   "metadata": {},
   "source": [
    "# Generating Sop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04052281-6acf-443d-a00a-682eed209cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4300)\n",
    "convo_ids = []\n",
    "while len(convo_ids) < 100:\n",
    "    convo_id =  random.choice(corpus.get_conversation_ids())\n",
    "    if count_real_utterance_num(convo_id) >= 4:\n",
    "        convo_ids.append(convo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c24b32-8c21-4c24-8149-d6723e46ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SCDs for conversations: 100%|██████| 100/100 [01:47<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "time_analysis_scd = {}\n",
    "bulletpoints = {}\n",
    "for convo_id in tqdm(convo_ids, desc=\"Generating SCDs and SoPs for conversations\"):\n",
    "    convo = corpus.get_conversation(convo_id)\n",
    "    utt_lst = convo.get_utterance_ids()\n",
    "    transcript = format_friends_transcript_from_convokit(corpus, convo_id)\n",
    "    scd, sop = scd_writer.get_scd_and_sop(friends_summary_prompt.format(transcript=transcript))\n",
    "    time_analysis_scd[convo_id] = scd\n",
    "    bulletpoints[convo_id] = sop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43597d39-7f34-436f-a895-f0fc3a0d7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + f\"friends_100_scd.json\", 'w') as file:\n",
    "    json.dump(time_analysis_scd, file, indent=4)\n",
    "\n",
    "with open(DATA_PATH + f\"friends_100_sop.json\", 'w') as file:\n",
    "    json.dump(bulletpoints, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb477181-fc75-4664-916c-05230bc35b47",
   "metadata": {},
   "source": [
    "# Calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ae109-5ac0-446e-b7b2-17ae4a5556d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating pairs similarity: 100%|█████████| 1225/1225 [11:05<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "num = 50\n",
    "all_combos = list(combinations(convo_ids[:num], 2))\n",
    "convo_scores = {}\n",
    "for convo_id1, convo_id2 in tqdm(all_combos, desc=\"Calculating pairs similarity\"):\n",
    "    if convo_id1 + \"_\" + convo_id2 in convo_scores or convo_id2 + \"_\" + convo_id1 in convo_scores:\n",
    "        continue\n",
    "    convo1 = corpus.get_conversation(convo_id1)\n",
    "    convo2 = corpus.get_conversation(convo_id2)\n",
    "    transcript1 = \"\\n\\n\".join(format_friends_transcript_from_convokit(corpus, convo_id1))\n",
    "    transcript2 = \"\\n\\n\".join(format_friends_transcript_from_convokit(corpus, convo_id2))\n",
    "\n",
    "    sop1 = bulletpoints[convo_id1]\n",
    "    sop2 = bulletpoints[convo_id2]\n",
    "    \n",
    "    result = condyns.compute_bidirectional_similarity(transcript1, transcript2, sop1, sop2)\n",
    "    score = condyns.compute_score_from_results(result)\n",
    "    \n",
    "    convo_scores[convo_id1 + \"_\" + convo_id2][\"result\"] = result\n",
    "    convo_scores[convo_id1 + \"_\" + convo_id2][\"score\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c6b08-5fb7-4cee-941a-a1576be95aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + f\"friends_50_scores.json\", 'w') as file:\n",
    "    json.dump(convo_scores, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f13ab-57fd-45e8-9001-97be28310f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(convo_id1, convo_id2):\n",
    "    if convo_id1 + \"_\" + convo_id2 in convo_scores:\n",
    "        return convo_scores[convo_id1 + \"_\" + convo_id2][\"score\"]\n",
    "    elif convo_id2 + \"_\" + convo_id1 in convo_scores:\n",
    "        return convo_scores[convo_id2 + \"_\" + convo_id1][\"score\"]\n",
    "    else:\n",
    "        print(\"Did not find the score\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e0b78",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ca1ec-5f81-46dd-88ba-2a45d77909a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the distance matrix\n",
    "n = len(convo_ids[:num])\n",
    "distance_matrix = np.zeros((n, n))\n",
    "\n",
    "# Fill the distance matrix\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        convo1, convo2 = convo_ids[i], convo_ids[j]\n",
    "        similarity = np.sum(get_similarity(convo1, convo2))\n",
    "        distance = 2 - similarity  # Convert similarity to distance\n",
    "        distance_matrix[i, j] = distance_matrix[j, i] = distance  # Symmetric matrix\n",
    "\n",
    "# Convert to condensed format for linkage function\n",
    "condensed_dist_matrix = squareform(distance_matrix)\n",
    "\n",
    "# Step 2: Perform hierarchical clustering\n",
    "linkage_matrix = linkage(condensed_dist_matrix, method=\"ward\")  # Ward's method minimizes variance\n",
    "\n",
    "top_level_clusters = fcluster(linkage_matrix, t=2, criterion='maxclust')\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "for idx, label in enumerate(top_level_clusters):\n",
    "    clusters[label].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/jmhessel/FightingWords/blob/master/fighting_words_py3.py\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def basic_sanitize(in_string):\n",
    "    '''Returns a very roughly sanitized version of the input string.'''\n",
    "    in_string = ''.join([ch for ch in in_string if ch not in exclude])\n",
    "    in_string = in_string.lower()\n",
    "    in_string = ' '.join(in_string.split())\n",
    "    return in_string\n",
    "\n",
    "def bayes_compare_language(l1, l2, ngram = 1, prior=.01, cv = None):\n",
    "    '''\n",
    "    Arguments:\n",
    "    - l1, l2; a list of strings from each language sample\n",
    "    - ngram; an int describing up to what n gram you want to consider (1 is unigrams,\n",
    "    2 is bigrams + unigrams, etc). Ignored if a custom CountVectorizer is passed.\n",
    "    - prior; either a float describing a uniform prior, or a vector describing a prior\n",
    "    over vocabulary items. If you're using a predefined vocabulary, make sure to specify that\n",
    "    when you make your CountVectorizer object.\n",
    "    - cv; a sklearn.feature_extraction.text.CountVectorizer object, if desired.\n",
    "\n",
    "    Returns:\n",
    "    - A list of length |Vocab| where each entry is a (n-gram, zscore) tuple.'''\n",
    "    if cv is None and type(prior) is not float:\n",
    "        print(\"If using a non-uniform prior:\")\n",
    "        print(\"Please also pass a count vectorizer with the vocabulary parameter set.\")\n",
    "        quit()\n",
    "    l1 = [basic_sanitize(l) for l in l1]\n",
    "    l2 = [basic_sanitize(l) for l in l2]\n",
    "    if cv is None:\n",
    "        cv = CV(decode_error = 'ignore', min_df=2, max_df=0.9, ngram_range=(1,ngram),\n",
    "                binary = False,\n",
    "                max_features = 15000)\n",
    "    counts_mat = cv.fit_transform(l1+l2).toarray()\n",
    "    # Now sum over languages...\n",
    "    vocab_size = len(cv.vocabulary_)\n",
    "    print(\"Vocab size is {}\".format(vocab_size))\n",
    "    if type(prior) is float:\n",
    "        priors = np.array([prior for i in range(vocab_size)])\n",
    "    else:\n",
    "        priors = prior\n",
    "    z_scores = np.empty(priors.shape[0])\n",
    "    count_matrix = np.empty([2, vocab_size], dtype=np.float32)\n",
    "    count_matrix[0, :] = np.sum(counts_mat[:len(l1), :], axis = 0)\n",
    "    count_matrix[1, :] = np.sum(counts_mat[len(l1):, :], axis = 0)\n",
    "    a0 = np.sum(priors)\n",
    "    n1 = 1.*np.sum(count_matrix[0,:])\n",
    "    n2 = 1.*np.sum(count_matrix[1,:])\n",
    "    print(\"Comparing language...\")\n",
    "    for i in range(vocab_size):\n",
    "        #compute delta\n",
    "        term1 = np.log((count_matrix[0,i] + priors[i])/(n1 + a0 - count_matrix[0,i] - priors[i]))\n",
    "        term2 = np.log((count_matrix[1,i] + priors[i])/(n2 + a0 - count_matrix[1,i] - priors[i]))\n",
    "        delta = term1 - term2\n",
    "        #compute variance on delta\n",
    "        var = 1./(count_matrix[0,i] + priors[i]) + 1./(count_matrix[1,i] + priors[i])\n",
    "        #store final score\n",
    "        z_scores[i] = delta/np.sqrt(var)\n",
    "    index_to_term = {v:k for k,v in cv.vocabulary_.items()}\n",
    "    sorted_indices = np.argsort(z_scores)\n",
    "    return_list = []\n",
    "    for i in sorted_indices:\n",
    "        return_list.append((index_to_term[i], z_scores[i]))\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53140ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fighting_words_matching_bullets(cluster1, cluster2, similarity_result=convo_scores, summaries_and_bullets=similarity_and_bulletpoints):\n",
    "    cluster1_combo = list(combinations(cluster1, 2))\n",
    "    matched_cluster1 = []\n",
    "    for convo_id1, convo_id2 in cluster1_combo:\n",
    "        key = f\"{convo_id1}_{convo_id2}\" if f\"{convo_id1}_{convo_id2}\" in similarity_result.keys() else f\"{convo_id2}_{convo_id1}\"\n",
    "        for k, result in enumerate(similarity_result[key][\"result\"]):\n",
    "            for index in result.keys():\n",
    "                if result[index]['score'] > 0.5:\n",
    "                    if k == 0:\n",
    "                        matched_cluster1.append(summaries_and_bullets['bulletpoints'][convo_id1][index])\n",
    "                    else:\n",
    "                        try:\n",
    "                            matched_cluster1.append(summaries_and_bullets['bulletpoints'][convo_id2][index])\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        \n",
    "    cluster2_combo = list(combinations(cluster2, 2))\n",
    "    matched_cluster2 = []\n",
    "    for convo_id1, convo_id2 in cluster2_combo:\n",
    "        key = f\"{convo_id1}_{convo_id2}\" if f\"{convo_id1}_{convo_id2}\" in similarity_result.keys() else f\"{convo_id2}_{convo_id1}\"\n",
    "        for k, result in enumerate(similarity_result[key][\"result\"]):\n",
    "            for index in result.keys():\n",
    "                if result[index]['score'] > 0.5:\n",
    "                    if k == 0:\n",
    "                        matched_cluster2.append(summaries_and_bullets['bulletpoints'][convo_id1][index])\n",
    "                    else:\n",
    "                        matched_cluster2.append(summaries_and_bullets['bulletpoints'][convo_id2][index])\n",
    "    \n",
    "    z_scores = bayes_compare_language(matched_cluster1, matched_cluster2, ngram = 3) \n",
    "    top_k = 15\n",
    "    top_k_class1 = list(reversed([(x[0], round(x[1],2)) for x in z_scores[-top_k:]]))\n",
    "    top_k_class2 = [(x[0], round(x[1],2)) for x in z_scores[:top_k]]\n",
    "    top_k_class1 = list(reversed([(x[0], round(x[1],2)) for x in z_scores[-top_k:]]))\n",
    "    top_k_class2 = [(x[0], round(x[1],2)) for x in z_scores[:top_k]]\n",
    "    print(f\"Fighting Words Comments between:\")\n",
    "    print(\"Cluster1: \", top_k_class1)\n",
    "    print(\"Cluster2: \", top_k_class2)\n",
    "    return matched_cluster1, matched_cluster2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61420259-1d1c-4b5e-9dce-ce416739a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1_ids = [convo_ids[i] for i in clusters[1]]\n",
    "cluster2_ids = [convo_ids[i] for i in clusters[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc70072",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1_bulletpoints, cluster2_bulletpoints = get_fighting_words_matching_bullets(cluster1_ids, cluster2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1_bulletpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bafb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2_bulletpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sj597-env",
   "language": "python",
   "name": "sj597-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
