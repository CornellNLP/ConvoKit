{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Tennis Corpus with Surprise\n",
    "This demo is based on the [Tie-breaker paper](https://www.cs.cornell.edu/~liye/tennis.html) on gender-bias in sports journalism. We compare utterances to a language model using cross entropy, as implemented by the Surprise transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/tushaar/Downloads/Cornell/Research/ConvoKit/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import convokit\n",
    "from convokit import Surprise, ConvoKitLanguageModel, Kenlm\n",
    "from convokit import Corpus, Speaker, Utterance, download\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create corpus using tennis game commentary dataset\n",
    "This dataset consists of a gender-balanced set of play-by-play commentaries from tennis matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../../examples' # replace with your path to tennis_data directory\n",
    "data_dir = f'{PATH}/tennis_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_speakers = {'COMMENTATOR': Speaker(id = 'COMMENTATOR', meta = {})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + 'text_commentaries.json', 'r') as f:\n",
    "    commentaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a63826376804976a9c7dcc38fa6233a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utterances = []\n",
    "count = 0\n",
    "for c in tqdm(commentaries):\n",
    "    idx = 'c{}'.format(count)\n",
    "    meta = {'player_gender': c['gender'], 'scoreline': c['scoreline']}\n",
    "    utterances.append(Utterance(id=idx, speaker=corpus_speakers['COMMENTATOR'], \n",
    "                                conversation_id=idx, text=c['commentary'], meta=meta))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_commentary_corpus = Corpus(utterances=utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load interview corpus\n",
    "This dataset contains transcripts from post-match press conferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/tushaar/.convokit/downloads/tennis-corpus\n"
     ]
    }
   ],
   "source": [
    "interview_corpus = Corpus(filename=download('tennis-corpus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 359\n",
      "Number of Utterances: 163948\n",
      "Number of Conversations: 81974\n"
     ]
    }
   ],
   "source": [
    "interview_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with the analysis, let's add a metadata attribute `'player_gender'` to each utterance that is a reporter question describing the gender of the player the question is posed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for utt in interview_corpus.iter_utterances(selector=lambda u: u.meta['is_question']):\n",
    "    utt.add_meta('player_gender', \n",
    "                 utt.get_conversation().get_utterance(utt.id.replace('q', 'a')).get_speaker().meta['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: How surprising is each interview question based on typical language used to describe tennis?\n",
    "\n",
    "For this demo, we want to train one model for the entire game language corpus, so we'll make our `model_key_selector` a function that returns the same key for every utterance in a corpus. We will use a custom tokenizer to convert to lowercase and remove punctuation. We will set the `context_sample_size` parameter to `None`, so that the entire game commentary corpus is used as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenizer_alnum(text):\n",
    "    return list(filter(lambda w: w.isalnum(), word_tokenize(text.lower())))\n",
    "\n",
    "def tokenizer_lower(text):\n",
    "    tokens = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        tokens += (word_tokenize(sentence.lower()) + ['\\n'])\n",
    "    return tokens\n",
    "\n",
    "surp = Surprise(model_key_selector=lambda utt: 'corpus', target_sample_size=10, tokenizer=tokenizer_lower,\n",
    "                context_sample_size=None, n_samples=3, n_jobs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we just want to look at how surprising questions asked by reporters are, we'll fit the transformer just on utterances that are questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a481a41699d04c3fb7cf54f7220d140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fit: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33520f924994f748e07c934092a5efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fit:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "surp = surp.fit(game_commentary_corpus, \n",
    "                text_func=lambda utt: [' '.join([u.text for u in game_commentary_corpus.iter_utterances()])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the demo, we'll select a random subset of interview questions to compute surprise scores for. To run the demo on the entire interview corpus, set `SAMPLE` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "SAMPLE = True\n",
    "SAMPLE_SIZE = 500  # edit this to change the number of interview questions to calculate surprise for\n",
    "\n",
    "subset_utts = \\\n",
    "    [interview_corpus.get_utterance(utt)\n",
    "     for utt in interview_corpus.get_utterances_dataframe(selector=lambda utt: \n",
    "                                                          utt.meta['is_question']).sample(SAMPLE_SIZE).index]\n",
    "subset_corpus = Corpus(utterances=subset_utts) if SAMPLE else interview_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we want to select only utterances that are questions to compute surprise for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_persistent': True,\n",
      " 'kenlm_path': '/Users/tushaar/kenlm',\n",
      " 'model_filename': 'kenlm_surprise',\n",
      " 'model_type': 'kenlm',\n",
      " 'models_dir': '../../../../examples/kenlm_models',\n",
      " 'n_jobs': 1,\n",
      " 'ngram_order': 2}\n"
     ]
    }
   ],
   "source": [
    "kenlm = Kenlm(kenlm_path='/Users/tushaar/kenlm', models_dir=f'{PATH}/kenlm_models', \n",
    "              model_filename='kenlm_surprise', is_persistent=True)\n",
    "pp.pprint(kenlm.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16b1d229eaa4a90a223a67eb932d16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transform: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subset_corpus = surp.transform(subset_corpus, obj_type='utterance',\n",
    "                               selector=lambda utt: utt.meta['is_question'], \n",
    "                               language_model=kenlm, eval_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Let's take a look at the average surprise score for questions posed to female players compared to those posed to male players. Based on results from the Tie-breaker paper, we should expect to see a higher average surprise score for questions posed to female players. A higher average surprise would indicate that questions asked to female players tend to be more different from typical tennis language. This may mean that female players are being asked questions that are less relevant to tennis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = subset_corpus.get_utterances_dataframe(selector=lambda utt: utt.meta['is_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.54281806945801"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "get_scores = lambda utterances: pd.Series([score['corpus']for score in utterances], index=utterances.index)\n",
    "\n",
    "female_qs = get_scores(utterances[utterances['meta.player_gender'] == 'F']['meta.surprise']).dropna()\n",
    "female_qs.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.73420365651448"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_qs = get_scores(utterances[utterances['meta.player_gender'] == 'M']['meta.surprise']).dropna()\n",
    "male_qs.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this demo multiple times, we see that sometimes the average surprise for female players is higher than male players, but sometimes it is lower. This may be due to the random sampling used by the Surprise transformer when selecting targets and contexts. Another possible explanation for the difference in results from the Tie-breaker paper may be that the paper used a bigram language model with modified Kneser-Nay smoothing. Our transformer currently only allows for unigram language models and add one Laplace smoothing. These differences may explain why we do not get the same statistically significant results as the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most and least surprising questions posed to each gender, we can see that the surprise scores assigned seem to make sense. The least surprising questions seem to relate well to the game of tennis while the most surprising focus on other things such as fashion choices or social lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_female_qs = female_qs.sort_values().keys()\n",
    "sorted_male_qs = male_qs.sort_values().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does she give you a program of what you need to do?\n",
      "How did you feel today? Sleepy? Awake? Energetic?\n",
      "Congratulations. That seemed like a very strong win for you. How did you feel about your performance?\n",
      "That could be against Sabine. I don't know the score right now.\n",
      "That was a highlevel match today. You start off pretty well and fall off. I saw the entire match. What happened then? Because you had more troubles for the serve, I think.\n"
     ]
    }
   ],
   "source": [
    "for utt in sorted_female_qs[:5]:\n",
    "    print(interview_corpus.get_utterance(utt).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You had a bit of a slow start. Was that nerves out there?\n",
      "Is it fair to say that last year you had this opponent who beat you, is that in your mind or is it old or do you want to beat him more?\n",
      "Rafael Nadal said something about a special tax regime in UK tournaments yesterday. What is your feeling about that issue?\n",
      "Is that a sign of lack of confidence?\n",
      "When you're remembering it, are you seeing it, too?\n"
     ]
    }
   ],
   "source": [
    "for utt in sorted_male_qs[:5]:\n",
    "    print(interview_corpus.get_utterance(utt).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With this win and also the win at the US Open, wondering if you think that you have an edge if this match goes to three sets against her? Just physically and mentally seems you've been able to outlast her in these tough, grueling matches.\n",
      "He didn't say Carlos sent an email or anything?\n",
      "Gulbis recently talked about top players in general?\n",
      "This is only your third Grand Slam main draw match. Do you think even without all the difficulties you've had this year you would still consider that a reasonably good success rate, to win your third main draw Grand Slam match?\n",
      "Your ambitions for the year have been enhanced by this tournament?  What had you hoped to do first of the year coming into here?\n"
     ]
    }
   ],
   "source": [
    "for utt in sorted_female_qs[-1:-6:-1]:\n",
    "    print(interview_corpus.get_utterance(utt).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janko Tipsarevic came through after being down two sets. He said he hates the idea of being called a top10 player because you have this expectation that you're going to walk in and crush everybody. He felt that his energy level was down. Do you have to worry at all in these early rounds that you're overconfident going into matches?\n",
      "Did you take anything from John Millman's performance last night?\n",
      "One of the TV guys told me you said you had blistered hands.\n",
      "Based on today's performance, do you think you can repeat your good result here last year?\n",
      "Can you give us an assessment of what you've been doing differently this year regarding previous years at Wimbledon?\n"
     ]
    }
   ],
   "source": [
    "for utt in sorted_male_qs[-1:-6:-1]:\n",
    "    print(interview_corpus.get_utterance(utt).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: How surprising is a question compared to all questions posed to male players and all questions posed to female players?\n",
    "\n",
    "Let's see how surprising questions are compared to questions posed to players of each gender. To do this, we'll want to make our `model_key_selector` return a key based on the player's gender. Recall that we added `'player_gender'` as a metadata field to each question earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_models_surp = Surprise(model_key_selector=lambda utt: utt.meta['player_gender'],\n",
    "                              target_sample_size=10, context_sample_size=5000,\n",
    "                              surprise_attr_name='surprise_gender_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9e3f7901064d84ac246ca0ed036d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fit: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a754d05ce3849649a81756bad73c32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fit:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_models_surp = gender_models_surp.fit(interview_corpus, \n",
    "                                            selector=lambda utt: utt.meta['is_question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for each question, we want to compute surprise based on both the male interview questions model and the female interview questions model, we will use the `group_and_models` parameter for the `transform` function. Each utterance should belong to it's own group and be compared to both the `'M'` and `'F'` gender models. \n",
    "\n",
    "Since each utterance belongs to only one group, we want the surprise attribute keys to just correspond to the model. We use the `group_model_attr_key` parameter to define this. This attribute takes in a group name (which will be the utterance id) and a model key (which will be either 'M' or 'F') and returns the corresponding key that should be added to the surprise metadata. For this case, we simply return the model key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "convokit_lm = ConvoKitLanguageModel(smooth=True)\n",
    "pp.pprint(convokit_lm.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus = \\\n",
    "    gender_models_surp.transform(subset_corpus, obj_type='utterance', \n",
    "                                 group_and_models=lambda utt: (utt.id, ['M', 'F']), \n",
    "                                 group_model_attr_key=lambda _, m: m,\n",
    "                                 selector=lambda utt: utt.meta['is_question'], \n",
    "                                 language_model=convokit_lm, eval_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Let's take a look at the surprise scores. We see that questions posed to a certain gendered player are on average more surprising when compared to all questions posed to the other gender. From this we can surmise that there may be some difference in the types of questions posed to each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = subset_corpus.get_utterances_dataframe(selector=lambda utt: utt.meta['is_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.820603795831438"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[utterances['meta.player_gender'] == 'F'] \\\n",
    "          ['meta.surprise_gender_model'].map(lambda x: x['M']).dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.773426120991382"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[utterances['meta.player_gender'] == 'F'] \\\n",
    "          ['meta.surprise_gender_model'].map(lambda x: x['F']).dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.79686635301196"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[utterances['meta.player_gender'] == 'M'] \\\n",
    "          ['meta.surprise_gender_model'].map(lambda x: x['M']).dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.830856237083713"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[utterances['meta.player_gender'] == 'M'] \\\n",
    "          ['meta.surprise_gender_model'].map(lambda x: x['F']).dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
