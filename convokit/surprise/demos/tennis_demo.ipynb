{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Tennis Corpus with Surprise\n",
    "This demo is based on the [Tie-breaker paper](https://www.cs.cornell.edu/~liye/tennis.html) on gender-bias in sports journalism. We compare utterances to a language model using cross entropy, as implemented by the Surprise transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import convokit\n",
    "from convokit import Surprise, ConvoKitLanguageModel, Kenlm\n",
    "from convokit import Corpus, Speaker, Utterance, download\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create corpus using tennis game commentary dataset\n",
    "This dataset consists of a gender-balanced set of play-by-play commentaries from tennis matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../../../examples' # replace with your path to tennis_data directory\n",
    "data_dir = f'{PATH}/tennis_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_speakers = {'COMMENTATOR': Speaker(id = 'COMMENTATOR', meta = {})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir + 'text_commentaries.json', 'r') as f:\n",
    "    commentaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef5f528cab74142aca4e45705e3e631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utterances = []\n",
    "count = 0\n",
    "for c in tqdm(commentaries):\n",
    "    idx = 'c{}'.format(count)\n",
    "    meta = {'player_gender': c['gender'], 'scoreline': c['scoreline']}\n",
    "    utterances.append(Utterance(id=idx, speaker=corpus_speakers['COMMENTATOR'], \n",
    "                                conversation_id=idx, text=c['commentary'], meta=meta))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_commentary_corpus = Corpus(utterances=utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load interview corpus\n",
    "This dataset contains transcripts from post-match press conferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/tushaar/.convokit/downloads/tennis-corpus\n"
     ]
    }
   ],
   "source": [
    "interview_corpus = Corpus(filename=download('tennis-corpus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 359\n",
      "Number of Utterances: 163948\n",
      "Number of Conversations: 81974\n"
     ]
    }
   ],
   "source": [
    "interview_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with the analysis, let's add a metadata attribute `'player_gender'` to each utterance that is a reporter question describing the gender of the player the question is posed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for utt in interview_corpus.iter_utterances(selector=lambda u: u.meta['is_question']):\n",
    "    utt.add_meta('player_gender', \n",
    "                 utt.get_conversation().get_utterance(utt.id.replace('q', 'a')).get_speaker().meta['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: How surprising is each interview question based on typical language used to describe tennis?\n",
    "\n",
    "For this demo, we want to train one model for the entire game language corpus, so we'll make our `model_key_selector` a function that returns the same key for every utterance in a corpus. We will use a custom tokenizer to convert to lowercase and remove punctuation. We will set the `context_sample_size` parameter to `None`, so that the entire game commentary corpus is used as the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenizer_alnum(text):\n",
    "    return list(filter(lambda w: w.isalnum(), word_tokenize(text.lower())))\n",
    "\n",
    "def tokenizer_lower(text):\n",
    "    tokens = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        tokens += (word_tokenize(sentence.lower()) + ['\\n'])\n",
    "    return tokens\n",
    "\n",
    "surp = Surprise(model_key_selector=lambda utt: 'corpus', target_sample_size=10, tokenizer=tokenizer_lower,\n",
    "                context_sample_size=None, n_samples=3, n_jobs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we just want to look at how surprising questions asked by reporters are, we'll fit the transformer just on utterances that are questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surp = surp.fit(game_commentary_corpus, \n",
    "                text_func=lambda utt: [' '.join([u.text for u in game_commentary_corpus.iter_utterances()])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up the demo, we'll select a random subset of interview questions to compute surprise scores for. To run the demo on the entire interview corpus, set `SAMPLE` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "SAMPLE = True\n",
    "SAMPLE_SIZE = 500  # edit this to change the number of interview questions to calculate surprise for\n",
    "\n",
    "subset_utts = \\\n",
    "    [interview_corpus.get_utterance(utt)\n",
    "     for utt in interview_corpus.get_utterances_dataframe(selector=lambda utt: \n",
    "                                                          utt.meta['is_question']).sample(SAMPLE_SIZE).index]\n",
    "subset_corpus = Corpus(utterances=subset_utts) if SAMPLE else interview_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we want to select only utterances that are questions to compute surprise for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_persistent': True,\n",
      " 'kenlm_path': '/Users/tushaar/kenlm',\n",
      " 'model_filename': 'kenlm_surprise',\n",
      " 'model_type': 'kenlm',\n",
      " 'models_dir': '../../../../examples/kenlm_models',\n",
      " 'n_jobs': 1,\n",
      " 'ngram_order': 2}\n"
     ]
    }
   ],
   "source": [
    "# Replace with appropriate paths to your kenlm directory.\n",
    "kenlm = Kenlm(kenlm_path='/Users/tushaar/kenlm', \n",
    "              models_dir=f'{PATH}/kenlm_models', \n",
    "              model_filename='kenlm_surprise', \n",
    "              is_persistent=True)\n",
    "pp.pprint(kenlm.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus = surp.transform(subset_corpus, obj_type='utterance',\n",
    "                               selector=lambda utt: utt.meta['is_question'], \n",
    "                               language_model=kenlm, eval_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Let's take a look at the average surprise score for questions posed to female players compared to those posed to male players. Based on results from the Tie-breaker paper, we should expect to see a higher average surprise score for questions posed to female players. A higher average surprise would indicate that questions asked to female players tend to be more different from typical tennis language. This may mean that female players are being asked questions that are less relevant to tennis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = subset_corpus.get_utterances_dataframe(selector=lambda utt: utt.meta['is_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.832740783691406"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "get_scores = lambda utterances: pd.Series([score['corpus']for score in utterances], index=utterances.index)\n",
    "\n",
    "female_qs = get_scores(utterances[utterances['meta.player_gender'] == 'F']['meta.surprise']).dropna()\n",
    "female_qs.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.093317667643234"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_qs = get_scores(utterances[utterances['meta.player_gender'] == 'M']['meta.surprise']).dropna()\n",
    "male_qs.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this demo multiple times, we see that sometimes the average surprise for female players is higher than male players, but sometimes it is lower. This may be due to the random sampling used by the Surprise transformer when selecting targets and contexts. Another possible explanation for the difference in results from the Tie-breaker paper may be that the paper used a bigram language model with modified Kneser-Nay smoothing. Our transformer currently only allows for unigram language models and add one Laplace smoothing. These differences may explain why we do not get the same statistically significant results as the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the most and least surprising questions posed to each gender, we can see that the surprise scores assigned seem to make sense. The least surprising questions seem to relate well to the game of tennis while the most surprising focus on other things such as fashion choices or social lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_female_qs = female_qs.sort_values().keys()\n",
    "sorted_male_qs = male_qs.sort_values().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was it like on court? When did you sense that she was vulnerable, beatable?\n",
      "Did you advise Serena not to play there?\n",
      "Were you aware of Richard and Venus coming in for the third set? You seemed to look to them after a couple points and showed real emotion.\n",
      "Can you describe how disappointed you are right now.\n",
      "In the beginning, you were almost down 4Love. Why the slow start?\n"
     ]
    }
   ],
   "source": [
    "for utt in sorted_female_qs[:5]:\n",
    "    print(interview_corpus.get_utterance(utt).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why? What happened? What went wrong?\n",
      "Is that your best result playing on grass? You had a good Wimbledon a couple years ago.\n",
      "You seemed to have a good rhythm before the second rain break. What was the effect of the roof coming across? Did it feel different? Do you think it benefited him?\n",
      "How is it different to play Roger on grass? You have played him on every surface. How is it different to play him here?\n",
      "Why was he giving you so much trouble to start with?  Was the problem with you or him or both?\n"
     ]
    }
   ],
   "source": [
    "for utt in sorted_male_qs[:5]:\n",
    "    print(interview_corpus.get_utterance(utt).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The WTA has announces that next year your dad could come out with a tablet that shows stats. Do you think that your father would like having that access, and could you imagine what that would be like?\n",
      "She's on an incredible run. Can you assess her as an upandcoming player? Is she somebody who can be an elite player in this game?\n",
      "You've played Grand Slams before against players who were in their own country and been okay. Why the nerves? You've been in great form, haven't lost in a while.\n",
      "When you look around the locker room you must feel like the senior citizen.\n",
      "I forgot that you mentioned you're having your 27th birthday this fall.\n"
     ]
    }
   ],
   "source": [
    "for utt in sorted_female_qs[-1:-6:-1]:\n",
    "    print(interview_corpus.get_utterance(utt).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your friend Tiger Woods was here in Doral last weekend. Did you get a chance to cross paths with him or talk to him prior to this tournament?\n",
      "You've started the year at the Australian Open the last couple years and done very well. Do you feel like this is a Major tournament to you that will make a big difference in your career here this week?\n",
      "You'll obviously go home with happy memories, despite what's happened today.\n",
      "Have you changed anything in your clay court preparations from last year?\n",
      "First match out here morning after a Davis Cup stint and traveling, how did you feel? Talk about the match.\n"
     ]
    }
   ],
   "source": [
    "for utt in sorted_male_qs[-1:-6:-1]:\n",
    "    print(interview_corpus.get_utterance(utt).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: How surprising is a question compared to all questions posed to male players and all questions posed to female players?\n",
    "\n",
    "Let's see how surprising questions are compared to questions posed to players of each gender. To do this, we'll want to make our `model_key_selector` return a key based on the player's gender. Recall that we added `'player_gender'` as a metadata field to each question earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_models_surp = Surprise(model_key_selector=lambda utt: utt.meta['player_gender'],\n",
    "                              target_sample_size=10, context_sample_size=5000,\n",
    "                              surprise_attr_name='surprise_gender_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3024b19405004768ba07bcd599b33e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fit: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489621b89e8b4e6e9766b690bd24f825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fit:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_models_surp = gender_models_surp.fit(interview_corpus, \n",
    "                                            selector=lambda utt: utt.meta['is_question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for each question, we want to compute surprise based on both the male interview questions model and the female interview questions model, we will use the `group_and_models` parameter for the `transform` function. Each utterance should belong to it's own group and be compared to both the `'M'` and `'F'` gender models. \n",
    "\n",
    "Since each utterance belongs to only one group, we want the surprise attribute keys to just correspond to the model. We use the `group_model_attr_key` parameter to define this. This attribute takes in a group name (which will be the utterance id) and a model key (which will be either 'M' or 'F') and returns the corresponding key that should be added to the surprise metadata. For this case, we simply return the model key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_type': 'convokit_lm', 'n_jobs': 1, 'smooth': True}\n"
     ]
    }
   ],
   "source": [
    "convokit_lm = ConvoKitLanguageModel(smooth=True)\n",
    "pp.pprint(convokit_lm.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1113e18e6f2a492b9a5de4c5c83874d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transform: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "surprise: 100%|##########| 1/1 [00:05<00:00,  5.21s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "surprise: 100%|##########| 1/1 [00:02<00:00,  2.20s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "surprise: 100%|##########| 1/1 [00:02<00:00,  2.02s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "surprise: 100%|##########| 1/1 [00:02<00:00,  2.13s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "surprise: 100%|##########| 1/1 [00:02<00:00,  2.16s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "surprise: 100%|##########| 1/1 [00:02<00:00,  2.52s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "surprise: 100%|##########| 1/1 [00:02<00:00,  2.04s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "surprise: 100%|##########| 1/1 [00:02<00:00,  2.14s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subset_corpus = \\\n",
    "    gender_models_surp.transform(subset_corpus, obj_type='utterance', \n",
    "                                 group_and_models=lambda utt: (utt.id, ['M', 'F']), \n",
    "                                 group_model_attr_key=lambda _, m: m,\n",
    "                                 selector=lambda utt: utt.meta['is_question'], \n",
    "                                 language_model=convokit_lm, eval_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Let's take a look at the surprise scores. We see that questions posed to a certain gendered player are on average more surprising when compared to all questions posed to the other gender. From this we can surmise that there may be some difference in the types of questions posed to each gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = subset_corpus.get_utterances_dataframe(selector=lambda utt: utt.meta['is_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.804742348431906"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[utterances['meta.player_gender'] == 'F'] \\\n",
    "          ['meta.surprise_gender_model'].map(lambda x: x['M']).dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.762531083154594"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[utterances['meta.player_gender'] == 'F'] \\\n",
    "          ['meta.surprise_gender_model'].map(lambda x: x['F']).dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7774629531902235"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[utterances['meta.player_gender'] == 'M'] \\\n",
    "          ['meta.surprise_gender_model'].map(lambda x: x['M']).dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.820980867869622"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterances[utterances['meta.player_gender'] == 'M'] \\\n",
    "          ['meta.surprise_gender_model'].map(lambda x: x['F']).dropna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
