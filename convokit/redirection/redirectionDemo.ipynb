{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a0fb8e",
   "metadata": {},
   "source": [
    "# Redirection Demo in US Supreme Court oral arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4894fbb",
   "metadata": {},
   "source": [
    "This notebook demonstrates our redirection framework introduced this paper: **Taking a turn for the better: Conversation redirection throughout the course of mental-health therapy.** In the paper, we define redirection as the extent to which speakers shift the immediate focus of the conversation and applied our measure in the context of long-term messaging therapy. In this demo, we provide an initial exploration into how our redirection framework can be applied in other domains in particular to a publicly available dataset of U.S. Supreme Court oral arguments (Danescu-Niculescu-Mizil et al., 2012; Chang et al., 2020). Although court proceedings differ from therapy in terms of topics, goals, and interaction styles, their relatively unstructured and dynamic nature enables an initial exploration of how such discussions are redirected.\n",
    "\n",
    "In this setting, we focus on the interactions between justices and lawyers. The power dynamics between these distinct roles reflect the asymmetric relationship between therapists and patients in mental-health domains, where one party generally holds more influence over the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc154fb",
   "metadata": {},
   "source": [
    "We first install and import all the necessary packages from Convokit including our wrapper models and config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3314f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/site-packages (from peft) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/site-packages (from peft) (2.1.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/site-packages (from peft) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from peft) (4.67.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/site-packages (from peft) (1.1.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/site-packages (from peft) (0.26.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/site-packages (from trl) (1.1.1)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.11/site-packages (from trl) (3.1.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/site-packages (from trl) (4.46.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (2.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.12.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (4.67.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.11.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (0.20.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich->trl) (2.16.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.17.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.8.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/site-packages (from bitsandbytes) (1.14.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/site-packages (from scipy->bitsandbytes) (1.25.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q convokit\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0bc1290",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "from likelihoodModel import LikelihoodModel\n",
    "from gemmaLikelihoodModel import GemmaLikelihoodModel\n",
    "from redirectionModel import RedirectionModel\n",
    "from config import DEFAULT_BNB_CONFIG, DEFAULT_LORA_CONFIG, DEFAULT_TRAIN_CONFIG\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6fc4d3",
   "metadata": {},
   "source": [
    "We then download the `supreme-court` corpus we will be using for training and analysis. If you already have the corpus saved locally, you can specify the path to load the corpus from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "536e4b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 8979\n",
      "Number of Utterances: 1700789\n",
      "Number of Conversations: 7817\n"
     ]
    }
   ],
   "source": [
    "# If you already have the corpus saved locally, load the corpus from the saved path.\n",
    "DATA_DIR = '/Users/vian/.convokit/downloads/supreme-corpus'\n",
    "corpus = Corpus(DATA_DIR)\n",
    "\n",
    "# Otherwise download the corpus\n",
    "# corpus = Corpus(filename=download('supreme-corpus'))\n",
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f39e0",
   "metadata": {},
   "source": [
    "For the purposes of the demo, we will randomly sample a small subset of 150 conversations for our analysis. Since in this demonstration, we focus on interactions between two distinct roles of justices and lawyers, we label the speaker role for each utterance (either justice or lawyer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2496d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "convos = [convo for convo in corpus.iter_conversations()]\n",
    "sample_convos = random.sample(convos, 10)\n",
    "print(len(sample_convos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99029a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for convo in sample_convos:\n",
    "  for utt in convo.iter_utterances():\n",
    "    if utt.speaker.id.startswith(\"j_\"):\n",
    "      utt.meta[\"role\"] = \"justice\"\n",
    "    else:\n",
    "      utt.meta[\"role\"] = \"lawyer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b427dbe",
   "metadata": {},
   "source": [
    "We will use a 90/10/10 train/val/test split. We then label the conversations with their corresponding split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8ffa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1 1\n"
     ]
    }
   ],
   "source": [
    "train_convos, temp_convos = train_test_split(sample_convos, test_size=0.2, random_state=10)\n",
    "val_convos, test_convos = train_test_split(temp_convos, test_size=0.5, random_state=10)\n",
    "print(len(train_convos), len(val_convos), len(test_convos))\n",
    "\n",
    "for convo in train_convos:\n",
    "  convo.meta[\"train\"] = True\n",
    "for convo in val_convos: \n",
    "  convo.meta[\"val\"] = True \n",
    "for convo in test_convos:\n",
    "  convo.meta[\"test\"] = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b1d3c6",
   "metadata": {},
   "source": [
    "Now, we define our likelihood model responsible for computing utterance likelihoods based on provided context.The likelihood probabilities are later used to compute redirection scores for each utterance. Here, we define a likelihood model using the Gemma-2B called `GemmaLikelihodModel` which inherits from a default `LikelihoodModel` interface. Different models (Gemma, Llama, Mistral, etc.) can be supported by inheriting from this base interface. \n",
    "\n",
    "Since in this demo, we are using Gemma-2B through HuggingFace, we need to provide an authentication token for access to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf38bac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gemma_likelihood_model \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mGemmaLikelihoodModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_qCNsFvxuvftFWEAVDZLVkGXYYidIdPemKx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-2b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEFAULT_TRAIN_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEFAULT_BNB_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEFAULT_LORA_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/research/ConvoKit/convokit/redirection/gemmaLikelihoodModel.py:28\u001b[0m, in \u001b[0;36mGemmaLikelihoodModel.__init__\u001b[0;34m(self, hf_token, model_id, device, train_config, bnb_config, lora_config)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     18\u001b[0m     hf_token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     lora_config\u001b[38;5;241m=\u001b[39mDEFAULT_LORA_CONFIG,\n\u001b[1;32m     24\u001b[0m ):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     26\u001b[0m         model_id, token\u001b[38;5;241m=\u001b[39mhf_token, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_token\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_token \u001b[38;5;241m=\u001b[39m hf_token\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/modeling_utils.py:3657\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3654\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3657\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3661\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:74\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "gemma_likelihood_model = \\\n",
    "    GemmaLikelihoodModel(\n",
    "        hf_token = \"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\",\n",
    "        model_id = \"google/gemma-2b\", \n",
    "        train_config = DEFAULT_TRAIN_CONFIG,\n",
    "        bnb_config = DEFAULT_BNB_CONFIG,\n",
    "        lora_config = DEFAULT_LORA_CONFIG,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d38969",
   "metadata": {},
   "source": [
    "We use the following default configs and parameters for fine-tuning. However, you may override these by defining your own configs and passing them to the `GemmaLikelihoodModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef250fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DEFAULT_BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "DEFAULT_LORA_CONFIG = LoraConfig(\n",
    "    r=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "DEFAULT_TRAIN_CONFIG = {\n",
    "    \"output_dir\": \"checkpoints\",\n",
    "    \"logging_dir\": \"logging\",\n",
    "    \"logging_steps\": 25,\n",
    "    \"eval_steps\": 50, \n",
    "    \"num_train_epochs\": 2, \n",
    "    \"per_device_train_batch_size\": 1,  \n",
    "    \"per_device_eval_batch_size\": 1,   \n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 50,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"max_seq_length\": 4096 \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04001232",
   "metadata": {},
   "source": [
    "Now we can define our redirection model, providing the initialized `gemma_likelihood_model` as our `LikelihoodModel`. The `redirection_attribute_name` represents the name of the meta-data field to save our redirection scores to in the corpus.\n",
    "\n",
    "We also note that it is possible to define your own `previous_context_selector` and `future_context_selector` to determine which contexts you would use to compute the likelihoods. The functions take as input an utterance and returns the previous (actual and reference) or future contexts for that particular utterance. By default, we use the immediate contexts described in our paper. Note that the default implementation for these contexts assumes we are working with two distinct speaker roles. You may write your own context selectors to customize them for more than two speaker types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4a285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "redirection_model = \\\n",
    "    RedirectionModel(\n",
    "        likelihood_model = gemma_likelihood_model,\n",
    "        redirection_attribute_name = \"redirection\"\n",
    "#         previous_context_selector = <YOUR OWN PREVIOUS CONTEXT SELECTOR>, \n",
    "#         future_context_selector = <YOUR OWN FUTURE CONTEXT SELECTOR>,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69928ba4",
   "metadata": {},
   "source": [
    "Now we can call the fit method to fine-tune our model on a subset of the conversations in the corpus. We use a selector function to only fine-tune on the `train` subset of our data. Alternatively, if you already have saved an existing model, you can load it into memory using `load_from_disk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07b8022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "redirection_model.fit(corpus, selector=lambda convo: convo.meta[\"train\"] == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a5d20",
   "metadata": {},
   "source": [
    "After we have our fine-tuned model, we can then run inference on the test conversations in order to compute the redirection scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f975087",
   "metadata": {},
   "outputs": [],
   "source": [
    "redirection_model.transform(corpus, selector=lambda convo: convo.meta['test'] == True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01427de1",
   "metadata": {},
   "source": [
    "We can then call summarize to view examples of high and low redirecting utterances from each speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "redirection_model.summarize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087cfcc0",
   "metadata": {},
   "source": [
    "We can also perform a FightingWords analysis to see distinguishing bigrams indicating high vs. low redirection from both speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48ab266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import FightingWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def9310",
   "metadata": {},
   "source": [
    "We first label top 10% and bottom 10% of utterances from both speakers based on their redirection scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "justice_utts = []\n",
    "lawyer_utts = []\n",
    "\n",
    "for convo in test_convos: \n",
    "  for utt in convo.iter_utterances():\n",
    "    if \"redirection\" in utt.meta:\n",
    "      if utt.meta[\"role\"] == \"justice\":\n",
    "        justice_utts.append(utt)\n",
    "      else:\n",
    "        lawyer_utts.append(utt)\n",
    "\n",
    "justice_utts = sorted(justice_utts, key=lambda utt: utt.meta[\"redirection\"])\n",
    "lawyer_utts = sorted(lawyer_utts, key=lambda utt: utt.meta[\"redirection\"])\n",
    "\n",
    "justice_threshold = int(len(justice_utts) * 0.10)\n",
    "lawyer_threshold = int(len(lawyer_utts) * 0.10)\n",
    "\n",
    "for utt in justice_utts[:justice_threshold]:\n",
    "  utt.meta['type'] == \"justice_low\"\n",
    "for utt in justice_utts[-justice_threshold:]:\n",
    "  utt.meta['type'] == \"justice_high\"\n",
    "\n",
    "for utt in lawyer_utts[:lawyer_threshold]:\n",
    "  utt.meta['type'] == \"lawyer_low\"\n",
    "for utt in lawyer_utts[-lawyer_threshold:]:\n",
    "  utt.meta['type'] == \"lawyer_high\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33167261",
   "metadata": {},
   "source": [
    "Here we first show phrasings indicative of low redirection from justices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dbe4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_justice = FightingWords(ngram_range=(2,2))\n",
    "class1 = 'justice_high'\n",
    "class2 = 'justice_low'\n",
    "fw_justice.fit(corpus, class1_func=lambda utt: 'type' in utt.meta and utt.meta['type'] == class1, \n",
    "               class2_func=lambda utt: 'type' in utt.meta and utt.meta['type'] == class2)\n",
    "justice = fw_justice.summarize(corpus, plot=False, class1_name=class1, class2_name=class2)\n",
    "justice.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816627d",
   "metadata": {},
   "source": [
    "Here we show phrasings indicative of high redirection from justices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad93c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "justice.tail(20)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34287b06",
   "metadata": {},
   "source": [
    "We can perform the corresponding analysis for lawyers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c101e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_lawyer = FightingWords(ngram_range=(2,2))\n",
    "class1 = 'lawyer_high'\n",
    "class2 = 'lawyer_low'\n",
    "fw_lawyer.fit(corpus, class1_func=lambda utt: 'type' in utt.meta and utt.meta['type'] == class1, \n",
    "               class2_func=lambda utt: 'type' in utt.meta and utt.meta['type'] == class2)\n",
    "lawyer = fw_lawyer.summarize(corpus, plot=False, class1_name=class1, class2_name=class2)\n",
    "lawyer.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe98f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lawyer.tail(20)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52baf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
