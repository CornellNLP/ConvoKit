{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will understand how to choose the best storage mode to use for your ConvoKit corpus: the original RAM based implementation, or the new database based storage mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, Speaker, Utterance, download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historically...\n",
    "Historically, ConvoKit allows you to work with conversational data directly in program memory through the Corpus class. Moreover, long term storage is provided by dumping the contents of a Corpus onto disk using the JSON format. This paradigm works well for distributing and storing static datasets, and for doing computations on conversational data that follow the pattern of doing computations on some or all of the data over a short time period and optionally storing these results in the permenant representation of the dataset. For example, ConvoKit distributes datasets included with the library in JSON format, which you can load into program memory to explore and compute with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/eoin/.convokit/downloads/reddit-corpus-small\n",
      "True\n",
      "Loading corpus None from disk at /Users/eoin/.convokit/downloads/reddit-corpus-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297132/297132 [00:01<00:00, 188091.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModerator: Talk about your day. Anything goes, but subreddit rules still apply. Please be polite to each other! \n",
      "\n",
      "------------------------------\n",
      "belmont_lay: How to spoil a kpop fangirl's day. Tell her you want to send her a pic of g dragon.\n",
      "\n",
      "She'll be expecting something like [this](https://cdn2.i-scmp.com/sites/default/files/styles/landscape/public/images/methode/2018/01/18/482a92dc-fc0b-11e7-b2f7-03450b80c791_1280x720_135443.jpg?itok=tXZBSZ0u), but no. Send his [candid pics without makeup](https://koreaboo-cdn.storage.googleapis.com/2017/10/GDragon-Beard-01.jpg).\n",
      "\n",
      "Amazing what makeup can do for guys too, not just girls.\n",
      "------------------------------\n",
      "littlefiredragon: His \"candid\" pics look better leh\n",
      "------------------------------\n",
      "belmont_lay: wat.. he looks like a random extra in a JAV\n",
      "------------------------------\n",
      "rheinl: “Hi Kpop girl can I send you a pic of g-dragon?”\n",
      "\n",
      "“Uhhh ok” (what the hell? this guy is so weird)\n",
      "\n",
      "“Here you go” *sends pic of g-dragon with no makeup*\n",
      "\n",
      "“Uhh thanks I guess” (Jesus I hope he never talks to me again)\n",
      "------------------------------\n",
      "belmont_lay: I'm sorry if that's how your interactions with friends go 😥\n",
      "------------------------------\n",
      "rheinl: “Hi Kpop girl can I send you a pic of g-dragon”\n",
      "\n",
      "“Woohoo I can’t wait! Please send it right away!”\n",
      "\n",
      "“Here you go” *sends a pic of g-dragon with no make up*\n",
      "\n",
      "“Ugh! No you spoilt my day! Hehe!”\n",
      "------------------------------\n",
      "ThenPoem: Both ugly \n",
      "------------------------------\n",
      "sea_lecture: Bruh, I don't even like g-dragon but his comment is accurate af in describing how you'd come off as some socially awkward neckbeard if you talked like that to a girl.\n",
      "------------------------------\n",
      "brianne0007: Lmao 🤣🤣\n",
      "------------------------------\n",
      "tauhuayislove: Ohhhh. Emmm. Geeeee. \n",
      "------------------------------\n",
      "[deleted]: [deleted]\n",
      "------------------------------\n",
      "belmont_lay: \"Bruh\", you're stupid if you think I talked to any of my friends in the manner he posted.\n",
      "\n",
      "I wrote a condensed version of what I did, not verbatim. \n",
      "\n",
      "End result was just people being surprised by how k pop idols look without make up, nothing awkward.\n",
      "\n",
      "I think people on this subreddit project too much when it comes to social awkwardness based on their own inadequacies.\n",
      "------------------------------\n",
      "belmont_lay: I'm sorry if that's how your interactions with friends go 😥\n",
      "\n",
      "But that's not how any of my whatsapps with my friends happened.\n",
      "------------------------------\n",
      "sea_lecture: &gt; if you think I talked to any of my friends in the manner he posted.\n",
      "\n",
      "Good thing I don't actually think that, but the fact that you're even fantasising and joking about it says a lot. \n",
      "\n",
      "Nah, you come off as a loser who doesn't know how to speak to women without scaring them off. That's a sad fact man. Now bye loser\n",
      "------------------------------\n",
      "dump to  /Users/eoin/.convokit/saved-corpora/reddit-corpus-small\n",
      "True\n",
      "Loading corpus reddit-corpus-small from disk at /Users/eoin/.convokit/saved-corpora/reddit-corpus-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297132/297132 [00:01<00:00, 177488.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# First we download the JSON files for the corpus from the ConvoKit servers, \n",
    "# if we don't already have a local copy. Then, we load the corpus into memory \n",
    "# by constructing a Corpus object and giving it the path to the corpus (as returned by download). \n",
    "reddit_small = Corpus(filename=download('reddit-corpus-small'), storage_type='mem')\n",
    "\n",
    "# Now we can easily work with the data through the corpus object to access the data within it.\n",
    "seen_utts = []\n",
    "for conversation in reddit_small.iter_conversations():\n",
    "    for utterance in conversation.iter_utterances():\n",
    "        print(f'{utterance.speaker.id}: {utterance.text}')\n",
    "        print('------------------------------')\n",
    "        utterance.meta['seen'] = True\n",
    "        seen_utts.append(utterance.id)\n",
    "    break\n",
    "    \n",
    "# Finally, to write the changes to the corpus (in this case, which utterances we saw in our test)\n",
    "# back to the JSON files so these changes are reflected in our local persistant representation of the corpus.\n",
    "reddit_small.dump(corpus_id='reddit-corpus-small')\n",
    "\n",
    "# We can confirm this by constructing a new corpus from the updated JSON files.\n",
    "reddit_small_2 = Corpus(corpus_id='reddit-corpus-small', storage_type='mem')\n",
    "for utterance in reddit_small_2.iter_utterances():\n",
    "    if utterance.id in seen_utts:\n",
    "        assert utterance.meta['seen'] == True\n",
    "    else:\n",
    "        assert 'seen' not in utterance.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in the above cell, we construct a corpus in two different ways. First, we use the  `filename` argument on line 4, and later use the `corpus_id` argument on lines 18 and 21. This is because we can consider a local download of a ConvoKit provided corpus as a locally cached version of the original corpus as it is distributed (i.e., we should not write to it directly). \n",
    "\n",
    "A downloaded corpus called `<corpus_id>` will by default be locally cached at `~/.convokit/downloads/<corpus_id>`; we use the `filename` paramater with download because download returns the full path to the corpus on disk. \n",
    "\n",
    "On the other hand, in general your local corpora live on disk in your `data_dir`: the directory specified in the configuration file at `~/.convokit/config.yml`; `data_dir` is `~/.convokit/saved-corpora` by default. Using the `corpus_id` paramater for initilization and to dump will read from/write to `<data_dir>/<corpus_id>`. You can also specify `data_dir` as an argument in `dump`, `download`, or a `Corpus` initilization to override the global default.\n",
    "\n",
    "Therefore, we use these two different ways to intilize a corpus to maintain the original, unaltered version of the Corpus at `~/.convokit/downloads/reddit-corpus-small`, while storing the version we are working with and modifying at `~/.convokit/saved-corpora/reddit-corpus-small`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In ConvoKit version (x.x.x)...\n",
    "In ConvoKit version (x.x.x), we introduce an new option for storing conversational data: Database storage. Consider a use case where you want to collect conversational data over a long time period and ensure you maintain a persistant representation of the dataset if your data collection program unexpectedly crashes. In the memory storage paradigm, this would require regularly dumping your corpus to JSON files, requiring repeated expensive write operations. On the other hand, with database storage all your data is automatically saved for long term storage in the database as it is added to the corpus. Lets view an example of constructing a corpus of reddit comments as they are posted, using ConvoKit alongside the praw wrapper library around the reddit API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 6.4.0 of praw is outdated. Version 7.5.0 was released Sunday November 14, 2021.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from time import sleep\n",
    "\n",
    "# You can follow these instructions to get a client_id and client_secret to run this code yourself\n",
    "# https://www.geeksforgeeks.org/how-to-get-client_id-and-client_secret-for-python-reddit-api-registration/\n",
    "# (or, just view the output of running this code from before I removed my own credentials)\n",
    "reddit = praw.Reddit(client_id='<redacted>',\n",
    "                     client_secret='<redacted>',\n",
    "                     user_agent='jack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus reddit_live_v0 not found in the DB; building new corpus\n"
     ]
    }
   ],
   "source": [
    "reddit_live = Corpus(corpus_id='reddit_live', storage_type='db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No filename or corpus name specified for DB storage; using name 667873\n",
      "Corpus 667873_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No filename or corpus name specified for DB storage; using name 193424\n",
      "Corpus 193424_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _merge_utterances\n",
      "No filename or corpus name specified for DB storage; using name 42080\n",
      "Corpus 42080_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No filename or corpus name specified for DB storage; using name 213506\n",
      "Corpus 213506_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _merge_utterances\n",
      "No filename or corpus name specified for DB storage; using name 209106\n",
      "Corpus 209106_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No filename or corpus name specified for DB storage; using name 671252\n",
      "Corpus 671252_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _merge_utterances\n",
      "No filename or corpus name specified for DB storage; using name 984186\n",
      "Corpus 984186_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 14.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No filename or corpus name specified for DB storage; using name 738356\n",
      "Corpus 738356_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _merge_utterances\n",
      "No filename or corpus name specified for DB storage; using name 636432\n",
      "Corpus 636432_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  9.76it/s]\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "ids = []\n",
    "last_corpus_id=None\n",
    "for comment in reddit.subreddit('funny').stream.comments(skip_existing=True):\n",
    "    utt = Utterance(id=comment.id,\n",
    "                    text=comment.body,\n",
    "                    reply_to=comment.parent_id.split('_')[1],\n",
    "                    speaker=Speaker(\n",
    "                        id=comment.author.name if comment.author is not None else \"n/a\",),\n",
    "                    conversation_id=comment.submission.id,\n",
    "                    timestamp=comment.created_utc)\n",
    "    ids.append(utt.id) # Will use this external list of ids for a check in the next cell.\n",
    "    reddit_live = reddit_live.add_utterances([utt])\n",
    "    last_corpus_id = reddit_live.id\n",
    "    sleep(1)\n",
    "    c += 1\n",
    "    if c >= 5:\n",
    "        # Simulating a server crash after 5 iterations\n",
    "        del reddit_live\n",
    "        break \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with no dump necessary, the data is already stored persistently in the database despite the crash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 636432_v0 not found in the DB; building new corpus\n",
      "still has utts ['hqpzhhs', 'hqpzhlj', 'hqpzj9l', 'hqpzjob', 'hqpzjve']\n",
      "Appropriate_Jacket_5: Nice\n",
      "------------------------------\n",
      "hypercube33: Take them to flavor town\n",
      "------------------------------\n",
      "SkyShazad: Who thinks of this SHIT\n",
      "------------------------------\n",
      "NoLifeGuy_4k: So where can i get a best freind to turn into a diamond but without the fleshy bits\n",
      "------------------------------\n",
      "CatGotNoTail: Damn, you’re a hoss. I’ve only done it twice. What’s your secret?\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "reddit_live_2 = Corpus(corpus_id=last_corpus_id, storage_type='db', in_place=True)\n",
    "print('still has utts',[utt.id for utt in reddit_live_2.iter_utterances()])\n",
    "for id in ids:\n",
    "    assert reddit_live_2.has_utterance(id)\n",
    "    utterance = reddit_live_2.get_utterance(id)\n",
    "    print(f'{utterance.speaker.id}: {utterance.text}')\n",
    "    print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
