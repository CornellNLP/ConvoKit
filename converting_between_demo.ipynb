{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d1b696",
   "metadata": {},
   "source": [
    "# Converting between memory and database storage\n",
    "\n",
    "This notebook is a demonstration of how to convert between the two storage formats ConvoKit offers: in program memory, and in a MongoDB database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63a03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, Speaker, Utterance\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8c7b2",
   "metadata": {},
   "source": [
    "First, let's create a Corpus that uses DB Storage and populate it with some data. The number of utterances in this demo is on the order of 100s of utterances so that this demo notebook is runable in a resonable amout of time, but the tradeoffs we will explore only become more pertinant with larger quantities of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be71c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus big_test_corpus_v0 not found in the DB; building new corpus\n",
      "No filename or corpus name specified for DB storage; using name 710306\n",
      "Corpus 710306_v0 not found in the DB; building new corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:01<00:00, 195.32it/s]\n"
     ]
    }
   ],
   "source": [
    "db_corpus = Corpus(corpus_id='big_test_corpus', storage_type='db')\n",
    "\n",
    "# Initilizing Speakers\n",
    "jim = Speaker(id=\"Jim\")\n",
    "bob = Speaker(id=\"Bob\")\n",
    "kate = Speaker(id=\"Kate\")\n",
    "\n",
    "# Initilizing Utterances\n",
    "utts = []\n",
    "\n",
    "# First, everything Jim said:\n",
    "for i in range(100):\n",
    "    utts.append(Utterance(id=f\"jim.{i}\", text=f\"I am the {i}'th comment!\", speaker=jim))\n",
    "                    \n",
    "# Then, all of Bob's replies to Jim's comments:\n",
    "for i in range(100):\n",
    "    utts.append(Utterance(id=f\"bob.{i}\", text=f\"Wow Jim, your {i}'th comment is so interesting!\", reply_to=f\"jim.{i}\", speaker=bob))\n",
    "                            \n",
    "# Then, all of Kate's replies to Bob to the corpus; however, Kate only responds to some of Bob's comments:\n",
    "for i in range(100):\n",
    "    if i % 2 == 0:\n",
    "        utts.append(Utterance(id=f\"kate.{i}\", text=f\"I totally agree Bob! Jim's {i}'th comment is insightful\", reply_to=f\"bob.{i}\", speaker=kate))\n",
    "        \n",
    "# Finally, we actually add these utterances to the DB corpus:\n",
    "db_corpus = db_corpus.add_utterances(utts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249626df",
   "metadata": {},
   "source": [
    "We can work with this data directly in the db_corpus. If we primarially want to store the data long term while maintaining the ability to occasionally modify and read the data very quickly, then database storage is a good option. For example, if you want to use the Corpus as the backbone of a webserver that stores conversational data, database storage is the best option. Here is an example of this type of use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef01294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance(id: 'kate.34', conversation_id: None, reply-to: bob.34, speaker: Speaker(id: Kate), timestamp: None, text: \"I totally agree Bob! Jim's 34'th comment is insightful\", vectors: [], meta: {})\n",
      "Utterance(id: 'bob.68', conversation_id: None, reply-to: jim.68, speaker: Speaker(id: Bob), timestamp: None, text: \"Wow Jim, your 68'th comment is so interesting!\", vectors: [], meta: {})\n",
      "Utterance(id: 'bob.68', conversation_id: None, reply-to: jim.68, speaker: Speaker(id: Bob), timestamp: None, text: 'Actually, on second thought Jim, your 68th comment is slightly less interesting than the other ones', vectors: [], meta: {})\n"
     ]
    }
   ],
   "source": [
    "# Example of responding to an incoming request for comment kate.34\n",
    "print(db_corpus.get_utterance('kate.34'))\n",
    "\n",
    "\n",
    "# Example of Bob viewing, then editing, his 68th comment\n",
    "print(db_corpus.get_utterance('bob.68'))\n",
    "db_corpus.get_utterance('bob.68').text = \"Actually, on second thought Jim, your 68th comment is slightly less interesting than the other ones\"\n",
    "print(db_corpus.get_utterance('bob.68'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4dcb2b",
   "metadata": {},
   "source": [
    "On the other hand, imagine that you want to take all the data on your webserver and run the latest Natural Language Processing algorithms on it in order to learn something about what people are talking about on your website. This is a computationaly intensive task, that will likely require reading the data many times in a short time period to do the computations. Working with the data in this manner—for computationally expensive operations that only need to access the data over a short time period—would be inneficient to do with the data stored in the database as it would require repeated reads from and writes back to the database; it would be more efficient to work with the data in memory. Thus, we can pull all the data into memory at once by creating a copy of the corpus using the memory storage format, do our expensive computations, then write the results back into the database all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9407e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import TextParser\n",
    "from convokit import PolitenessStrategies\n",
    "\n",
    "parser = TextParser(verbosity=1000)\n",
    "ps = PolitenessStrategies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0729b277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 utterances processed\n"
     ]
    }
   ],
   "source": [
    "mem_start = time()\n",
    "\n",
    "mem_corpus = Corpus(storage_type='mem', from_corpus=db_corpus)\n",
    "\n",
    "mem_corpus = parser.transform(mem_corpus)\n",
    "mem_corpus = ps.transform(mem_corpus, markers=True)\n",
    "\n",
    "mem_end = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908da31f",
   "metadata": {},
   "source": [
    "Lets try running the same computations directly on db_corpus to compare how long each strategy takes — including the time to convert from database to in-memory storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ae86c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 utterances processed\n"
     ]
    }
   ],
   "source": [
    "db_start = time()\n",
    "\n",
    "db_corpus = parser.transform(db_corpus)\n",
    "db_corpus = ps.transform(db_corpus, markers=True)\n",
    "\n",
    "db_end = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee05ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting from db -> mem storage + doing computations in memory took 13.639177083969116 \n",
      "Doing computations with the DB corpus directly took 7.874305009841919 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Converting from db -> mem storage + doing computations in memory took {mem_end - mem_start} \")\n",
    "print(f\"Doing computations with the DB corpus directly took {db_end - db_start} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
