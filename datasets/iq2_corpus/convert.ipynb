{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert IQ2 debates to Convokit\n",
    "*by Marianne Aubin Le Quere and Lucas Van Bramer*\n",
    "\n",
    "This python script converts the raw IQ2 dataset into a Convokit format. The original dataset can be found at http://tisjune.github.io/research/iq2. The input file is:\n",
    "  * iq2_data_release.json\n",
    "  \n",
    "Much of the instructions below were taken from this Github tutorial: https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit/blob/master/examples/converting_movie_corpus.ipynb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "The first step is to ensure your environment is correctly set up. You need to be able to access the convokit package to use this notebook. For more information about how to install convokit please visit the Github page https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/marianneaubin/Documents/Classes/CS6742/IQ2', '/Users/marianneaubin/anaconda3/lib/python37.zip', '/Users/marianneaubin/anaconda3/lib/python3.7', '/Users/marianneaubin/anaconda3/lib/python3.7/lib-dynload', '', '/Users/marianneaubin/anaconda3/lib/python3.7/site-packages', '/Users/marianneaubin/anaconda3/lib/python3.7/site-packages/aeosa', '/Users/marianneaubin/anaconda3/lib/python3.7/site-packages/IPython/extensions', '/Users/marianneaubin/.ipython']\n"
     ]
    }
   ],
   "source": [
    "# First, we start by ensuring that the current path is correct. \n",
    "# Replace the file path with your local version of convokit to avoid import issues.\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/marianneaubin/Documents/Classes/CS6742/Cornell-Conversational-Analysis-Toolkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (2.1.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (1.16.4)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (0.1.0)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (7.0.8)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/marianneaubin/anaconda3/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.32.1)\n",
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    2.1.8                         \n",
      "Location         /Users/marianneaubin/anaconda3/lib/python3.7/site-packages/spacy\n",
      "Platform         Darwin-18.7.0-x86_64-i386-64bit\n",
      "Python version   3.7.3                         \n",
      "Models           en                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you are having issues with spacy, you may need to download it here.\n",
    "# this is an optional step.\n",
    "\n",
    "!{sys.executable} -m pip install spacy\n",
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'convokit' from '/Users/marianneaubin/Documents/Classes/CS6742/Cornell-Conversational-Analysis-Toolkit/convokit/__init__.py'>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import convokit, then validate it is correctly imported\n",
    "# by running the convokit command and checking it exists\n",
    "import convokit\n",
    "convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, User, Utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing your data\n",
    "\n",
    "Now that you have correctly set up your environment and gotten convokit to work, it's time to important and represent your data! We are working with a json file, so it should be easy to load into a workable dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your data directory to the IQ2 file location\n",
    "data_dir = \"../IQ2/iq2_data_release/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(data_dir + \"iq2_data_release.json\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    debates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, check the data is represented correctly\n",
    "# note this will print the whole first debate, so may be long\n",
    "\n",
    "print(str(debates['PerformanceEnhancingDrugs-011508']))\n",
    "print(str(debates['PerformanceEnhancingDrugs-011508']['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the debates do not have an id from the outset, we will create one for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add debate id to dict\n",
    "id = 1;\n",
    "for debate in debates:\n",
    "    debates[debate]['id'] = id\n",
    "    id = id + 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Users\n",
    "\n",
    "Our dataset already has all speakers listed along with their metadata. They are broken up into 'for,' 'against,' or 'moderator.' For each of these personas, we will create User representation in our corpus that contains the following meta data:\n",
    "  * speaker name\n",
    "  * bio, if available\n",
    "  * short bio, if available\n",
    "  * debate id\n",
    "  * debate name\n",
    "  * position (define as 'for,' 'against,' 'moderator,' or 'misc')\n",
    "  \n",
    "The key to access a user is their name.\n",
    "\n",
    "Note that we do not have a perfectly clean dataset. There are some speakers who speak but are not represented in the given speaker list of the dataset because they are not officially a part of the debate. One example of this is the 'host' of the debate or a 'panelist.' To account for this, we create a generic user called 'Misc,' to map all of these utterances on to later. That way the utterances are still represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following is to synthesise all the user metadata\n",
    "user_meta = {}\n",
    "\n",
    "for debate in debates:\n",
    "    speakers = debates[debate]['speakers']\n",
    "    \n",
    "    #for speakers\n",
    "    for speaker in speakers['for']:\n",
    "        user_info = {};\n",
    "        user_info['bio'] = speaker['bio'];\n",
    "        user_info['bio_short'] = speaker['bio_short'];\n",
    "        user_info['debate_id'] = debates[debate]['id'];\n",
    "        user_info['debate_name'] = debate\n",
    "        user_info['position'] = 'for';\n",
    "        user_info['name'] = speaker['name'];\n",
    "        user_meta[speaker['name']] = user_info;\n",
    "        \n",
    "    #against speakers\n",
    "    for speaker in speakers['against']:\n",
    "        user_info = {};\n",
    "        user_info['bio'] = speaker['bio'];\n",
    "        user_info['bio_short'] = speaker['bio_short'];\n",
    "        user_info['debate_id'] = debates[debate]['id'];\n",
    "        user_info['debate_name'] = debate\n",
    "        user_info['position'] = 'against';\n",
    "        user_info['name'] = speaker['name'];\n",
    "        user_meta[speaker['name']] = user_info;\n",
    "    \n",
    "    #moderator\n",
    "    user_info = {};\n",
    "    user_info['bio'] = speakers['moderator']['bio'];\n",
    "    user_info['bio_short'] = speakers['moderator']['bio_short'];\n",
    "    user_info['debate_id'] = debates[debate]['id'];\n",
    "    user_info['debate_name'] = debate\n",
    "    user_info['position'] = 'moderator';\n",
    "    user_info['name'] = (speakers['moderator']['name']);\n",
    "    user_meta[(speakers['moderator']['name'])] = user_info;\n",
    "    \n",
    "    #misc speaker\n",
    "    user_info = {};\n",
    "    user_info['bio'] = None;\n",
    "    user_info['bio_short'] = None;\n",
    "    user_info['debate_id'] = debates[debate]['id'];\n",
    "    user_info['debate_name'] = debate\n",
    "    user_info['position'] = 'misc';\n",
    "    user_info['name'] = 'Misc';\n",
    "    user_meta['Misc'] = user_info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now create the corpus of users\n",
    "corpus_users = {k: User(name = k, meta = v) for k,v in user_meta.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users in the data = 470\n",
      "User([('name', 'Bob Costas')])\n",
      "{'bio': None, 'bio_short': None, 'debate_id': 1, 'debate_name': 'PerformanceEnhancingDrugs-011508', 'position': 'moderator', 'name': 'Bob Costas'}\n"
     ]
    }
   ],
   "source": [
    "# sanity check that the number of users is as expected\n",
    "print(\"number of users in the data = {0}\".format(len(corpus_users)))\n",
    "# sanity check on one instance\n",
    "print(corpus_users['Bob Costas'])\n",
    "print(corpus_users['Bob Costas'].meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating utterances\n",
    "\n",
    "Each time a speaker speaks uninterrupted, this counts as an utterance in our dataset. Each of the dataset utterances consists of:\n",
    "  * utterance id: this is a unique id we have created for each utterance in the format \"debateid_utteranceid.\" This renderes each utterance unique throughout the dataset\n",
    "  * user: the speaking user. This will be 'Misc' if the speaker is not officially listed as 'for,' 'against,' or a 'moderator'\n",
    "  * root: this is the first utterance of the debate\n",
    "  * reply_to: the id of the preceding utterance\n",
    "  * timestamp: not present in this case\n",
    "  * text: text of the utterance\n",
    "  * metadata:\n",
    "      * the debate id\n",
    "      * the current segment\n",
    "      * nontextual information (e.g. applause, laughter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now moving on to creating Utterances\n",
    "\n",
    "utterance_corpus = {}\n",
    "\n",
    "for debate in debates:\n",
    "    utt_id = 1;\n",
    "    transcript = debates[debate]['transcript']\n",
    "    debate_id = debates[debate]['id']\n",
    "    for utt in transcript:\n",
    "        speaker = utt['speaker']\n",
    "        text = utt['paragraphs']\n",
    "        meta = {'debate id': debates[debate]['id'], 'segment': utt['segment'], 'nontext': utt['nontext']}\n",
    "        \n",
    "        utt_unique_id = str(debate_id)+'_'+str(utt_id)\n",
    "        if utt_id != 1:\n",
    "            reply_to = str(debate_id)+'_'+str(utt_id-1)\n",
    "        else:\n",
    "            reply_to = None\n",
    "            \n",
    "        root = str(debate_id) + '_1'\n",
    "            \n",
    "        if utt['speakertype'] != ('mod' or 'for' or 'against'):\n",
    "            speaker=\"Misc\"\n",
    "            \n",
    "        utt_id = utt_id +1;\n",
    "        utterance_corpus[utt_unique_id] = Utterance(utt_unique_id, corpus_users[speaker], root, reply_to, None, text, meta)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance({'id': '1_1', 'user': User([('name', 'Bob Costas')]), 'root': '1_1', 'reply_to': None, 'timestamp': None, 'text': ['… And now I’d like to introduce Robert Rosenkranz, who is the chairman of the Rosenkranz Foundation, and the sponsor of Intelligence Squared, who will frame tonight’s debate. Bob? This is Bob.'], 'meta': {'debate id': 1, 'segment': 0, 'nontext': {'applause': [[0, 29]]}}})\n",
      "Utterance({'id': '1_53', 'user': User([('name', 'Bob Costas')]), 'root': '1_1', 'reply_to': '1_52', 'timestamp': None, 'text': ['All right, suppose we were to adopt Julian’s suggestion, that there were regulated, permissible regulated use of performance- The Rosenkranz Foundation - Intelligence Squared US Debate “Performance Enhancing Drugs in Competitive Sports” enhancing drugs, and in each case it was appropriate to the spirit of the particular sport, that’s fine in the ideal. But it’s naïve to believe that each competitor, many of them obsessed with victory and believing in the full bloom of youth that they’re invulnerable, would stay within those limits, once there were unfettered access, they could just take as much as they wanted to gain whatever short-term competitive advantage they wanted, couldn’t they?'], 'meta': {'debate id': 1, 'segment': 1, 'nontext': {}}})\n",
      "Utterance({'id': '4_7', 'user': User([('name', 'Robert Siegel')]), 'root': '4_1', 'reply_to': '4_6', 'timestamp': None, 'text': ['Thank you, uh, Joseph Phillips, speaking in favor of the motion. Uh, now, to make an opening statement against the motion, “It’s time to end affirmative action,” Tim Wise.'], 'meta': {'debate id': 4, 'segment': 0, 'nontext': {}}})\n",
      "Gray Davis\n"
     ]
    }
   ],
   "source": [
    "#sanity check a few utterances\n",
    "print(utterance_corpus['1_1'])\n",
    "print(utterance_corpus['1_53'])\n",
    "print(utterance_corpus['4_7'])\n",
    "len(utterance_corpus)\n",
    "\n",
    "Utterance({'id': 'L666499', 'user': User([('name', 'u9028')]), 'root': 'L666497', 'reply_to': 'L666498', 'timestamp': None, 'text': 'How quickly can you move your artillery forward?', 'meta': {'movie_id': 'm616', 'test': []}})\n",
    "\n",
    "print(corpus_users['Gray Davis'].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating corpus from list of utterances\n",
    "utterance_list = [utterance for k,utterance in utterance_corpus.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating corpus\n",
    "\n",
    "Create the corpus. Convokit will automatically create conversations from the data in the utterances list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create corpus\n",
    "iq2_corpus = Corpus(utterances=utterance_list, version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations in the dataset = 108\n"
     ]
    }
   ],
   "source": [
    "# in our case, the number of conversations will be equivalent to the number of debates\n",
    "print(\"number of conversations in the dataset = {}\".format(len(iq2_corpus.get_conversation_ids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check a conversation if desired\n",
    "# note this will be quite long\n",
    "convo_ids = iq2_corpus.get_conversation_ids()\n",
    "for i, convo_idx in enumerate(convo_ids[0:5]):\n",
    "    print(\"sample conversation {}:\".format(i))\n",
    "    print(iq2_corpus.get_conversation(convo_idx).get_utterance_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marianneaubin/anaconda3/bin/python: Error while finding module specification for 'spacy==2.0.12' (ModuleNotFoundError: No module named 'spacy==2')\r\n"
     ]
    }
   ],
   "source": [
    "# if needed, download en package for spacy\n",
    "#!python -m spacy download en\n",
    "#import nltk;\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-284-afeddbc37264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconvokit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mannotator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0miq2_corpus_parsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miq2_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Classes/CS6742/Cornell-Conversational-Analysis-Toolkit/convokit/transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \"\"\"\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Classes/CS6742/Cornell-Conversational-Analysis-Toolkit/convokit/parser/parser.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mspacy_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mutt_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutt_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;31m# add the spacy parses to the utterance metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mutt_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Classes/CS6742/Cornell-Conversational-Analysis-Toolkit/convokit/parser/parser.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# if the user specifies multithreading, we will enable parallelized parsing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# using spacy.pipe. Otherwise we will operate sequentially.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_threads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mspacy_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutt_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mutt_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutt_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             )\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": [
    "# if desired, parse info\n",
    "# not currently working\n",
    "#from convokit import Parser\n",
    "#annotator = Parser()\n",
    "#iq2_corpus_parsed = annotator.fit_transform(iq2_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Corpus level meta data\n",
    "\n",
    "Each debate also has some meta data associated to it that we want to capture. For each debate, we want to include in the metadata:\n",
    "  * debate id\n",
    "  * debate results\n",
    "  * debate title\n",
    "  * debate date\n",
    "  * debate url\n",
    "  * debate summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_meta = {}\n",
    "for debate in debates:\n",
    "    d = debates[debate]\n",
    "    debate_id, results, title, date, url, summary = \\\n",
    "        d['id'], d['results'], d['title'], d['date'], d['url'], d['summary']\n",
    "    debate_meta[debate_id] = {'title': title, \"url\": url, 'summary': summary, 'date': date, 'results': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Freedom of Expression Must Include the License to Offend', 'url': 'http://intelligencesquaredus.org/debates/past-debates/item/545-freedom-of-expression-must-include-the-license-to-offend', 'summary': 'Debate description coming soon.', 'date': 'Tuesday, October 16, 2006', 'results': {'breakdown': None, 'post': {'undecided': 1.0, 'for': 83.0, 'against': 16.0}, 'pre': {'undecided': 11.0, 'for': 78.0, 'against': 11.0}}}\n"
     ]
    }
   ],
   "source": [
    "# sanity check for random debate\n",
    "len(debate_meta)\n",
    "print(debate_meta[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "iq2_corpus.meta['debate_metadata'] = debate_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset\n",
    "We will now perform a dump of the dataset wherever is preferred, and validate that it correctly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the dataset\n",
    "iq2_corpus.meta['name'] = \"IQ2 Debates Corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the path where you want to save the corpus\n",
    "iq2_corpus.dump(\"iq2-corpus\", base_path='datasets/iq2-corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import meta_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterances-index': {'debate id': \"<class 'int'>\",\n",
       "  'segment': \"<class 'int'>\",\n",
       "  'nontext': \"<class 'dict'>\"},\n",
       " 'users-index': {'bio': \"<class 'NoneType'>\",\n",
       "  'bio_short': \"<class 'NoneType'>\",\n",
       "  'debate_id': \"<class 'int'>\",\n",
       "  'debate_name': \"<class 'str'>\",\n",
       "  'position': \"<class 'str'>\",\n",
       "  'name': \"<class 'str'>\"},\n",
       " 'conversations-index': {},\n",
       " 'overall-index': {'debate_metadata': \"<class 'dict'>\",\n",
       "  'name': \"<class 'str'>\"},\n",
       " 'version': 1}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_index(filename = \"datasets/iq2-corpus/iq2-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
