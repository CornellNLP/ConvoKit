{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gender analysis (d1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_YNRDvb3etb",
        "colab_type": "text"
      },
      "source": [
        "# Converting the Friends dataset into ConvoKit format\n",
        "\n",
        "This notebook describes how we converted the Friends dataset (https://github.com/emorynlp/character-mining) into a Corpus with ConvoKit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp3WIlBN4D7l",
        "colab_type": "code",
        "outputId": "79672007-fd62-465a-ffb1-dd968cb889b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install convokit\n",
        "# !python3 -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting convokit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/4c/66b8c4dcdefc6c688f1fb4e25f765bc7359671fca759bc32e3af63be7e15/convokit-2.1.11.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from convokit) (3.0.3)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.6/dist-packages (from convokit) (0.24.2)\n",
            "Collecting msgpack-numpy==0.4.3.2 (from convokit)\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/45/464be6da85b5ca893cfcbd5de3b31a6710f636ccb8521b17bd4110a08d94/msgpack_numpy-0.4.3.2-py2.py3-none-any.whl\n",
            "Collecting spacy==2.0.12 (from convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/de/ac14cd453c98656d6738a5669f96a4ac7f668493d5e6b78227ac933c5fd4/spacy-2.0.12.tar.gz (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from convokit) (1.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from convokit) (0.21.3)\n",
            "Collecting nltk>=3.4 (from convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 38.7MB/s \n",
            "\u001b[?25hCollecting dill==0.2.9 (from convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (2.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (1.16.5)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.4->convokit) (2018.9)\n",
            "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy==0.4.3.2->convokit) (0.5.6)\n",
            "Collecting murmurhash<0.29,>=0.28 (from spacy==2.0.12->convokit)\n",
            "  Downloading https://files.pythonhosted.org/packages/82/55/7f050e9f73c9a58df219c63e77304b0ff01676847061dc99abb484cff3a8/murmurhash-0.28.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting cymem<1.32,>=1.30 (from spacy==2.0.12->convokit)\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/0f/d29aa68c55db37844c77e7e96143bd96651fd0f4453c9f6ee043ac846b77/cymem-1.31.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting preshed<2.0.0,>=1.0.0 (from spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/88/57a818051f3d71e800bfb7ba4df56d3ea5793482ef11f1d2109b726f3bac/preshed-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 25.8MB/s \n",
            "\u001b[?25hCollecting thinc<6.11.0,>=6.10.3 (from spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/b1/47a88072d0a38b3594c0a638a62f9ef7c742b8b8a87f7b105f7ed720b14b/thinc-6.10.3.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.12->convokit) (0.9.6)\n",
            "Collecting ujson>=1.35 (from spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 49.0MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.12->convokit) (2.21.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->convokit) (0.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.4->convokit) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->convokit) (41.2.0)\n",
            "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz (443kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 29.1MB/s \n",
            "\u001b[?25hCollecting wrapt<1.11.0,>=1.10.0 (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (3.0.4)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit) (0.10.0)\n",
            "Building wheels for collected packages: convokit, spacy, nltk, dill, thinc, ujson, regex, cytoolz, wrapt\n",
            "  Building wheel for convokit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-2.1.11-cp36-none-any.whl size=89359 sha256=72cd48dfda51e54132ba2d15f35a825279a07beb1c631bac40eca669c6c9ce1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/3b/39/a075dde800be3b330c273e28458ec3145d67254a7daa9b249a\n",
            "  Building wheel for spacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy: filename=spacy-2.0.12-cp36-cp36m-linux_x86_64.whl size=29062703 sha256=15ca587abc6c5566a2f9268fea0f5699c3da1ee4b07f74cff9206b9a1ce6bf12\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/0b/bb/7c2e28db574dbb2358176934eddd32a1c5f838ba0bc23eaaab\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449909 sha256=5437c09cfe9a978e61c799e1505445b94aa5fbe7cbdd009406cbed0418eb128c\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.2.9-cp36-none-any.whl size=77403 sha256=714d5ede8167ec075f9ad98c504932f181b78f42029932b00a5eb4fe00ce6ee5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
            "  Building wheel for thinc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thinc: filename=thinc-6.10.3-cp36-cp36m-linux_x86_64.whl size=4120085 sha256=704f7d17e170ff18de636d0a3f5b68afc84dadfdcc4fe9fdae501fd4883489bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/bc/e1/9b321b6b203288cf636a56e668ed5700076af4ed66062278ca\n",
            "  Building wheel for ujson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ujson: filename=ujson-1.35-cp36-cp36m-linux_x86_64.whl size=68028 sha256=57c672674c4890efd83374e127d5375ab3ac4ee50c4b0b5b0cf3b548c41bf33a\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533181 sha256=50268e609afe26d4265e1595e53ac38053bc02ef434dd9d9ed9e066edc93c3d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cytoolz: filename=cytoolz-0.9.0.1-cp36-cp36m-linux_x86_64.whl size=1247742 sha256=8062c319d5a3adb96fe875f66fbb8077c9c9a53c8c371e1d0e3bb895d0fcc2e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/f3/11/9817b001e59ab04889e8cffcbd9087e2e2155b9ebecfc8dd38\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.10.11-cp36-cp36m-linux_x86_64.whl size=65117 sha256=dce6fecf62b3eaa2e57c17eec476b6189fd55df8d5374ccbc601b180fd88f336\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n",
            "Successfully built convokit spacy nltk dill thinc ujson regex cytoolz wrapt\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement wrapt>=1.11.1, but you'll have wrapt 1.10.11 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.8 has requirement dill>=0.3.0, but you'll have dill 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.57 has requirement spacy>=2.0.18, but you'll have spacy 2.0.12 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.1.0 has requirement spacy>=2.1.0, but you'll have spacy 2.0.12 which is incompatible.\u001b[0m\n",
            "Installing collected packages: msgpack-numpy, murmurhash, cymem, preshed, cytoolz, wrapt, dill, thinc, ujson, regex, spacy, nltk, convokit\n",
            "  Found existing installation: murmurhash 1.0.2\n",
            "    Uninstalling murmurhash-1.0.2:\n",
            "      Successfully uninstalled murmurhash-1.0.2\n",
            "  Found existing installation: cymem 2.0.2\n",
            "    Uninstalling cymem-2.0.2:\n",
            "      Successfully uninstalled cymem-2.0.2\n",
            "  Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Found existing installation: wrapt 1.11.2\n",
            "    Uninstalling wrapt-1.11.2:\n",
            "      Successfully uninstalled wrapt-1.11.2\n",
            "  Found existing installation: dill 0.3.0\n",
            "    Uninstalling dill-0.3.0:\n",
            "      Successfully uninstalled dill-0.3.0\n",
            "  Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Found existing installation: spacy 2.1.8\n",
            "    Uninstalling spacy-2.1.8:\n",
            "      Successfully uninstalled spacy-2.1.8\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed convokit-2.1.11 cymem-1.31.2 cytoolz-0.9.0.1 dill-0.2.9 msgpack-numpy-0.4.3.2 murmurhash-0.28.0 nltk-3.4.5 preshed-1.0.1 regex-2017.4.5 spacy-2.0.12 thinc-6.10.3 ujson-1.35 wrapt-1.10.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXk4Biep3xHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from convokit import Corpus, User, Utterance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmw72j5J44E4",
        "colab_type": "text"
      },
      "source": [
        "## The Friends Dataset\n",
        "\n",
        "The original dataset (https://github.com/emorynlp/character-mining) contains a set of 10 JSON files, each of which represents a complete transcript of 1 season of <i>Friends</i>. Since the data are available in JSON format from this GitHub repo, we download the raw data directly using the `requests` module. You will not need to download raw data files to use this script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7CI3mm378N8",
        "colab_type": "text"
      },
      "source": [
        "## Gather information about the corpus\n",
        "For the **corpus.json** file, it will include information of number of episodes, number of scenes, number of utterances and number of speakers.\n",
        "When counting the number of utterances, we ignore utterances that have no conversations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN79FO2u6B8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 0\n",
        "num_scenes = 0\n",
        "num_utterances = 0\n",
        "speakers = set()\n",
        "for i in range(1,11):\n",
        "  season_number = '0'+str(i) if i < 10 else '10'\n",
        "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
        "  r = requests.get(json_file)\n",
        "  \n",
        "  season = json.loads(r.text)\n",
        "  episodes = season['episodes']\n",
        "  num_episodes += len(episodes)\n",
        "  for j in range(len(episodes)):\n",
        "    episode = episodes[j]\n",
        "    scenes = episode['scenes']\n",
        "    num_scenes += len(scenes)\n",
        "    for k in range(len(scenes)):\n",
        "      scene = scenes[k]\n",
        "      utterances = scene['utterances']\n",
        "      for l in range(len(utterances)):\n",
        "        utterance = utterances[l]\n",
        "        speaker = utterance['speakers']\n",
        "        speakers.update(speaker)\n",
        "        num_utterances += 1 if len(speaker) != 0 else 0\n",
        "corpus = {'friends': 'friends corpus', 'num_episodes': num_episodes, 'num_scenes': num_scenes, 'num_utterances': num_utterances, 'num_speakers': len(speakers)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o827biTbBtlu",
        "colab_type": "code",
        "outputId": "2f9220ac-f1ce-4300-c1a0-10def33c2205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(corpus)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'friends': 'friends corpus', 'num_episodes': 236, 'num_scenes': 3107, 'num_utterances': 61338, 'num_speakers': 700}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhlL6aNCvxr",
        "colab_type": "text"
      },
      "source": [
        "## Generating user information\n",
        "Since our dataset doesn't have any existing user information, we extract speaker information from the conversation. For each user, we collect the episode in which he/she first appears and guess his/her gender based on the name using the gender_guesser module.\n",
        "\n",
        "Users are indexed by their name, which is a `<str>`. For each user, we create an object with:\n",
        "\n",
        "- <b>first_appearance:</b> the episode in which he or she first appeared\n",
        "- <b>gender:</b> the character's gender, as defined by the `gender_guesser` module's guess of his/her name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw_7_7YJMIWp",
        "colab_type": "code",
        "outputId": "042fc993-c16a-459c-ded4-20ee6b2e8dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "! pip3 install gender_guesser\n",
        "import gender_guesser.detector as gender\n",
        "d = gender.Detector()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gender_guesser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/fb/3f2aac40cd2421e164cab1668e0ca10685fcf896bd6b3671088f8aab356e/gender_guesser-0.4.0-py2.py3-none-any.whl (379kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: gender-guesser\n",
            "Successfully installed gender-guesser-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjNZT57Qv7Vd",
        "colab_type": "code",
        "outputId": "38b93218-082b-4c15-fadd-623222ffd368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "users = {}\n",
        "for i in tqdm(range(1,11)):\n",
        "  season_number = '0'+str(i) if i < 10 else '10'\n",
        "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
        "  r = requests.get(json_file)\n",
        "  \n",
        "  season = json.loads(r.text)\n",
        "  episodes = season['episodes']\n",
        "  for j in range(len(episodes)):\n",
        "    episode = episodes[j]\n",
        "    scenes = episode['scenes']\n",
        "    for k in range(len(scenes)):\n",
        "      scene = scenes[k]\n",
        "      utterances = scene['utterances']\n",
        "      for l in range(len(utterances)):\n",
        "        utterance = utterances[l]\n",
        "        speaker_list = utterance['speakers']\n",
        "        for speaker in speaker_list:\n",
        "          if speaker not in users:\n",
        "            users[speaker] = {'first_appearance': episode['episode_id'], 'gender': d.get_gender(speaker.split()[0])}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:03<00:00,  3.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJqo7WSSCua1",
        "colab_type": "text"
      },
      "source": [
        "Sanity-checking the user data, we should see the correct genders assigned to the 6 friends:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeKBCPZqCMeH",
        "colab_type": "code",
        "outputId": "a86de171-4985-431c-b930-effcb6327c85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print(\"number of users in the data = {}/700\".format(len(users)))\n",
        "print(\"Monica Geller object: \", users[\"Monica Geller\"])\n",
        "print(\"Joey Tribbiani object: \", users[\"Joey Tribbiani\"])\n",
        "print(\"Chandler Bing object: \", users[\"Chandler Bing\"])\n",
        "print(\"Phoebe Buffay object: \", users[\"Phoebe Buffay\"])\n",
        "print(\"Ross Geller object: \", users[\"Ross Geller\"])\n",
        "print(\"Rachel Green object: \", users[\"Rachel Green\"])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of users in the data = 700/700\n",
            "Monica Geller object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n",
            "Joey Tribbiani object:  {'first_appearance': 's01_e01', 'gender': 'male'}\n",
            "Chandler Bing object:  {'first_appearance': 's01_e01', 'gender': 'mostly_male'}\n",
            "Phoebe Buffay object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n",
            "Ross Geller object:  {'first_appearance': 's01_e01', 'gender': 'male'}\n",
            "Rachel Green object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Csa06wuFh8U",
        "colab_type": "text"
      },
      "source": [
        "We then create a User object for each unique character in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs7QjJJJFg2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_users = {k: User(name=k, meta=v) for k,v in users.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfMxU1u_GJD7",
        "colab_type": "code",
        "outputId": "93d7af5d-dbca-4465-f72e-18a8fd10a991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(corpus_users['Monica Geller'].name)\n",
        "print(corpus_users['Monica Geller'].meta)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Monica Geller\n",
            "{'first_appearance': 's01_e01', 'gender': 'female'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tJ_9BMXFygP",
        "colab_type": "text"
      },
      "source": [
        "## Generating Utterances\n",
        "\n",
        "We then loop through the data to generate a list of all utterances in the series. To align with the Utterance schema ConvoKit expects, we construct for each utterance:\n",
        "\n",
        "- **id:** index of the utterance\n",
        "\n",
        "- **user:** the user who authored the utterance; the speaker in our case\n",
        "\n",
        "- **root:** id of the conversation root of the utterance; the first utterance in the scene, in our case\n",
        "\n",
        "- **reply_to:** id of the utterance to which this utterance replies to; None if the utterance is not a reply.\n",
        "\n",
        "- **timestamp:** time of the utterance (None for us -- the dataset does not contain this information)\n",
        "\n",
        "- **text:** textual content of the utterance\n",
        "\n",
        "We also pull in the following metadata including:\n",
        "- **tokens** a tokenized representation of the text (handy for sentence separation)\n",
        "-**character_entities** available for some but not all utterances; `None` if unavailable. These are intended to identify who the user is speaking to and/or about.\n",
        "-**emotion** emotion labels for each token. Available for some but not all utterances; `None` if unavailable. \n",
        "-**caption**  available for some but not all utterances; `None` if unavailable. This contains the begin time, end time, and text sans punctuation. Only available for seasons 6-9.\n",
        "-**transcript_with_note**  a version of the text with an action note (e.g. \"(to Ross) Hand me the coffee\" vs. \"Hand me the coffee\"). Available for some but not all utterances; `None` if unavailable.\n",
        "-**token_with_note** a tokenized representation of the above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDgFZjF6GN3O",
        "colab_type": "code",
        "outputId": "dbbba243-201c-40f3-8c60-b7cd89e14d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_utterances = {}\n",
        "\n",
        "\n",
        "\n",
        "for i in tqdm(range(1,11)):\n",
        "  season_number = '0'+str(i) if i < 10 else '10'\n",
        "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
        "  r = requests.get(json_file)\n",
        "  \n",
        "  season = json.loads(r.text)\n",
        "  episodes = season['episodes']\n",
        "  for j in range(len(episodes)):\n",
        "    episode = episodes[j]\n",
        "    scenes = episode['scenes']\n",
        "    for k in range(len(scenes)):\n",
        "      scene = scenes[k]\n",
        "      utterances = scene['utterances']\n",
        "      \n",
        "      root = utterances[0] #set the root as the first utterance in the scene for now\n",
        "      \n",
        "      prev_utt = None\n",
        "\n",
        "      for l in range(len(utterances)):\n",
        "        utterance = utterances[l]\n",
        "        \n",
        "        speaker = utterance['speakers']\n",
        "        \n",
        "        if len(speaker) == 0:\n",
        "          prev_utt = None\n",
        "          continue\n",
        "        \n",
        "        # Add meta       \n",
        "        meta = {\n",
        "            'tokens': utterance.get('tokens'),\n",
        "            'character_entities': utterance.get('character_entities'),\n",
        "            'emotion': utterance.get('emotion'),\n",
        "            'caption': utterance.get('caption'),\n",
        "            'transcript_with_note': utterance.get('transcript_with_note'),\n",
        "            'tokens_with_note': utterance.get('tokens_with_note')\n",
        "        }\n",
        "        \n",
        "        # Create the Utterance, including meta\n",
        "        all_utterances[utterance['utterance_id']] = Utterance(\n",
        "            id=utterance['utterance_id'],\n",
        "            user=corpus_users[speaker[0]],\n",
        "            root=root['utterance_id'],\n",
        "            reply_to=prev_utt,\n",
        "            timestamp=None,\n",
        "            text=utterance['transcript'],\n",
        "            meta=meta\n",
        "        )\n",
        "        \n",
        "        # Get the prev_utt for the next iteration\n",
        "        prev_utt = utterance['utterance_id']\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmVQJYBiI1aY",
        "colab_type": "code",
        "outputId": "ba38de51-a9af-4272-82a3-0ba843cefa99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"This corpus has {}/61309 utterances\".format(len(all_utterances)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This corpus has 61338/61309 utterances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkOm5qDdZYRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#all_utterances['s01_e18_c05_u021']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjb5fkwmN5Ma",
        "colab_type": "text"
      },
      "source": [
        "## Creating the corpus from a list of utterances\n",
        "\n",
        "We now create the corpus from our dict of utterances. Note, we are are allowing convokit to create conversations IDs automatically after loading the utterances list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C_xjYMfOKVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "utterance_list = [utt for k, utt in all_utterances.items()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVyGuspKOU7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "friends_corpus = Corpus(utterances=utterance_list, version=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW5uEDZIcjWO",
        "colab_type": "text"
      },
      "source": [
        "Sanity checks for the number of conversations in the dataset and the first 5 conversations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCQcxnu1OZXv",
        "colab_type": "code",
        "outputId": "7f613faa-5b0b-47de-a35c-933a4768b336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"number of conversations in the dataset={}\".format(len(friends_corpus.get_conversation_ids())))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of conversations in the dataset=3099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uypy9F-Vx0kM",
        "colab_type": "code",
        "outputId": "b64cf9dc-4124-4636-cd49-fadec7321763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "convo_ids = friends_corpus.get_conversation_ids()\n",
        "for i, convo_idx in enumerate(convo_ids[0:5]):\n",
        "    print(\"sample conversation {}:\".format(i))\n",
        "    print(friends_corpus.get_conversation(convo_idx).get_utterance_ids())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample conversation 0:\n",
            "['s01_e01_c01_u001', 's01_e01_c01_u002', 's01_e01_c01_u003', 's01_e01_c01_u004', 's01_e01_c01_u006', 's01_e01_c01_u007', 's01_e01_c01_u008', 's01_e01_c01_u010', 's01_e01_c01_u011', 's01_e01_c01_u012', 's01_e01_c01_u013', 's01_e01_c01_u014', 's01_e01_c01_u015', 's01_e01_c01_u016', 's01_e01_c01_u017', 's01_e01_c01_u018', 's01_e01_c01_u019', 's01_e01_c01_u021', 's01_e01_c01_u022', 's01_e01_c01_u023', 's01_e01_c01_u024', 's01_e01_c01_u025', 's01_e01_c01_u026', 's01_e01_c01_u027', 's01_e01_c01_u028', 's01_e01_c01_u029', 's01_e01_c01_u030', 's01_e01_c01_u031', 's01_e01_c01_u032', 's01_e01_c01_u033', 's01_e01_c01_u034', 's01_e01_c01_u035', 's01_e01_c01_u036', 's01_e01_c01_u037', 's01_e01_c01_u038', 's01_e01_c01_u039', 's01_e01_c01_u040', 's01_e01_c01_u041', 's01_e01_c01_u042', 's01_e01_c01_u044', 's01_e01_c01_u045', 's01_e01_c01_u047', 's01_e01_c01_u048', 's01_e01_c01_u049', 's01_e01_c01_u050', 's01_e01_c01_u051', 's01_e01_c01_u052', 's01_e01_c01_u053', 's01_e01_c01_u055', 's01_e01_c01_u056', 's01_e01_c01_u057', 's01_e01_c01_u058']\n",
            "sample conversation 1:\n",
            "['s01_e01_c02_u001', 's01_e01_c02_u002', 's01_e01_c02_u003', 's01_e01_c02_u004', 's01_e01_c02_u006', 's01_e01_c02_u007', 's01_e01_c02_u008', 's01_e01_c02_u009', 's01_e01_c02_u011', 's01_e01_c02_u012', 's01_e01_c02_u013', 's01_e01_c02_u014', 's01_e01_c02_u015', 's01_e01_c02_u017', 's01_e01_c02_u018', 's01_e01_c02_u019', 's01_e01_c02_u020', 's01_e01_c02_u021', 's01_e01_c02_u022', 's01_e01_c02_u023', 's01_e01_c02_u024', 's01_e01_c02_u026', 's01_e01_c02_u027', 's01_e01_c02_u028', 's01_e01_c02_u029', 's01_e01_c02_u030', 's01_e01_c02_u031', 's01_e01_c02_u032', 's01_e01_c02_u033', 's01_e01_c02_u034', 's01_e01_c02_u035', 's01_e01_c02_u036', 's01_e01_c02_u037', 's01_e01_c02_u038', 's01_e01_c02_u039', 's01_e01_c02_u040', 's01_e01_c02_u041', 's01_e01_c02_u043', 's01_e01_c02_u044', 's01_e01_c02_u045', 's01_e01_c02_u046', 's01_e01_c02_u047', 's01_e01_c02_u048', 's01_e01_c02_u049', 's01_e01_c02_u051', 's01_e01_c02_u052', 's01_e01_c02_u053', 's01_e01_c02_u054', 's01_e01_c02_u055', 's01_e01_c02_u056', 's01_e01_c02_u057', 's01_e01_c02_u058', 's01_e01_c02_u059', 's01_e01_c02_u060', 's01_e01_c02_u061', 's01_e01_c02_u062']\n",
            "sample conversation 2:\n",
            "['s01_e01_c03_u001']\n",
            "sample conversation 3:\n",
            "['s01_e01_c04_u001', 's01_e01_c04_u003', 's01_e01_c04_u004', 's01_e01_c04_u005', 's01_e01_c04_u006', 's01_e01_c04_u007', 's01_e01_c04_u008', 's01_e01_c04_u010', 's01_e01_c04_u011', 's01_e01_c04_u012', 's01_e01_c04_u013', 's01_e01_c04_u014', 's01_e01_c04_u015', 's01_e01_c04_u016', 's01_e01_c04_u017', 's01_e01_c04_u018', 's01_e01_c04_u019']\n",
            "sample conversation 4:\n",
            "['s01_e01_c05_u001', 's01_e01_c05_u002', 's01_e01_c05_u003', 's01_e01_c05_u004', 's01_e01_c05_u005', 's01_e01_c05_u006', 's01_e01_c05_u007', 's01_e01_c05_u008', 's01_e01_c05_u009']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAHTbGQxcor3",
        "colab_type": "text"
      },
      "source": [
        "Summary stats for the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0pyZSmWW9_1",
        "colab_type": "code",
        "outputId": "ef148fd6-12bc-49b9-e2cc-1641e675ec9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "friends_corpus.print_summary_stats()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Users: 699\n",
            "Number of Utterances: 61338\n",
            "Number of Conversations: 3099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoTiJaTq02RO",
        "colab_type": "text"
      },
      "source": [
        "## Updating Gender for our Main Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LkzxnCY0hcH",
        "colab_type": "code",
        "outputId": "94b8b13f-f818-42aa-d6a2-caaee5d14f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "users['Chandler Bing']['gender'] = 'male'\n",
        "users['Carol Willick']['gender'] = 'female'\n",
        "print(users['Chandler Bing'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'first_appearance': 's01_e01', 'gender': 'male'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y7iXV9V1QVl",
        "colab_type": "text"
      },
      "source": [
        "# D1. Exploring Dataset\n",
        "## Taking character entities and placing at conversation level\n",
        "For each scene, we extract unique people that are mentioned during converstion from character entities and unique speakers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nD0nJ5Q01mz",
        "colab_type": "code",
        "outputId": "5df029cd-e077-451f-fec2-73fbdafe42c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "import string\n",
        "string.ascii_uppercase\n",
        "\n",
        "all_ce = {}\n",
        "\n",
        "\n",
        "for i in tqdm(range(1,11)):\n",
        "  season_number = '0'+str(i) if i < 10 else '10'\n",
        "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
        "  r = requests.get(json_file)\n",
        "  \n",
        "  season = json.loads(r.text)\n",
        "  episodes = season['episodes']\n",
        "  for j in range(len(episodes)):\n",
        "    episode = episodes[j]\n",
        "    scenes = episode['scenes']\n",
        "    for k in range(len(scenes)):\n",
        "      scene = scenes[k]\n",
        "      utterances = scene['utterances']\n",
        "      ces = set()\n",
        "      spkrs = set()\n",
        "      for l in range(len(utterances)):\n",
        "        utterance = utterances[l]\n",
        "        if 'character_entities' in utterance.keys():\n",
        "          character_entities = utterance['character_entities']\n",
        "          speakers = utterance['speakers']\n",
        "          for char in character_entities:\n",
        "            if len(char) != 0:\n",
        "               for li in char:\n",
        "                  name=li[2]\n",
        "                  if name != speakers[0]:\n",
        "                    ces.add(name)\n",
        "                    ceg=[]\n",
        "                    for n in ces:\n",
        "                      ce_g = d.get_gender(n.split()[0])\n",
        "                      ceg.append(ce_g)\n",
        "                      c_f=[]\n",
        "                      c_m=[]\n",
        "                      for cg in ceg:\n",
        "                        if cg=='female':\n",
        "                          cf=1\n",
        "                          c_f.append(cf)\n",
        "                        if cg=='male':\n",
        "                          cm=1\n",
        "                          c_m.append(cm)\n",
        "        if 'speakers' in utterance.keys():\n",
        "          speakers = utterance['speakers']\n",
        "          for sp in speakers:\n",
        "            for list in sp:\n",
        "              sname=sp\n",
        "              spkrs.add(sname) \n",
        "              spg=[]\n",
        "              for z in spkrs:\n",
        "                sp_g = d.get_gender(z.split()[0])\n",
        "                spg.append(sp_g)\n",
        "                s_f=[]\n",
        "                s_m=[]\n",
        "                for sg in spg:\n",
        "                  if sg=='female':\n",
        "                    sf=1\n",
        "                    s_f.append(sf)\n",
        "                  if sg=='male' or sg=='mostly_male':\n",
        "                    sm=1\n",
        "                    s_m.append(sm)\n",
        "      all_ce[scene['scene_id']] = {'character_entities': ces, 'ce_m': c_m, 'ce_f': c_f, 'speakers': spkrs, 'spkr_m': s_m, 'spkr_f': s_f}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:18<00:00,  1.67s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3pwQJT9lSTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(all_ce['s05_e01_c01'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_XaAVpQDR0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#all_ce.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLmWb71FlmRI",
        "colab_type": "text"
      },
      "source": [
        "## Extracting Romantic Words from Conversation\n",
        "For each scene, we'll check what romantic (and nonromantic) words are used during the conversation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELjcL6cwlluw",
        "colab_type": "code",
        "outputId": "85d3b5e7-37f2-4391-a85a-8b9da019285d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d555c679-00e2-48ec-bdd9-165f384e8c37\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d555c679-00e2-48ec-bdd9-165f384e8c37\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving RomanticWords.txt to RomanticWords.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs68HHklo4Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3REMxBVqh4D",
        "colab_type": "text"
      },
      "source": [
        "Read from RomanticWords.txt and store in root word. We use only words instead of phrases to reduce the checking time of each utterance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GQl0aC9mtvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "romantic_words = [ps.stem(line.rstrip('\\n')) for line in open('RomanticWords.txt', 'r')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bLf2Z78nEc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(romantic_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FxXruSInZtd",
        "colab_type": "code",
        "outputId": "a14dd0d3-cb74-43d3-e88e-3f032c7b4d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import regex\n",
        "import re\n",
        "\n",
        "extract_romantic = {}\n",
        "\n",
        "for i in tqdm(range(1,11)):\n",
        "  season_number = '0'+str(i) if i < 10 else '10'\n",
        "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
        "  r = requests.get(json_file)\n",
        "  \n",
        "  season = json.loads(r.text)\n",
        "  episodes = season['episodes']\n",
        "  for j in range(len(episodes)):\n",
        "    episode = episodes[j]\n",
        "    scenes = episode['scenes']\n",
        "    for k in range(len(scenes)):\n",
        "      scene = scenes[k]\n",
        "      utterances = scene['utterances']\n",
        "      romantic = []\n",
        "      non_romantic = []\n",
        "      for l in range(len(utterances)):\n",
        "        utterance = utterances[l]\n",
        "        if len(utterance['speakers']) == 0 :\n",
        "          continue\n",
        "        tokens = utterance['tokens']\n",
        "        for token in tokens:\n",
        "          for word in token:\n",
        "            word=re.sub(r'[^\\w\\s]', '', word) #remove all punctuation so it isn't included as a word\n",
        "            if ps.stem(word) in romantic_words:\n",
        "              romantic.append(word)\n",
        "            else:\n",
        "                if len(word)!=0:\n",
        "                  non_romantic.append(word)\n",
        "      extract_romantic[scene['scene_id']] = {'romantic words': romantic, 'nonromantic words': non_romantic}"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyaaDTCarjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract_romantic.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2cImn_0pxHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(extract_romantic['s01_e01_c01'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8mmhuEsFvRT",
        "colab_type": "text"
      },
      "source": [
        "# Assessing the Romantic Words and Genders\n",
        "We create a dataframe of the total number of romantic and nonromantic words used per scene, as well as counts of the total number of character entities (and speakers) in all and by sex by scene. We then use these variables to calculate statistics on the types of conversations occuring. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBfgwsitF0OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r= [(scene_id, len(extract_romantic[scene_id]['romantic words']), len(extract_romantic[scene_id]['nonromantic words'])) for scene_id in extract_romantic.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgkFv7deJoGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c=[(scene_id, len(all_ce[scene_id]['character_entities']), len(all_ce[scene_id]['ce_m']), len(all_ce[scene_id]['ce_f']) , len(all_ce[scene_id]['speakers']), len(all_ce[scene_id]['spkr_f']), len(all_ce[scene_id]['spkr_m'])) for scene_id in all_ce.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6cfvil3KNdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "c1= pd.DataFrame.from_dict(c)\n",
        "c1.columns = ['scene_id', 'character_entities', 'ce_m', 'ce_f', 'speakers', 'spkr_f', 'spkr_m']  \n",
        "r1= pd.DataFrame.from_dict(r)\n",
        "r1.columns=['scene_id', 'romantic words', 'nonromantic words']\n",
        "df = pd.merge(c1, r1, left_on='scene_id', right_on='scene_id')\n",
        "\n",
        "dfn = df[\"scene_id\"].str.split(\"_\", n = 1, expand = True)\n",
        "dfn= dfn[0].str.replace(\"s\", \"\", regex=True)\n",
        "df[\"season\"]= dfn\n",
        "df['season'] = df['season'].astype(int)\n",
        "#print (df.dtypes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfE5DY89eJYk",
        "colab_type": "text"
      },
      "source": [
        "Calculating the percent of romantic words used by scene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdGMKxt0aTFh",
        "colab_type": "code",
        "outputId": "e4bea11d-4e75-4821-8694-1f5be7cba642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def calculate_average(row):\n",
        "    return ((row['romantic words']+1)/(row['romantic words']+row['nonromantic words']+1))*100\n",
        "\n",
        "df.apply(calculate_average, axis=1)\n",
        "df['avg_rom'] = df.apply(calculate_average, axis=1)\n",
        "\n",
        "avg_rom=[]\n",
        "x= df['avg_rom'].mean()\n",
        "avg_rom.append(x)\n",
        "\n",
        "y= df['avg_rom'].min()\n",
        "avg_rom.append(y)\n",
        "\n",
        "z= df['avg_rom'].max()\n",
        "avg_rom.append(z)\n",
        "\n",
        "\n",
        "print(avg_rom)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.186444081665329, 0.2127659574468085, 100.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x6F0U91fErd",
        "colab_type": "text"
      },
      "source": [
        "Calculating the total number of scenes with only male and only female characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0028f32b-f63c-482f-b5e8-b13be8e7f665",
        "id": "JFZSEKezfDbu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#Female Total\n",
        "f_all = len(df[(df['spkr_m']==0) & (df['spkr_f']>=2)]) # (255)\n",
        "print(f_all)\n",
        "\n",
        "f_14=len(df[(df['spkr_m']==0) & (df['spkr_f']>=2) & (df['season']<=4)]) # (127)\n",
        "print(f_14)\n",
        "\n",
        "\n",
        "#Male total \n",
        "m_all = len(df[(df['spkr_f']==0) & (df['spkr_m']>=2)]) # (343)\n",
        "print(m_all)\n",
        "\n",
        "m_14 = len(df[(df['spkr_f']==0) & (df['spkr_m']>=2) & (df['season']<=4)]) # (155)\n",
        "print(m_14)\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "255\n",
            "127\n",
            "343\n",
            "155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny0G1gJEqPoT",
        "colab_type": "text"
      },
      "source": [
        "Calculating the total number of scenes with only male and only female characters with above average rates of romantic words used and 50% or more of the character entities of the opposite sex. NOTE: This is only one way in which we can define romantic relationships. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZFYUSjPulY0",
        "colab_type": "code",
        "outputId": "6a14b81d-3e5e-4a3a-cf31-3b1b5960af88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Proportion of male character entities (calculating only for seasons 1-4)\n",
        "def calculate_propm(row):\n",
        "    return ((row['ce_m']+1)/(row['character_entities']+1))*100\n",
        "\n",
        "df.apply(calculate_propm, axis=1)\n",
        "df['ce_mp'] = df.apply(calculate_propm, axis=1)\n",
        "\n",
        "\n",
        "ce_mp=[]\n",
        "x= df['ce_mp'].iloc[0:1304].mean() \n",
        "ce_mp.append(x)\n",
        "\n",
        "y= df['ce_mp'].iloc[0:1304].min()\n",
        "ce_mp.append(y)\n",
        "\n",
        "z= df['ce_mp'].iloc[0:1304].max()\n",
        "ce_mp.append(z)\n",
        "\n",
        "\n",
        "print(ce_mp)\n",
        "\n",
        "#Proportion of female character entities\n",
        "def calculate_propf(row):\n",
        "    return ((row['ce_f']+1)/(row['character_entities']+1))*100\n",
        "\n",
        "df.apply(calculate_propf, axis=1)\n",
        "df['ce_fp'] = df.apply(calculate_propf, axis=1)\n",
        "\n",
        "\n",
        "ce_fp=[]\n",
        "x= df['ce_fp'].iloc[0:1304].mean()\n",
        "ce_fp.append(x)\n",
        "\n",
        "y= df['ce_fp'].iloc[0:1304].min()\n",
        "ce_fp.append(y)\n",
        "\n",
        "z= df['ce_fp'].iloc[0:1304].max()\n",
        "ce_fp.append(z)\n",
        "\n",
        "\n",
        "print(ce_fp)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[43.76059440586523, 12.5, 500.0]\n",
            "[44.058208472973504, 11.11111111111111, 600.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRTrmavtrNyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only female with higher than average romantic language (& >=50% male character entities for seasons 1-4) (6)\n",
        "f_mr_14 = len(df[(df['spkr_m']==0) & (df['spkr_f']>=2) & (df['avg_rom']>=2.18) & (df['ce_mp']>=50) & (df['season']<=4)])\n",
        "f_mr_all = len(df[(df['spkr_m']==0) & (df['spkr_f']>=2) & (df['avg_rom']>=2.18)])\n",
        "\n",
        "# only male with higher than average romantic language (& >=50% female character entities for seasons 1-4) (10)\n",
        "m_fr_14 = len(df[(df['spkr_f']==0) & (df['spkr_m']>=2) & (df['avg_rom']>=2.18) & (df['ce_fp']>=50)  & (df['season']<=4)])\n",
        "m_fr_all = len(df[(df['spkr_f']==0) & (df['spkr_m']>=2) & (df['avg_rom']>=2.18)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmVqSCTxTYn",
        "colab_type": "code",
        "outputId": "1c27dcc2-c509-4988-b4d3-ca57f03773f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# of the conversations with only females, what percent were about romantic conversations\n",
        "print(f_mr_14/f_14)\n",
        "print(f_mr_all/f_all)\n",
        "\n",
        "# of the conversations with only females, what percent were about romantic conversations\n",
        "print(m_fr_14/m_14)\n",
        "print(m_fr_all/m_all)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.047244094488188976\n",
            "0.23529411764705882\n",
            "0.06451612903225806\n",
            "0.2594752186588921\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}