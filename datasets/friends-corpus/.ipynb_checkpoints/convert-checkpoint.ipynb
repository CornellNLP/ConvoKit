{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_YNRDvb3etb"
   },
   "source": [
    "# Converting the Friends dataset into ConvoKit format\n",
    "\n",
    "This notebook describes how we converted the Friends dataset (https://github.com/emorynlp/character-mining) into a Corpus with ConvoKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Wp3WIlBN4D7l",
    "outputId": "d175f9db-6bab-416b-d2d1-b98b1af1fd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting convokit\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f3/d04f33525a2dbf316c31bfab383d1ee84280714b474718e272882e8b2d6a/convokit-2.0.11.tar.gz (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 3.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from convokit) (3.0.3)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.6/dist-packages (from convokit) (0.24.2)\n",
      "Collecting msgpack-numpy==0.4.3.2 (from convokit)\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/45/464be6da85b5ca893cfcbd5de3b31a6710f636ccb8521b17bd4110a08d94/msgpack_numpy-0.4.3.2-py2.py3-none-any.whl\n",
      "Collecting spacy==2.0.12 (from convokit)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/de/ac14cd453c98656d6738a5669f96a4ac7f668493d5e6b78227ac933c5fd4/spacy-2.0.12.tar.gz (22.0MB)\n",
      "\u001b[K     |████████████████████████████████| 22.0MB 1.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from convokit) (1.3.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from convokit) (0.21.3)\n",
      "Collecting nltk>=3.4 (from convokit)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 36.6MB/s \n",
      "\u001b[?25hCollecting dill==0.2.9 (from convokit)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/42/bfe2e0857bc284cbe6a011d93f2a9ad58a22cb894461b199ae72cfef0f29/dill-0.2.9.tar.gz (150kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 48.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (1.16.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (2.5.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->convokit) (2.4.2)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.4->convokit) (2018.9)\n",
      "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy==0.4.3.2->convokit) (0.5.6)\n",
      "Collecting murmurhash<0.29,>=0.28 (from spacy==2.0.12->convokit)\n",
      "  Downloading https://files.pythonhosted.org/packages/82/55/7f050e9f73c9a58df219c63e77304b0ff01676847061dc99abb484cff3a8/murmurhash-0.28.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting cymem<1.32,>=1.30 (from spacy==2.0.12->convokit)\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/0f/d29aa68c55db37844c77e7e96143bd96651fd0f4453c9f6ee043ac846b77/cymem-1.31.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting preshed<2.0.0,>=1.0.0 (from spacy==2.0.12->convokit)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/88/57a818051f3d71e800bfb7ba4df56d3ea5793482ef11f1d2109b726f3bac/preshed-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 29.1MB/s \n",
      "\u001b[?25hCollecting thinc<6.11.0,>=6.10.3 (from spacy==2.0.12->convokit)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/b1/47a88072d0a38b3594c0a638a62f9ef7c742b8b8a87f7b105f7ed720b14b/thinc-6.10.3.tar.gz (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 43.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.12->convokit) (0.9.6)\n",
      "Collecting ujson>=1.35 (from spacy==2.0.12->convokit)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 53.6MB/s \n",
      "\u001b[?25hCollecting regex==2017.4.5 (from spacy==2.0.12->convokit)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 51.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.12->convokit) (2.21.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->convokit) (0.13.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.4->convokit) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->convokit) (41.2.0)\n",
      "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz (443kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 53.6MB/s \n",
      "\u001b[?25hCollecting wrapt<1.11.0,>=1.10.0 (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit) (4.28.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit) (2019.6.16)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit) (0.10.0)\n",
      "Building wheels for collected packages: convokit, spacy, nltk, dill, thinc, ujson, regex, cytoolz, wrapt\n",
      "  Building wheel for convokit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for convokit: filename=convokit-2.0.11-cp36-none-any.whl size=82047 sha256=8c1c88e6452cf589b03683afa944799b5c82ef771edbba24571bfa46a19e74f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/a2/74/ba/90b7717e5dcfcc83ce63c34cc6e20d60ef850381dba5b3a0c9\n",
      "  Building wheel for spacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for spacy: filename=spacy-2.0.12-cp36-cp36m-linux_x86_64.whl size=29062596 sha256=8d7ab456613fd9ceb7b6da43fd521230561fd5c638820b5d574ab72b894c7b02\n",
      "  Stored in directory: /root/.cache/pip/wheels/60/0b/bb/7c2e28db574dbb2358176934eddd32a1c5f838ba0bc23eaaab\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449907 sha256=34ce540105dc08138085e434fe1988d55f2c359241c622720852aae9afa6dcd0\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for dill: filename=dill-0.2.9-cp36-none-any.whl size=77403 sha256=25e87d1ddd5841d16ac4d74baacf401220988b1d7ad6f0ebe8956a30f0182a96\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/d7/0f/e58eae695403de585269f4e4a94e0cd6ca60ec0c202936fa4a\n",
      "  Building wheel for thinc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for thinc: filename=thinc-6.10.3-cp36-cp36m-linux_x86_64.whl size=4120057 sha256=c306ec528059db9d08831d1ca0409cf9465472f42a6290562412f92c478be194\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/bc/e1/9b321b6b203288cf636a56e668ed5700076af4ed66062278ca\n",
      "  Building wheel for ujson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ujson: filename=ujson-1.35-cp36-cp36m-linux_x86_64.whl size=68040 sha256=79112025513e4542deec9c67a7f8280058a69e6a986c4065d63b74393ae3ba7b\n",
      "  Stored in directory: /root/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533183 sha256=ca63e6e0e37add16202c98bbb126f30a70405cc49261ee7476f33f969f146ffc\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for cytoolz: filename=cytoolz-0.9.0.1-cp36-cp36m-linux_x86_64.whl size=1247737 sha256=f3f79072919437eb1feda9a71eb1540b0b7b4489aca34c86afa46cbd611c8098\n",
      "  Stored in directory: /root/.cache/pip/wheels/88/f3/11/9817b001e59ab04889e8cffcbd9087e2e2155b9ebecfc8dd38\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.10.11-cp36-cp36m-linux_x86_64.whl size=65120 sha256=25177f2a5f0d4f5a759b869023ee93f83db85b8954832acf04a6f3923625d7ca\n",
      "  Stored in directory: /root/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n",
      "Successfully built convokit spacy nltk dill thinc ujson regex cytoolz wrapt\n",
      "\u001b[31mERROR: tensorflow 1.14.0 has requirement wrapt>=1.11.1, but you'll have wrapt 1.10.11 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: multiprocess 0.70.8 has requirement dill>=0.3.0, but you'll have dill 0.2.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fastai 1.0.57 has requirement spacy>=2.0.18, but you'll have spacy 2.0.12 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: en-core-web-sm 2.1.0 has requirement spacy>=2.1.0, but you'll have spacy 2.0.12 which is incompatible.\u001b[0m\n",
      "Installing collected packages: msgpack-numpy, murmurhash, cymem, preshed, cytoolz, wrapt, dill, thinc, ujson, regex, spacy, nltk, convokit\n",
      "  Found existing installation: murmurhash 1.0.2\n",
      "    Uninstalling murmurhash-1.0.2:\n",
      "      Successfully uninstalled murmurhash-1.0.2\n",
      "  Found existing installation: cymem 2.0.2\n",
      "    Uninstalling cymem-2.0.2:\n",
      "      Successfully uninstalled cymem-2.0.2\n",
      "  Found existing installation: preshed 2.0.1\n",
      "    Uninstalling preshed-2.0.1:\n",
      "      Successfully uninstalled preshed-2.0.1\n",
      "  Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "  Found existing installation: dill 0.3.0\n",
      "    Uninstalling dill-0.3.0:\n",
      "      Successfully uninstalled dill-0.3.0\n",
      "  Found existing installation: thinc 7.0.8\n",
      "    Uninstalling thinc-7.0.8:\n",
      "      Successfully uninstalled thinc-7.0.8\n",
      "  Found existing installation: spacy 2.1.8\n",
      "    Uninstalling spacy-2.1.8:\n",
      "      Successfully uninstalled spacy-2.1.8\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "Successfully installed convokit-2.0.11 cymem-1.31.2 cytoolz-0.9.0.1 dill-0.2.9 msgpack-numpy-0.4.3.2 murmurhash-0.28.0 nltk-3.4.5 preshed-1.0.1 regex-2017.4.5 spacy-2.0.12 thinc-6.10.3 ujson-1.35 wrapt-1.10.11\n"
     ]
    }
   ],
   "source": [
    "!pip3 install convokit\n",
    "# !python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXk4Biep3xHH"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, User, Utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmw72j5J44E4"
   },
   "source": [
    "## The Friends Dataset\n",
    "\n",
    "The original dataset (https://github.com/emorynlp/character-mining) contains a set of 10 JSON files, each of which represents a complete transcript of 1 season of <i>Friends</i>. Since the data are available in JSON format from this GitHub repo, we download the raw data directly using the `requests` module. You will not need to download raw data files to use this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxhlL6aNCvxr"
   },
   "source": [
    "## Generating user information\n",
    "Since our dataset doesn't have any existing user information, we extract speaker information from the conversation. For each user, we collect the episode in which he/she first appears and guess his/her gender based on the name using the gender_guesser module.\n",
    "\n",
    "Users are indexed by their name, which is a `<str>`. For each user, we create an object with:\n",
    "\n",
    "- <b>first_appearance:</b> the episode in which he or she first appeared\n",
    "- <b>gender:</b> the character's gender, as defined by the `gender_guesser` module's guess of his/her name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "sw_7_7YJMIWp",
    "outputId": "db9ad417-6448-4075-fa0b-163126f2b706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gender_guesser\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/fb/3f2aac40cd2421e164cab1668e0ca10685fcf896bd6b3671088f8aab356e/gender_guesser-0.4.0-py2.py3-none-any.whl (379kB)\n",
      "\u001b[K     |████████████████████████████████| 389kB 2.8MB/s \n",
      "\u001b[?25hInstalling collected packages: gender-guesser\n",
      "Successfully installed gender-guesser-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install gender_guesser\n",
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mjNZT57Qv7Vd",
    "outputId": "cd19eef4-c487-4fd3-a336-705e284c0e32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "users = {}\n",
    "for i in tqdm(range(1,11)):\n",
    "  season_number = '0'+str(i) if i < 10 else '10'\n",
    "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
    "  r = requests.get(json_file)\n",
    "  \n",
    "  season = json.loads(r.text)\n",
    "  episodes = season['episodes']\n",
    "  for j in range(len(episodes)):\n",
    "    episode = episodes[j]\n",
    "    scenes = episode['scenes']\n",
    "    for k in range(len(scenes)):\n",
    "      scene = scenes[k]\n",
    "      utterances = scene['utterances']\n",
    "      for l in range(len(utterances)):\n",
    "        utterance = utterances[l]\n",
    "        speaker_list = utterance['speakers']\n",
    "        for speaker in speaker_list:\n",
    "          if speaker not in users:\n",
    "            users[speaker] = {'first_appearance': episode['episode_id'], 'gender': d.get_gender(speaker.split()[0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJqo7WSSCua1"
   },
   "source": [
    "Sanity-checking the user data, we should see the correct genders assigned to the 6 friends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "YeKBCPZqCMeH",
    "outputId": "3d8d81bf-36f4-4c7f-8424-437b32bbca56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users in the data = 700/700\n",
      "Monica Geller object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n",
      "Joey Tribbiani object:  {'first_appearance': 's01_e01', 'gender': 'male'}\n",
      "Chandler Bing object:  {'first_appearance': 's01_e01', 'gender': 'mostly_male'}\n",
      "Phoebe Buffay object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n",
      "Ross Geller object:  {'first_appearance': 's01_e01', 'gender': 'male'}\n",
      "Rachel Green object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n"
     ]
    }
   ],
   "source": [
    "print(\"number of users in the data = {}/700\".format(len(users)))\n",
    "print(\"Monica Geller object: \", users[\"Monica Geller\"])\n",
    "print(\"Joey Tribbiani object: \", users[\"Joey Tribbiani\"])\n",
    "print(\"Chandler Bing object: \", users[\"Chandler Bing\"])\n",
    "print(\"Phoebe Buffay object: \", users[\"Phoebe Buffay\"])\n",
    "print(\"Ross Geller object: \", users[\"Ross Geller\"])\n",
    "print(\"Rachel Green object: \", users[\"Rachel Green\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Csa06wuFh8U"
   },
   "source": [
    "We then create a User object for each unique character in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xs7QjJJJFg2S"
   },
   "outputs": [],
   "source": [
    "corpus_users = {k: User(name=k, meta=v) for k,v in users.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cfMxU1u_GJD7",
    "outputId": "9608f357-a05d-4305-bc57-f2468f9d119e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica Geller\n",
      "{'first_appearance': 's01_e01', 'gender': 'female'}\n"
     ]
    }
   ],
   "source": [
    "print(corpus_users['Monica Geller'].name)\n",
    "print(corpus_users['Monica Geller'].meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tJ_9BMXFygP"
   },
   "source": [
    "## Generating Utterances\n",
    "\n",
    "We then loop through the data to generate a list of all utterances in the series. To align with the Utterance schema ConvoKit expects, we construct for each utterance:\n",
    "\n",
    "- **id:** index of the utterance\n",
    "\n",
    "- **user:** the user who authored the utterance; the speaker in our case\n",
    "\n",
    "- **root:** id of the conversation root of the utterance; the first utterance in the scene, in our case\n",
    "\n",
    "- **reply_to:** id of the utterance to which this utterance replies to; None if the utterance is not a reply.\n",
    "\n",
    "- **timestamp:** time of the utterance (None for us -- the dataset does not contain this information)\n",
    "\n",
    "- **text:** textual content of the utterance\n",
    "\n",
    "We also pull in the following metadata including:\n",
    "- **tokens** a tokenized representation of the text (handy for sentence separation)\n",
    "-**character_entities** available for some but not all utterances; `None` if unavailable. These are intended to identify who the user is speaking to and/or about.\n",
    "-**emotion** emotion labels for each token. Available for some but not all utterances; `None` if unavailable. \n",
    "-**caption**  available for some but not all utterances; `None` if unavailable. This contains the begin time, end time, and text sans punctuation. Only available for seasons 6-9.\n",
    "-**transcript_with_note**  a version of the text with an action note (e.g. \"(to Ross) Hand me the coffee\" vs. \"Hand me the coffee\"). Available for some but not all utterances; `None` if unavailable.\n",
    "-**token_with_note** a tokenized representation of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MDgFZjF6GN3O",
    "outputId": "3a963fc3-cba2-4a3a-b932-cc9526568439"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "all_utterances = {}\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(1,11)):\n",
    "  season_number = '0'+str(i) if i < 10 else '10'\n",
    "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
    "  r = requests.get(json_file)\n",
    "  \n",
    "  season = json.loads(r.text)\n",
    "  episodes = season['episodes']\n",
    "  for j in range(len(episodes)):\n",
    "    episode = episodes[j]\n",
    "    scenes = episode['scenes']\n",
    "    for k in range(len(scenes)):\n",
    "      scene = scenes[k]\n",
    "      utterances = scene['utterances']\n",
    "      \n",
    "      root = utterances[0] #set the root as the first utterance in the scene for now\n",
    "      \n",
    "      prev_utt = None\n",
    "\n",
    "      for l in range(len(utterances)):\n",
    "        utterance = utterances[l]\n",
    "        \n",
    "        speaker = utterance['speakers']\n",
    "        \n",
    "        if len(speaker) == 0:\n",
    "          prev_utt = None\n",
    "          continue\n",
    "        \n",
    "        # Add meta       \n",
    "        meta = {\n",
    "            'tokens': utterance.get('tokens'),\n",
    "            'character_entities': utterance.get('character_entities'),\n",
    "            'emotion': utterance.get('emotion'),\n",
    "            'caption': utterance.get('caption'),\n",
    "            'transcript_with_note': utterance.get('transcript_with_note'),\n",
    "            'tokens_with_note': utterance.get('tokens_with_note')\n",
    "        }\n",
    "        \n",
    "        # Create the Utterance, including meta\n",
    "        all_utterances[utterance['utterance_id']] = Utterance(\n",
    "            id=utterance['utterance_id'],\n",
    "            user=corpus_users[speaker[0]],\n",
    "            root=root['utterance_id'],\n",
    "            reply_to=prev_utt,\n",
    "            timestamp=None,\n",
    "            text=utterance['transcript'],\n",
    "            meta=meta\n",
    "        )\n",
    "        \n",
    "        # Get the prev_utt for the next iteration\n",
    "        prev_utt = utterance['utterance_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pmVQJYBiI1aY",
    "outputId": "5fe98756-0ba8-4052-a288-59117417cac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus has 61338/61309 utterances\n"
     ]
    }
   ],
   "source": [
    "print(\"This corpus has {}/61309 utterances\".format(len(all_utterances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "NkOm5qDdZYRQ",
    "outputId": "ea4e4a62-1260-4179-8e41-0450404fe3e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utterance({'id': 's01_e18_c05_u021', 'user': User([('name', 'Ross Geller')]), 'root': 's01_e18_c05_u001', 'reply_to': 's01_e18_c05_u020', 'timestamp': None, 'text': 'Alright.', 'meta': {'tokens': [['Alright', '.']], 'character_entities': [[]], 'emotion': ['Neutral', ['Neutral', 'Neutral', 'Neutral', 'Neutral']], 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}})"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_utterances['s01_e18_c05_u021']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjb5fkwmN5Ma"
   },
   "source": [
    "## Creating the corpus from a list of utterances\n",
    "\n",
    "We now create the corpus from our dict of utterances. Note, we are are allowing convokit to create conversations IDs automatically after loading the utterances list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0C_xjYMfOKVL"
   },
   "outputs": [],
   "source": [
    "utterance_list = [utt for k, utt in all_utterances.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVyGuspKOU7E"
   },
   "outputs": [],
   "source": [
    "friends_corpus = Corpus(utterances=utterance_list, version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UW5uEDZIcjWO"
   },
   "source": [
    "Sanity checks for the number of conversations in the dataset and the first 5 conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oCQcxnu1OZXv",
    "outputId": "d6c78d98-9a75-4f64-a1e0-4ab8634e023b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations in the dataset=3099\n"
     ]
    }
   ],
   "source": [
    "print(\"number of conversations in the dataset={}\".format(len(friends_corpus.get_conversation_ids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "Uypy9F-Vx0kM",
    "outputId": "7d7dcf7c-40cf-4366-c15d-df2d96397877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample conversation 0:\n",
      "['s01_e01_c01_u001', 's01_e01_c01_u002', 's01_e01_c01_u003', 's01_e01_c01_u004', 's01_e01_c01_u006', 's01_e01_c01_u007', 's01_e01_c01_u008', 's01_e01_c01_u010', 's01_e01_c01_u011', 's01_e01_c01_u012', 's01_e01_c01_u013', 's01_e01_c01_u014', 's01_e01_c01_u015', 's01_e01_c01_u016', 's01_e01_c01_u017', 's01_e01_c01_u018', 's01_e01_c01_u019', 's01_e01_c01_u021', 's01_e01_c01_u022', 's01_e01_c01_u023', 's01_e01_c01_u024', 's01_e01_c01_u025', 's01_e01_c01_u026', 's01_e01_c01_u027', 's01_e01_c01_u028', 's01_e01_c01_u029', 's01_e01_c01_u030', 's01_e01_c01_u031', 's01_e01_c01_u032', 's01_e01_c01_u033', 's01_e01_c01_u034', 's01_e01_c01_u035', 's01_e01_c01_u036', 's01_e01_c01_u037', 's01_e01_c01_u038', 's01_e01_c01_u039', 's01_e01_c01_u040', 's01_e01_c01_u041', 's01_e01_c01_u042', 's01_e01_c01_u044', 's01_e01_c01_u045', 's01_e01_c01_u047', 's01_e01_c01_u048', 's01_e01_c01_u049', 's01_e01_c01_u050', 's01_e01_c01_u051', 's01_e01_c01_u052', 's01_e01_c01_u053', 's01_e01_c01_u055', 's01_e01_c01_u056', 's01_e01_c01_u057', 's01_e01_c01_u058']\n",
      "sample conversation 1:\n",
      "['s01_e01_c02_u001', 's01_e01_c02_u002', 's01_e01_c02_u003', 's01_e01_c02_u004', 's01_e01_c02_u006', 's01_e01_c02_u007', 's01_e01_c02_u008', 's01_e01_c02_u009', 's01_e01_c02_u011', 's01_e01_c02_u012', 's01_e01_c02_u013', 's01_e01_c02_u014', 's01_e01_c02_u015', 's01_e01_c02_u017', 's01_e01_c02_u018', 's01_e01_c02_u019', 's01_e01_c02_u020', 's01_e01_c02_u021', 's01_e01_c02_u022', 's01_e01_c02_u023', 's01_e01_c02_u024', 's01_e01_c02_u026', 's01_e01_c02_u027', 's01_e01_c02_u028', 's01_e01_c02_u029', 's01_e01_c02_u030', 's01_e01_c02_u031', 's01_e01_c02_u032', 's01_e01_c02_u033', 's01_e01_c02_u034', 's01_e01_c02_u035', 's01_e01_c02_u036', 's01_e01_c02_u037', 's01_e01_c02_u038', 's01_e01_c02_u039', 's01_e01_c02_u040', 's01_e01_c02_u041', 's01_e01_c02_u043', 's01_e01_c02_u044', 's01_e01_c02_u045', 's01_e01_c02_u046', 's01_e01_c02_u047', 's01_e01_c02_u048', 's01_e01_c02_u049', 's01_e01_c02_u051', 's01_e01_c02_u052', 's01_e01_c02_u053', 's01_e01_c02_u054', 's01_e01_c02_u055', 's01_e01_c02_u056', 's01_e01_c02_u057', 's01_e01_c02_u058', 's01_e01_c02_u059', 's01_e01_c02_u060', 's01_e01_c02_u061', 's01_e01_c02_u062']\n",
      "sample conversation 2:\n",
      "['s01_e01_c03_u001']\n",
      "sample conversation 3:\n",
      "['s01_e01_c04_u001', 's01_e01_c04_u003', 's01_e01_c04_u004', 's01_e01_c04_u005', 's01_e01_c04_u006', 's01_e01_c04_u007', 's01_e01_c04_u008', 's01_e01_c04_u010', 's01_e01_c04_u011', 's01_e01_c04_u012', 's01_e01_c04_u013', 's01_e01_c04_u014', 's01_e01_c04_u015', 's01_e01_c04_u016', 's01_e01_c04_u017', 's01_e01_c04_u018', 's01_e01_c04_u019']\n",
      "sample conversation 4:\n",
      "['s01_e01_c05_u001', 's01_e01_c05_u002', 's01_e01_c05_u003', 's01_e01_c05_u004', 's01_e01_c05_u005', 's01_e01_c05_u006', 's01_e01_c05_u007', 's01_e01_c05_u008', 's01_e01_c05_u009']\n"
     ]
    }
   ],
   "source": [
    "convo_ids = friends_corpus.get_conversation_ids()\n",
    "for i, convo_idx in enumerate(convo_ids[0:5]):\n",
    "    print(\"sample conversation {}:\".format(i))\n",
    "    print(friends_corpus.get_conversation(convo_idx).get_utterance_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAHTbGQxcor3"
   },
   "source": [
    "Summary stats for the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "G0pyZSmWW9_1",
    "outputId": "e8e9d98a-c07b-4664-a048-4322ff105c43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 699\n",
      "Number of Utterances: 61338\n",
      "Number of Conversations: 3099\n"
     ]
    }
   ],
   "source": [
    "friends_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0WxYF6jbY5l"
   },
   "source": [
    "## Adding corpus-level metadata\n",
    "\n",
    "We add the name of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oGMAvEza5hL"
   },
   "outputs": [],
   "source": [
    "friends_corpus.meta['name'] = 'Friends Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LiUf8K6hTNjV"
   },
   "source": [
    "# Create the corpus dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaM2QdLJ4kPl"
   },
   "source": [
    "If working in a locally mounted notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PtD1v0Jo4nf7"
   },
   "outputs": [],
   "source": [
    "friends_corpus.dump(\"friends-corpus\", base_path = \"YOUR_BASE_PATH/datasets/friends-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uygI-2aU2PKn"
   },
   "source": [
    "If working in Google Colab, first mount your Google Drive, then dump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "aMcBSeS2xumI",
    "outputId": "bb123206-1a9d-47e9-e085-f3c4b8744d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "import google\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1TGJ37nzKUh"
   },
   "outputs": [],
   "source": [
    "friends_corpus.dump(\"friends-corpus\", base_path = \"gdrive/My Drive/F19-CS6742/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aN2CraqRaUYb"
   },
   "outputs": [],
   "source": [
    "with ZipFile(\"gdrive/My Drive/CS6742/friends-corpus.zip\", 'w') as zip_f:\n",
    "  for fname in os.listdir(\"gdrive/My Drive/CS6742/friends-corpus\"):\n",
    "    zip_f.write(\"gdrive/My Drive/CS6742/friends-corpus/\"+fname)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "convert-et397.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
