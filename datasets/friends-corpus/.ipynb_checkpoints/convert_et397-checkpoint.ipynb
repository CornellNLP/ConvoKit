{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_YNRDvb3etb"
   },
   "source": [
    "# Converting the Friends dataset into ConvoKit format\n",
    "\n",
    "This notebook describes how we converted the Friends dataset (https://github.com/emorynlp/character-mining) into a Corpus with ConvoKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Wp3WIlBN4D7l",
    "outputId": "77391cf8-1ce6-4b0c-977e-d8bfdfa54729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting convokit\n",
      "Collecting scikit-learn>=0.20.0 (from convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/e9/57/8a9889d49d0d77905af5a7524fb2b468d2ef5fc723684f51f5ca63efed0d/scikit_learn-0.21.3-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting dill==0.2.9 (from convokit)\n",
      "Collecting pandas>=0.23.4 (from convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/39/73/99aa822ee88cef5829607217c11bf24ecc1171ae5d49d5f780085f5da518/pandas-0.25.1-cp37-cp37m-macosx_10_9_x86_64.macosx_10_10_x86_64.whl\n",
      "Collecting spacy==2.0.12 (from convokit)\n",
      "Collecting msgpack-numpy==0.4.3.2 (from convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/45/464be6da85b5ca893cfcbd5de3b31a6710f636ccb8521b17bd4110a08d94/msgpack_numpy-0.4.3.2-py2.py3-none-any.whl\n",
      "Collecting matplotlib>=3.0.0 (from convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/8b/af9e0984f5c0df06d3fab0bf396eb09cbf05f8452de4e9502b182f59c33b/matplotlib-3.1.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting nltk>=3.4 (from convokit)\n",
      "Collecting scipy>=1.1.0 (from convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/d5/06/1a696649f4b2e706c509cb9333fdc6331fbe71251cede945f9e1fa13ea34/scipy-1.3.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting numpy>=1.11.0 (from scikit-learn>=0.20.0->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/b4/e8/5ececadd9cc220bb783b4ce6ffaa9266925d37ed41237bc23bc530ab4f3d/numpy-1.17.2-cp37-cp37m-macosx_10_6_intel.whl\n",
      "Collecting joblib>=0.11 (from scikit-learn>=0.20.0->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl\n",
      "Collecting pytz>=2017.2 (from pandas>=0.23.4->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/87/76/46d697698a143e05f77bec5a526bf4e56a0be61d63425b68f4ba553b51f2/pytz-2019.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/site-packages (from pandas>=0.23.4->convokit) (2.8.0)\n",
      "Collecting preshed<2.0.0,>=1.0.0 (from spacy==2.0.12->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/71/60659082ccb54d6bc2b9420d0f95c2a2c7423ce0777f9305723145c11f35/preshed-1.0.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting plac<1.0.0,>=0.9.6 (from spacy==2.0.12->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
      "Collecting cymem<1.32,>=1.30 (from spacy==2.0.12->convokit)\n",
      "Collecting ujson>=1.35 (from spacy==2.0.12->convokit)\n",
      "Collecting thinc<6.11.0,>=6.10.3 (from spacy==2.0.12->convokit)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy==2.0.12->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\n",
      "Collecting murmurhash<0.29,>=0.28 (from spacy==2.0.12->convokit)\n",
      "Collecting regex==2017.4.5 (from spacy==2.0.12->convokit)\n",
      "Collecting msgpack>=0.3.0 (from msgpack-numpy==0.4.3.2->convokit)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.0.0->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib>=3.0.0->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/df/93/8bc9b52a8846be2b9572aa0a7c881930939b06e4abe1162da6a0430b794f/kiwisolver-1.1.0-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib>=3.0.0->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/11/fa/0160cd525c62d7abd076a070ff02b2b94de589f1a9789774f17d7c54058e/pyparsing-2.4.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /Users/emilytseng/Library/Python/3.7/lib/python/site-packages (from nltk>=3.4->convokit) (1.12.0)\n",
      "Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
      "Collecting wrapt<1.11.0,>=1.10.0 (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/88/d3213e2f3492daf09d8b41631ad6899f56db17ce83ea9c8a579902bafe5e/tqdm-4.35.0-py2.py3-none-any.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl\n",
      "Collecting idna<2.9,>=2.5 (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy==2.0.12->convokit)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/b0/8146a4f8dd402f60744fa380bc73ca47303cccf8b9190fd16a827281eac2/certifi-2019.9.11-py2.py3-none-any.whl (154kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->convokit) (41.0.1)\n",
      "Collecting toolz>=0.8.0 (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy==2.0.12->convokit)\n",
      "Installing collected packages: numpy, scipy, joblib, scikit-learn, dill, pytz, pandas, cymem, preshed, plac, ujson, msgpack, toolz, cytoolz, wrapt, msgpack-numpy, murmurhash, tqdm, thinc, chardet, urllib3, idna, certifi, requests, regex, spacy, cycler, kiwisolver, pyparsing, matplotlib, nltk, convokit\n",
      "  Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed certifi-2019.9.11 chardet-3.0.4 convokit-2.0.11 cycler-0.10.0 cymem-1.31.2 cytoolz-0.9.0.1 dill-0.2.9 idna-2.8 joblib-0.13.2 kiwisolver-1.1.0 matplotlib-3.1.1 msgpack-0.6.1 msgpack-numpy-0.4.3.2 murmurhash-0.28.0 nltk-3.4.5 numpy-1.17.2 pandas-0.25.1 plac-0.9.6 preshed-1.0.1 pyparsing-2.4.2 pytz-2019.2 regex-2017.4.5 requests-2.22.0 scikit-learn-0.21.3 scipy-1.3.1 spacy-2.0.12 thinc-6.10.3 toolz-0.10.0 tqdm-4.35.0 ujson-1.35 urllib3-1.25.3 wrapt-1.10.11\n",
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K     |████████████████████████████████| 37.4MB 8.9MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/rb/gnb30d2s5ls0jdfrx8frt7_h0000gp/T/pip-ephem-wheel-cache-dfzzk81y/wheels/54/7c/d8/f86364af8fbba7258e14adae115f18dd2c91552406edc3fdaa\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.0.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /usr/local/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "    /usr/local/lib/python3.7/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip3 install convokit\n",
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXk4Biep3xHH"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from convokit import Corpus, User, Utterance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmw72j5J44E4"
   },
   "source": [
    "## The Friends Dataset\n",
    "\n",
    "The original dataset (https://github.com/emorynlp/character-mining) contains a set of 10 JSON files, each of which represents a complete transcript of 1 season of <i>Friends</i>. Since the data are available in JSON format from this GitHub repo, we download the raw data directly using the `requests` module. You will not need to download raw data files to use this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7CI3mm378N8"
   },
   "source": [
    "## Gather information about the corpus\n",
    "For the **corpus.json** file, it will include information of number of episodes, number of scenes, number of utterances and number of speakers.\n",
    "When counting the number of utterances, we ignore utterances that have no conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xN79FO2u6B8q"
   },
   "outputs": [],
   "source": [
    "num_episodes = 0\n",
    "num_scenes = 0\n",
    "num_utterances = 0\n",
    "speakers = set()\n",
    "for i in range(1,11):\n",
    "  season_number = '0'+str(i) if i < 10 else '10'\n",
    "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
    "  r = requests.get(json_file)\n",
    "  \n",
    "  season = json.loads(r.text)\n",
    "  episodes = season['episodes']\n",
    "  num_episodes += len(episodes)\n",
    "  for j in range(len(episodes)):\n",
    "    episode = episodes[j]\n",
    "    scenes = episode['scenes']\n",
    "    num_scenes += len(scenes)\n",
    "    for k in range(len(scenes)):\n",
    "      scene = scenes[k]\n",
    "      utterances = scene['utterances']\n",
    "      for l in range(len(utterances)):\n",
    "        utterance = utterances[l]\n",
    "        speaker = utterance['speakers']\n",
    "        speakers.update(speaker)\n",
    "        num_utterances += 1 if len(speaker) != 0 else 0\n",
    "corpus = {'friends': 'friends corpus', 'num_episodes': num_episodes, 'num_scenes': num_scenes, 'num_utterances': num_utterances, 'num_speakers': len(speakers)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "o827biTbBtlu",
    "outputId": "f85449cf-cded-4229-e5af-b7add85f2a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'friends': 'friends corpus', 'num_episodes': 236, 'num_scenes': 3107, 'num_utterances': 61338, 'num_speakers': 700}\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxhlL6aNCvxr"
   },
   "source": [
    "## Generating user information\n",
    "Since our dataset doesn't have any existing user information, we extract speaker information from the conversation. For each user, we collect the episode in which he/she first appears and guess his/her gender based on the name using the gender_guesser module.\n",
    "\n",
    "For each user, we create an object with:\n",
    "\n",
    "- <b>name:</b> the name of the character\n",
    "- <b>first_appearance:</b> the episode in which he or she first appeared\n",
    "- <b>gender:</b> the character's gender, as defined by the `gender_guesser` module's guess of his/her name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "sw_7_7YJMIWp",
    "outputId": "fb1f6b61-5b5e-4c18-8b08-7db8fca4e39b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gender_guesser\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/fb/3f2aac40cd2421e164cab1668e0ca10685fcf896bd6b3671088f8aab356e/gender_guesser-0.4.0-py2.py3-none-any.whl (379kB)\n",
      "\u001b[K     |████████████████████████████████| 389kB 2.8MB/s \n",
      "\u001b[?25hInstalling collected packages: gender-guesser\n",
      "Successfully installed gender-guesser-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install gender_guesser\n",
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjNZT57Qv7Vd"
   },
   "outputs": [],
   "source": [
    "users = {}\n",
    "for i in range(1,11):\n",
    "  season_number = '0'+str(i) if i < 10 else '10'\n",
    "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
    "  r = requests.get(json_file)\n",
    "  \n",
    "  season = json.loads(r.text)\n",
    "  episodes = season['episodes']\n",
    "  for j in range(len(episodes)):\n",
    "    episode = episodes[j]\n",
    "    scenes = episode['scenes']\n",
    "    for k in range(len(scenes)):\n",
    "      scene = scenes[k]\n",
    "      utterances = scene['utterances']\n",
    "      for l in range(len(utterances)):\n",
    "        utterance = utterances[l]\n",
    "        speaker_list = utterance['speakers']\n",
    "        for speaker in speaker_list:\n",
    "          if speaker not in users:\n",
    "            users[speaker] = {'first_appearance': episode['episode_id'], 'gender': d.get_gender(speaker.split()[0])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJqo7WSSCua1"
   },
   "source": [
    "Sanity-checking the user data, we should see the correct genders assigned to the 6 friends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "YeKBCPZqCMeH",
    "outputId": "3fd779d3-eda9-424d-e032-64a3034665bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users in the data = 700/700\n",
      "Monica Geller object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n",
      "Joey Tribbiani object:  {'first_appearance': 's01_e01', 'gender': 'male'}\n",
      "Chandler Bing object:  {'first_appearance': 's01_e01', 'gender': 'mostly_male'}\n",
      "Phoebe Buffay object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n",
      "Ross Geller object:  {'first_appearance': 's01_e01', 'gender': 'male'}\n",
      "Rachel Green object:  {'first_appearance': 's01_e01', 'gender': 'female'}\n"
     ]
    }
   ],
   "source": [
    "print(\"number of users in the data = {}/700\".format(len(users)))\n",
    "print(\"Monica Geller object: \", users[\"Monica Geller\"])\n",
    "print(\"Joey Tribbiani object: \", users[\"Joey Tribbiani\"])\n",
    "print(\"Chandler Bing object: \", users[\"Chandler Bing\"])\n",
    "print(\"Phoebe Buffay object: \", users[\"Phoebe Buffay\"])\n",
    "print(\"Ross Geller object: \", users[\"Ross Geller\"])\n",
    "print(\"Rachel Green object: \", users[\"Rachel Green\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Csa06wuFh8U"
   },
   "source": [
    "We then create a User object for each unique character in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xs7QjJJJFg2S"
   },
   "outputs": [],
   "source": [
    "corpus_users = {k: User(name=k, meta=v) for k,v in users.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cfMxU1u_GJD7",
    "outputId": "c1a29dd9-a2b9-4832-c1d0-10a94e86aae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monica Geller\n",
      "{'first_appearance': 's01_e01', 'gender': 'female'}\n"
     ]
    }
   ],
   "source": [
    "print(corpus_users['Monica Geller'].name)\n",
    "print(corpus_users['Monica Geller'].meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tJ_9BMXFygP"
   },
   "source": [
    "## Generating Utterances\n",
    "\n",
    "We then loop through the data to generate a list of all utterances in the series. To align with the Utterance schema ConvoKit expects, we construct for each utterance:\n",
    "\n",
    "- **id:** index of the utterance\n",
    "\n",
    "- **user:** the user who authored the utterance; the speaker in our case\n",
    "\n",
    "- **root:** id of the conversation root of the utterance; the scene id in our case\n",
    "\n",
    "- **reply_to:** id of the utterance to which this utterance replies to; None if the utterance is not a reply. The previous speaker, to simplify the process.\n",
    "\n",
    "- **timestamp:** time of the utterance\n",
    "\n",
    "- **text:** textual content of the utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "MDgFZjF6GN3O",
    "outputId": "df2c3b44-537d-4b73-cbbc-7bbea4d0eac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped season 1/10: 5976 utterances\n",
      "Scraped season 2/10: 5752 utterances\n",
      "Scraped season 3/10: 12472 utterances\n",
      "Scraped season 4/10: 12073 utterances\n",
      "Scraped season 5/10: 18693 utterances\n",
      "Scraped season 6/10: 18532 utterances\n",
      "Scraped season 7/10: 25007 utterances\n",
      "Scraped season 8/10: 24753 utterances\n",
      "Scraped season 9/10: 31337 utterances\n",
      "Scraped season 10/10: 30001 utterances\n"
     ]
    }
   ],
   "source": [
    "all_utterances = {}\n",
    "\n",
    "last_seasons_utterances = 0\n",
    "for i in range(1,11):\n",
    "  season_number = '0'+str(i) if i < 10 else '10'\n",
    "  json_file = 'https://raw.githubusercontent.com/emorynlp/character-mining/master/json/friends_season_'+str(season_number)+'.json'\n",
    "  r = requests.get(json_file)\n",
    "  \n",
    "  season = json.loads(r.text)\n",
    "  episodes = season['episodes']\n",
    "  for j in range(len(episodes)):\n",
    "    episode = episodes[j]\n",
    "    scenes = episode['scenes']\n",
    "    for k in range(len(scenes)):\n",
    "      scene = scenes[k]\n",
    "      utterances = scene['utterances']\n",
    "      prev_utt = None\n",
    "      for l in range(len(utterances)):\n",
    "        utterance = utterances[l]\n",
    "        speaker = utterance['speakers']\n",
    "        if len(speaker) == 0:\n",
    "          prev_utt = None\n",
    "          continue\n",
    "        all_utterances[utterance['utterance_id']] = Utterance(\n",
    "            id=utterance['utterance_id'],\n",
    "            user=corpus_users[speaker[0]],\n",
    "            root=scene['scene_id'],\n",
    "            reply_to=prev_utt,\n",
    "            timestamp=None,\n",
    "            text=utterance['transcript']\n",
    "        )\n",
    "#         utterance_info = {}\n",
    "#         utterance_info['id'] = utterance['utterance_id']\n",
    "#         utterance_info['user'] = speaker[0]\n",
    "#         utterance_info['root'] = scene['scene_id']\n",
    "#         utterance_info['reply_to'] = prev_utt\n",
    "#         utterance_info['timestamp'] = None\n",
    "#         utterance_info['text'] = utterance['transcript']\n",
    "        prev_utt = utterance['utterance_id']\n",
    "  print('Scraped season {}/10: {} utterances'.format(i, len(all_utterances) - last_seasons_utterances))\n",
    "  last_seasons_utterances = len(all_utterances) - last_seasons_utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pmVQJYBiI1aY",
    "outputId": "70d965af-91c4-4b0d-9fec-477a06b42ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus has 61338/61309 utterances\n"
     ]
    }
   ],
   "source": [
    "print(\"This corpus has {}/61309 utterances\".format(len(all_utterances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjb5fkwmN5Ma"
   },
   "source": [
    "# Creating the corpus from a list of utterances\n",
    "\n",
    "We now create the corpus from our dict of utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0C_xjYMfOKVL"
   },
   "outputs": [],
   "source": [
    "utterance_list = [utt for k, utt in all_utterances.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVyGuspKOU7E"
   },
   "outputs": [],
   "source": [
    "friends_corpus = Corpus(utterances=utterance_list, version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oCQcxnu1OZXv",
    "outputId": "37b502a4-f0ff-4cb9-c64d-3679c462344d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of conversations in the dataset=3099\n"
     ]
    }
   ],
   "source": [
    "print(\"number of conversations in the dataset={}\".format(len(friends_corpus.get_conversation_ids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7rL-5pRYRcAg"
   },
   "outputs": [],
   "source": [
    "friends_corpus.get_usernames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LiUf8K6hTNjV"
   },
   "source": [
    "# Create the corpus dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uhbt1xcCTQGl"
   },
   "outputs": [],
   "source": [
    "friends_corpus.dump(\"corpus\", base_path=\"datasets/friends-corpus\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "convert-et397.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
