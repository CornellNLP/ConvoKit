{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Get Closest Cluster Given Question Text\n",
    "## get_cluster_for_q_text\n",
    "\n",
    "This notebook defines several helper functions for and a function that gets the closest cluster number given a question text that does not necessarily have to come from the corpus.\n",
    "\n",
    "The function 'get_cluster_for_q_text' itself is defined in the last cell of this notebook.\n",
    "\n",
    "Example usage coming soon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pkg_resources\n",
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from convokit import Corpus, QuestionTypology, download, MotifsExtractor, QuestionTypologyUtils\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from ast import literal_eval as make_tuple\n",
    "from collections import defaultdict, Counter\n",
    "from scipy import sparse\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from spacy.en import English\n",
    "from spacy.symbols import *\n",
    "from spacy.tokens.doc import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_NLP = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_all(x):\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_arcs(comment_text, selector=select_all):\n",
    "    sent_df = []\n",
    "    spacy_obj = spacy_NLP(comment_text)\n",
    "    for s_idx, sent in enumerate(spacy_obj.sents):\n",
    "        sent_text = sent.text.strip()\n",
    "        if len(sent_text) == 0: continue\n",
    "        if selector(sent_text):\n",
    "            sent_df.append({\n",
    "                    'idx': 'A', 'sent_idx': s_idx, 'span': sent, \n",
    "                    'arc_sets': MotifsExtractor.get_arcs(sent.root, True),\n",
    "                    'content': sent_text, 'sent_key': 'A' + '_' + str(s_idx)\n",
    "                })\n",
    "    sent_df = pd.DataFrame(sent_df)\n",
    "    return sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_motif_info(motif_dir):\n",
    "    super_mappings = {}\n",
    "    with open(os.path.join(motif_dir, 'question_supersets_arcset_to_super.json')) as f:\n",
    "        for line in f.readlines():\n",
    "            entry = json.loads(line)\n",
    "            super_mappings[tuple(entry['arcset'])] = tuple(entry['super'])\n",
    "\n",
    "    downlinks = MotifsExtractor.read_downlinks(os.path.join(motif_dir, 'question_tree_downlinks.json'))    \n",
    "    node_counts = MotifsExtractor.read_nodecounts(os.path.join(motif_dir, 'question_tree_arc_set_counts.tsv'))\n",
    "    return super_mappings, downlinks, node_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_questions_and_answers(sent_df, q_vocab, a_vocab, \n",
    "                            super_mappings, downlink_info, node_count_info,\n",
    "                            threshold, outfile=None, per_sent=False): \n",
    "\n",
    "    question_to_fits = defaultdict(set)\n",
    "    question_to_leaf_fits = defaultdict(set)\n",
    "    question_to_a_fits = defaultdict(set)\n",
    "\n",
    "    for tup in sent_df.itertuples():\n",
    "        if per_sent:\n",
    "            key = tup.sent_key\n",
    "        else:\n",
    "            key = tup.idx\n",
    "        for arc in tup.arc_sets:\n",
    "            if arc in a_vocab: question_to_a_fits[key].add(arc)\n",
    "\n",
    "        motif_fits = MotifsExtractor.fit_question(tup.arc_sets, downlink_info, node_count_info)\n",
    "        for entry in motif_fits.values():\n",
    "            motif = entry['arcset']\n",
    "            if motif == ('*', ): continue\n",
    "            super_motif = super_mappings.get(motif, '')\n",
    "            if super_motif not in q_vocab: continue\n",
    "            if entry['arcset_count'] < threshold: continue\n",
    "            if entry['max_valid_child_count'] < threshold:\n",
    "                question_to_leaf_fits[key].add(super_motif)\n",
    "            question_to_fits[key].add(super_motif)\n",
    "    if outfile is not None:\n",
    "        df = pd.DataFrame.from_dict({\n",
    "                'question_fits': question_to_fits,\n",
    "                'question_leaf_fits': question_to_leaf_fits,\n",
    "                'question_a_fits': question_to_a_fits\n",
    "            })\n",
    "        df.to_csv(outfile + '.fits.tsv', sep='\\t')\n",
    "    return question_to_fits, question_to_leaf_fits, question_to_a_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_new_qa_mtx_obj(question_to_fits, question_to_leaf_fits, question_to_a_fits, ref_mtx_obj,\n",
    "        outfile=None):\n",
    "\n",
    "    docs = [x for x,y in question_to_fits.items() if len(y) > 0]\n",
    "    doc_to_idx = {doc:idx for idx,doc in enumerate(docs)}\n",
    "    qterm_idxes = []\n",
    "    leaves = []\n",
    "    qdoc_idxes = []\n",
    "    aterm_idxes = []\n",
    "    adoc_idxes = []\n",
    "\n",
    "    for doc in docs:\n",
    "        qterms = question_to_fits[doc]\n",
    "        for term in qterms:\n",
    "            qterm_idxes.append(ref_mtx_obj['q_term_to_idx'][term])\n",
    "            leaves.append(term in question_to_leaf_fits[doc])\n",
    "            qdoc_idxes.append(doc_to_idx[doc])\n",
    "        aterms = question_to_a_fits[doc]\n",
    "        for term in aterms:\n",
    "            aterm_idxes.append(ref_mtx_obj['a_term_to_idx'][term])\n",
    "            adoc_idxes.append(doc_to_idx[doc])\n",
    "\n",
    "    qterm_idxes = np.array(qterm_idxes)\n",
    "    leaves = np.array(leaves)\n",
    "    qdoc_idxes = np.array(qdoc_idxes)\n",
    "    aterm_idxes = np.array(aterm_idxes)\n",
    "    adoc_idxes = np.array(adoc_idxes)\n",
    "    new_mtx_obj = {'q_terms': ref_mtx_obj['q_terms'], 'q_didxes': qdoc_idxes, 'docs': docs, 'q_leaves': leaves,\n",
    "                  'q_term_counts': ref_mtx_obj['q_term_counts'], 'q_term_to_idx': ref_mtx_obj['q_term_to_idx'],\n",
    "                  'doc_to_idx': doc_to_idx, 'q_tidxes': qterm_idxes, 'N_idf_docs': len(ref_mtx_obj['docs']),\n",
    "                   'a_terms': ref_mtx_obj['a_terms'],\n",
    "                  'a_term_counts': ref_mtx_obj['a_term_counts'], 'a_term_to_idx': ref_mtx_obj['a_term_to_idx'],\n",
    "                  'a_tidxes': aterm_idxes, 'a_didxes': adoc_idxes}\n",
    "    if outfile is not None:\n",
    "        np.save(outfile + '.q.tidx.npy', qterm_idxes)\n",
    "        np.save(outfile + '.q.leaves.npy', leaves)\n",
    "        np.save(outfile + '.a.tidx.npy', aterm_idxes)\n",
    "        np.save(outfile + '.q.didx.npy', qdoc_idxes)\n",
    "        np.save(outfile + '.a.didx.npy', adoc_idxes)\n",
    "        with open(outfile + '.docs.txt', 'w') as f:\n",
    "            f.write('\\n'.join(docs))\n",
    "\n",
    "    return new_mtx_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mtx(mtx_obj, data_type, norm, idf, leaves_only):\n",
    "    #norm = l2, idf = False, leaves_only = True\n",
    "    N_terms = len(mtx_obj[data_type + '_terms'])\n",
    "    N_docs = len(mtx_obj['docs'])\n",
    "    if 'N_idf_docs' in mtx_obj:\n",
    "        N_idf_docs = mtx_obj['N_idf_docs']  # technical detail:  we want IDFs on the *training* data\n",
    "    else:\n",
    "        N_idf_docs = N_docs\n",
    "    if idf:\n",
    "        data = np.log(N_docs) - np.log(mtx_obj[data_type + '_term_counts'][mtx_obj[data_type + '_tidxes']])\n",
    "    else:\n",
    "        data = np.ones_like(mtx_obj[data_type + '_tidxes'])\n",
    "        if leaves_only:\n",
    "            data[~mtx_obj[data_type + '_leaves']] = 0\n",
    "    mtx = sparse.csr_matrix((data, (mtx_obj[data_type + '_tidxes'], mtx_obj[data_type + '_didxes'])),\n",
    "        shape=(N_terms,N_docs))\n",
    "    if norm:\n",
    "        mtx = Normalizer(norm=norm).fit_transform(mtx.astype(np.double))\n",
    "    return mtx\n",
    "\n",
    "def project_qa_embeddings(mtx_obj, lq, au, outfile=None):\n",
    "\n",
    "    qmtx = build_mtx(mtx_obj,'q',norm='l2', idf=False, leaves_only=True)\n",
    "    amtx = build_mtx(mtx_obj, 'a', norm='l2', idf=True, leaves_only=False)\n",
    "\n",
    "    lq_norm = Normalizer().fit_transform(lq)\n",
    "    au_norm = Normalizer().fit_transform(au)\n",
    "\n",
    "    qdoc_vects = Normalizer().fit_transform(qmtx.T) * lq_norm\n",
    "    adoc_vects = ((amtx.T) * au)\n",
    "\n",
    "    if outfile is not None:\n",
    "        np.save(outfile + '.qdoc', qdoc_vects)\n",
    "        np.save(outfile + '.adoc', adoc_vects)\n",
    "\n",
    "    return qdoc_vects, adoc_vects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_qtype_model(mtx_obj, lq, au, n_clusters, snip=True, \n",
    "                            random_state=None, max_iter=1000,\n",
    "                            display=None, max_dist_quantile=None,\n",
    "                            outfile=None):\n",
    "\n",
    "    lq_norm = Normalizer().fit_transform(lq)\n",
    "    au_norm = Normalizer().fit_transform(au)\n",
    "\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=random_state, max_iter=max_iter)\n",
    "    km.fit(lq_norm)\n",
    "\n",
    "    motif_labels = km.predict(lq_norm)\n",
    "    motif_dists = km.transform(lq_norm).min(axis=1)\n",
    "    aarc_labels = km.predict(au_norm)\n",
    "    aarc_dists = km.transform(au_norm).min(axis=1)\n",
    "    motif_df = pd.DataFrame({'motif': mtx_obj['q_terms'], 'cluster': motif_labels, 'cluster_dist': motif_dists})[['motif', 'cluster_dist', 'cluster']]\n",
    "    aarc_df = pd.DataFrame({'aarc': mtx_obj['a_terms'], 'cluster': aarc_labels, 'cluster_dist': aarc_dists})[['aarc', 'cluster_dist', 'cluster']]\n",
    "\n",
    "    if display is not None:\n",
    "        print('displaying for %d clusters' % n_clusters)\n",
    "        print('-----')\n",
    "        for c in range(n_clusters):\n",
    "            print(c)\n",
    "            print('--------')\n",
    "            motif_subset = motif_df[motif_df.cluster == c]\n",
    "            aarc_subset = aarc_df[aarc_df.cluster == c]\n",
    "            print('\\tquestions (%d):' % len(motif_subset))\n",
    "            display_top_motifs(motif_subset, display, max_dist_quantile, random_state)\n",
    "            print('')\n",
    "            print('\\tanswers (%d):' % len(aarc_subset))\n",
    "            display_top_motifs(aarc_subset, display, max_dist_quantile, random_state)\n",
    "            print('')\n",
    "        print('\\n=====\\n')\n",
    "\n",
    "    if outfile is not None:\n",
    "        joblib.dump(km, '%s.%d.km' % (outfile, n_clusters))\n",
    "        motif_df.to_csv('%s.%d.motifs.tsv' % (outfile, n_clusters), sep='\\t')\n",
    "        aarc_df.to_csv('%s.%d.aarcs.tsv' % (outfile, n_clusters), sep='\\t')\n",
    "\n",
    "    return km, motif_df, aarc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_cluster(qdoc_vects, adoc_vects, km):\n",
    "\n",
    "    n_clusters = km.n_clusters\n",
    "    qdoc_norm = Normalizer().fit_transform(qdoc_vects)\n",
    "    adoc_norm = Normalizer().fit_transform(adoc_vects)\n",
    "\n",
    "    qdoc_labels = km.predict(qdoc_norm)\n",
    "\n",
    "    return int(qdoc_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_for_q_text(q_text, questionTypology):\n",
    "    '''\n",
    "    Takes a string containing the question text and a question typology object and returns the closest cluster.\n",
    "    :param q_text: the question text\n",
    "    :param questionTypology: the question typology object\n",
    "    :return: the closest cluster number in questionTypology that q_text corresponds to\n",
    "    '''\n",
    "    sent_df = extract_arcs(q_text)\n",
    "    super_mappings, downlinks, node_counts = load_motif_info(questionTypology.motifs_dir)\n",
    "    avocab = set(questionTypology.mtx_obj['a_terms'])\n",
    "    qvocab = set(questionTypology.mtx_obj['q_terms'])\n",
    "    question_to_fits, question_to_leaf_fits, question_to_a_fits = fit_questions_and_answers(sent_df, qvocab, avocab, \n",
    "        super_mappings, downlinks, node_counts, questionTypology.question_threshold)\n",
    "    new_mtx_obj = make_new_qa_mtx_obj(question_to_fits, question_to_leaf_fits, question_to_a_fits, \n",
    "        questionTypology.mtx_obj)\n",
    "    qdoc_vects, adoc_vects = project_qa_embeddings(new_mtx_obj, questionTypology.lq, questionTypology.a_u)\n",
    "    return get_best_cluster(qdoc_vects, adoc_vects, questionTypology.km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
